2023-12-15 17:56:37 - INFO :       
********************************************************************************************************************************************************************************************************

2023-12-15 17:56:37 - INFO :       DATASET: commonsense_qa
Single
Index 0
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]
2023-12-15 17:56:49 - INFO :       Use taylor pruner...
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
)
DataParallel(
  (module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 5120)
      (layers): ModuleList(
        (0-39): 40 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
            (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
  )
)
Traceback (most recent call last):
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/dependency.py", line 679, in _trace
    out = model(*example_inputs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
ValueError: Caught ValueError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/models/hf_llama/modeling_llama.py", line 689, in forward
    outputs = self.model(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/models/hf_llama/modeling_llama.py", line 509, in forward
    batch_size, seq_length = input_ids.shape
ValueError: not enough values to unpack (expected 2, got 1)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 187, in <module>
    main(args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 126, in main
    distribution =compute_single(logger,dataset_list,args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/compute_single.py", line 222, in compute_single
    pruner = tp.pruner.MetaPruner(
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/pruner/algorithms/metapruner.py", line 80, in __init__
    self.DG = dependency.DependencyGraph().build_dependency(
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/dependency.py", line 373, in build_dependency
    self.module2node = self._trace(
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/dependency.py", line 681, in _trace
    out = model(example_inputs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 170, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 175, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 91, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 71, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 58, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 1; 44.38 GiB total capacity; 42.11 GiB already allocated; 38.62 MiB free; 42.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
