Single
Index 0
2024-01-16 14:35:07 - INFO :       
********************************************************************************************************************************************************************************************************

2024-01-16 14:35:07 - INFO :       DATASET: commonsense_qa
Single
Index 0
2024-01-16 14:35:07 - INFO :       
********************************************************************************************************************************************************************************************************

2024-01-16 14:35:07 - INFO :       DATASET: commonsense_qa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]
2024-01-16 14:35:15 - INFO :       Start Pruning
2024-01-16 14:35:15 - INFO :       Use taylor pruner...
2024-01-16 14:35:15 - INFO :       Pruning Attention Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
2024-01-16 14:35:15 - INFO :       Pruning MLP Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
Rank: 1 | PID: 585408|
2024-01-16 14:35:19 - INFO :       Start Pruning
Rank: 0 | PID: 585407|
FullyShardedDataParallel(
  (_fsdp_wrapped_module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096)
      (layers): ModuleList(
        (0-31): 32 x FullyShardedDataParallel(
          (_fsdp_wrapped_module): LlamaDecoderLayer(
            (self_attn): FullyShardedDataParallel(
              (_fsdp_wrapped_module): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
            )
            (mlp): FullyShardedDataParallel(
              (_fsdp_wrapped_module): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (act_fn): SiLUActivation()
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
******************************
Loading Dataset
******************************
Generating Samples
******************************
Prepare Validation Dataset
2024-01-16 14:35:26 - INFO :       Start Backwarding in iterative steps = 0...
Parent ID | PID: 585408|
Before Evaluation
	 1 Before Forward
FullyShardedDataParallel(
  (_fsdp_wrapped_module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096)
      (layers): ModuleList(
        (0-31): 32 x FullyShardedDataParallel(
          (_fsdp_wrapped_module): LlamaDecoderLayer(
            (self_attn): FullyShardedDataParallel(
              (_fsdp_wrapped_module): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
            )
            (mlp): FullyShardedDataParallel(
              (_fsdp_wrapped_module): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (act_fn): SiLUActivation()
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
******************************
Loading Dataset
******************************
Generating Samples
******************************
Prepare Validation Dataset
2024-01-16 14:35:28 - INFO :       Start Backwarding in iterative steps = 0...
Parent ID | PID: 585407|
Before Evaluation
	 0 Before Forward
	 0 Before Backward	
 1 Before Backward
		  0 0 0After Backward 
1 After Backward
	 0 0 Finished one iteration
	 0 1 Finished one iteration
After Evaluation
Completed 
Before Barrier 
After Evaluation
	Loss: 8.750000
Completed 
Before Barrier 
After Barrier 
After Barrier 
Completed  0
Completed  1
2024-01-16 14:36:07 - INFO :       Finished collecting gradient
After Everything
Traceback (most recent call last):
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 188, in <module>
    main(args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 127, in main
    distribution =compute_single(logger,dataset_list,args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/compute_single_dist.py", line 422, in compute_single
    logger.log("#Param before: {}, #Param after: {}, Ratio = {:.4f}%".format(before_pruning_parameters, after_pruning_parameters,  100.0*after_pruning_parameters/before_pruning_parameters))
UnboundLocalError: local variable 'after_pruning_parameters' referenced before assignment
2024-01-16 14:36:07 - INFO :       Finished collecting gradient
After Everything
/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/pruner/hf_llama_pruner.py:265: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)
  salience = layer.weight * layer.weight.grad
Traceback (most recent call last):
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 188, in <module>
    main(args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/hf_prune.py", line 127, in main
    distribution =compute_single(logger,dataset_list,args)
  File "/home/bhandk/MLNeuron/LLM-Pruner/compute_single_dist.py", line 345, in compute_single
    pruner.step()
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/pruner/algorithms/metapruner.py", line 186, in step
    for group in self.prune_local():
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/pruner/algorithms/metapruner.py", line 245, in prune_local
    imp = self.estimate_importance(group, ch_groups=ch_groups, consecutive_groups=consecutive_groups)
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/torch_pruning/pruner/algorithms/metapruner.py", line 190, in estimate_importance
    return self.importance(group, ch_groups=ch_groups, consecutive_groups=consecutive_groups)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/bhandk/MLNeuron/LLM-Pruner/LLMPruner/pruner/hf_llama_pruner.py", line 265, in __call__
    salience = layer.weight * layer.weight.grad
TypeError: unsupported operand type(s) for *: 'Tensor' and 'NoneType'
[2024-01-16 14:36:19,724] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 585407) of binary: /home/bhandk/miniconda3/envs/MLNeuron/bin/python
Traceback (most recent call last):
  File "/home/bhandk/miniconda3/envs/MLNeuron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
hf_prune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-01-16_14:36:19
  host      : netsci2
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 585408)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-01-16_14:36:19
  host      : netsci2
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 585407)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
