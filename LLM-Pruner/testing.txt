Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [02:21<04:42, 141.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<00:59, 59.01s/it] Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 32.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.88s/it]
Parent ID | PID: 749918|
******************************
Loading Dataset
******************************
Generating Samples
******************************
Prepare Validation Dataset
Rank: 0 | PID: 749918|
Traceback (most recent call last):
  File "/home/bhandk/MLNeuron/LLM-Pruner/finetune_large.py", line 259, in <module>
    model = get_gradient(rank, world_size, model,example_prompts)
  File "/home/bhandk/MLNeuron/LLM-Pruner/finetune_large.py", line 180, in get_gradient
    model = FSDP(model,
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 463, in __init__
    _auto_wrap(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 537, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 537, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 537, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 555, in _recursive_wrap
    return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 484, in _wrap
    return wrapper_cls(module, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.18 GiB. GPU 0 has a total capacty of 44.38 GiB of which 894.62 MiB is free. Process 749045 has 1.12 GiB memory in use. Including non-PyTorch memory, this process has 42.39 GiB memory in use. Of the allocated memory 41.56 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-01-18 20:46:44,907] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 749918) of binary: /home/bhandk/miniconda3/envs/MLNeuron/bin/python
Traceback (most recent call last):
  File "/home/bhandk/miniconda3/envs/MLNeuron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bhandk/miniconda3/envs/MLNeuron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
finetune_large.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-01-18_20:46:44
  host      : netsci2
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 749918)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
