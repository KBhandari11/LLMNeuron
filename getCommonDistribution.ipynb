{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import f\n",
    "from matplotlib.colors import LogNorm, BoundaryNorm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import cm\n",
    "sns.set(font=\"Times New Roman\")\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] =12 \n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'Times New Roman'\n",
    "})\n",
    "#import itertools\n",
    "def l_0_norm(vector):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        for sub_element in element:\n",
    "            if sub_element != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "def take_average(dict):\n",
    "    data = dict[\"0\"]\n",
    "    iterations_block = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    iterations_channel = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    #for style , iterations in zip ([\"channel\",\"channel\",\"block_random\",\"channel_random\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "    for style , iterations in zip ([\"block\",\"channel\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "        for iter in iterations:\n",
    "            if iter == \"0\":\n",
    "                continue\n",
    "            for ratio in dict[iter][style]:\n",
    "                for dataset in dict[iter][style][ratio]:\n",
    "                    for norm in dict[iter][style][ratio][dataset]:\n",
    "                        value = np.array(dict[iter][style][ratio][dataset][norm])\n",
    "                        data[style][ratio][dataset][norm]= (np.array(data[style][ratio][dataset][norm])+value)\n",
    "                        if iter == iterations[-1]:\n",
    "                            data[style][ratio][dataset][norm] = data[style][ratio][dataset][norm]/len(iterations)\n",
    "\n",
    "    return data\n",
    "def strip(name):\n",
    "    name = name.split(\"/\")[-1]\n",
    "    name = name.split(\"_\")[0]\n",
    "    return name \n",
    "\n",
    "\n",
    "def loop_over(dict):\n",
    "    if isinstance(dict, list):\n",
    "        print(\"end\")\n",
    "    else: \n",
    "        print(dict.keys())\n",
    "        for keys in dict:\n",
    "            loop_over(dict[keys])\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append(subset)\n",
    "    return dataname\n",
    "\n",
    "def get_community_dict(dataset_list, dataset_community):\n",
    "    community_dict = {}\n",
    "    for dataset in dataset_list:\n",
    "        for comm in dataset_community:\n",
    "            if dataset in list(dataset_community[comm].keys()):\n",
    "                break\n",
    "        community_dict[dataset] = int(comm)\n",
    "    return community_dict\n",
    "\n",
    "def get_cluster_dict(dataset_list, model_cluster):\n",
    "    cluster_dict = {}\n",
    "    for dataset in dataset_list:\n",
    "        for clust in model_cluster:\n",
    "            if dataset in list(model_cluster[clust]):\n",
    "                break\n",
    "        cluster_dict[dataset] = int(clust)\n",
    "    return cluster_dict\n",
    "\n",
    "#Dataset List\n",
    "with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        dataset_list = json.load(openfile)\n",
    "#Original Distribution\n",
    "with open(\"result/original_distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    vicuna_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_original = json.load(openfile)\n",
    "#Pruned Distribution\n",
    "with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    vicuna_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_distribution= json.load(openfile)\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_community= json.load(openfile)\n",
    "dataset_list = get_dataset_list(dataset_list)\n",
    "llama_distribution = take_average(llama_distribution)\n",
    "vicuna_distribution = take_average(vicuna_distribution)\n",
    "llama_chat_distribution = take_average(llama_chat_distribution)\n",
    "community_dict = get_community_dict(dataset_list, dataset_community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_accuracy(distribution_list,original_distribution_list,models_list,pruner_style=\"block\", ratio_list=[\"15\"]):\n",
    "    data = {\"Accuracy\":[] ,\"model\":[], \"ratio\":[]}\n",
    "    data[\"Accuracy\"] = []\n",
    "    for model_idx, model in enumerate(models_list):\n",
    "        for pruner_ratio in ratio_list:\n",
    "            for node in original_distribution_list[model_idx]:\n",
    "                node= node.split(\"/\")[-1]\n",
    "                if node in [\"distribution\"]:\n",
    "                    continue\n",
    "                pruned_acc = original_distribution_list[model_idx][node][-1] * 100\n",
    "                data[\"Accuracy\"].append(pruned_acc)\n",
    "                data[\"ratio\"].append(f\"original\" )\n",
    "                data[\"model\"].append(f\"{model}\")\n",
    "\n",
    "    for model_idx, model in enumerate(models_list):\n",
    "        for pruner_ratio in ratio_list:\n",
    "            if pruner_ratio not in distribution_list[model_idx][pruner_style].keys():\n",
    "                continue\n",
    "            for node in distribution_list[model_idx][pruner_style][pruner_ratio]:\n",
    "                node= node.split(\"/\")[-1]\n",
    "                pruned_acc = distribution_list[model_idx][pruner_style][pruner_ratio][node][\"Accuracy\"][-1] * 100\n",
    "                data[\"Accuracy\"].append(pruned_acc)\n",
    "                data[\"ratio\"].append(f\"{pruner_ratio}%\" )\n",
    "                data[\"model\"].append(f\"{model}\")\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #sns.set_theme(style=\"ticks\", palette=\"pastel\")  \n",
    "    sns.set_theme(style=\"ticks\", palette=\"husl\")  \n",
    "    #sns.boxplot(x=\"model\", y=\"accuracy\", hue=\"ratio\",hue_order=[\"original\"]+[f\"{pruner_ratio}%\" for pruner_ratio in ratio_list],data=data, width=.5)\n",
    "    #sns.boxplot(x=\"ratio\", y=\"accuracy\", hue=\"model\",hue_order=models_list ,data=data, width=.5)\n",
    "    g =sns.lineplot(x=\"ratio\", y=\"Accuracy\", hue=\"model\",hue_order=models_list ,data=data)\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    g.legend_.set_title(None)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Pruning Sparsity Ratio (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./figure/accuracy_{pruner_style}.pdf\",dpi=300)\n",
    "    plt.suptitle(f\"Pruning Strategy: {pruner_style}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc_based_community(distribution,original_distribution,plot_title,pruner_style=\"block\", ratio_list=[\"15\"]):\n",
    "    with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        cluster_dataset_list = json.load(openfile)\n",
    "    data = {\"accuracy\":[] ,\"ratio\":[], \"cognitive category\":[]}\n",
    "    data[\"accuracy\"] = []\n",
    "    #label= [\"spatial\\ncognition\", \"social\\ncognition\", \"mathematical\\ncognition\", \"linguistic\\ncognition\", \"logical reasoning\\ncogntition\", \"random\"]\n",
    "    for comm_idx, comm in enumerate(cluster_dataset_list):\n",
    "        for idx, pruner_ratio in enumerate(ratio_list):\n",
    "            if pruner_ratio not in distribution[pruner_style].keys():\n",
    "                continue\n",
    "            for node in cluster_dataset_list[comm]:\n",
    "                node= node.split(\"/\")[-1]\n",
    "                pruned_acc = distribution[pruner_style][pruner_ratio][node][\"Accuracy\"][-1]\n",
    "                data[\"accuracy\"].append(pruned_acc)\n",
    "                data[\"ratio\"].append(f\"{pruner_ratio}%\" )\n",
    "                data[\"cognitive category\"].append(f\"{comm_idx}\")\n",
    "                if idx == 0:\n",
    "                    org_comm_acc = (original_distribution[node][-1])\n",
    "                    data[\"accuracy\"].append(org_comm_acc)\n",
    "                    data[\"ratio\"].append(\"original\")\n",
    "                    data[\"cognitive category\"].append(f\"{comm_idx}\")\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.set_theme(style=\"ticks\", palette=\"pastel\")  \n",
    "    g = sns.boxplot(x=\"cognitive category\", y=\"accuracy\", hue=\"ratio\",hue_order=[\"original\"]+[f\"{pruner_ratio}%\" for pruner_ratio in ratio_list],data=data, width=.5)\n",
    "    g.legend_.set_title(None)\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    \n",
    "    plt.suptitle(f\"Model: {plot_title} | Pruning Strategy: {pruner_style}\")\n",
    "    plt.show()\n",
    "#plot_acc_based_community(llama_distribution,llama_original,plot_title = \"Llama-7b\", pruner_style=\"block\", ratio_list=[\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"])\n",
    "#plot_acc_based_community(vicuna_distribution,vicuna_original,plot_title = \"Vicuna-7b\",pruner_style=\"block\", ratio_list=[\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"])\n",
    "#plot_acc_based_community(llama_chat_distribution,llama_chat_original,plot_title = \"Llama-7b-chat\",pruner_style=\"block\", ratio_list=[\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"])\n",
    "distribution_list= [llama_distribution,vicuna_distribution,llama_chat_distribution]\n",
    "original_distribution_list= [llama_original,vicuna_original,llama_chat_original]\n",
    "models_list=[\"Llama-7b\",\"Vicuna-7b\", \"Llama-7b-chat\"]\n",
    "plot_accuracy(distribution_list,original_distribution_list,models_list,pruner_style=\"block\",ratio_list=[\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"])\n",
    "plot_accuracy(distribution_list,original_distribution_list,models_list,pruner_style=\"channel\",ratio_list=[\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and t-SNE comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Flatten to a single layer and use KMeans for cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def visualize(number_cluster, all_data,X_pca,clusters_pca,random_pca=[], style=\"channel\", ratio=\"15\"):\n",
    "        cmap = plt.cm.tab20  # define the colormap\n",
    "        # extract all colors from the .jet map\n",
    "        cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "        # force the first color entry to be grey\n",
    "        cmaplist[0] = (.5, .5, .5, 1.0)\n",
    "\n",
    "        # create the new map\n",
    "        cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "            'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "        # define the bins and normalize\n",
    "        bounds = np.linspace(0, number_cluster,number_cluster+1)\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "\n",
    "\n",
    "        indexes_pca = [(x,np.where(clusters_pca == x)[0]) for x in set(clusters_pca)]\n",
    "        lw = 2\n",
    "        figure, axis = plt.subplots(figsize=(8, 6))\n",
    "        if len(random_pca) != 0:\n",
    "            axis.scatter(random_pca[:, 0],  random_pca[:, 1], color = \"black\",label=\"Random\")\n",
    "        axis.scatter(X_pca[:, 0],  X_pca[:, 1], c=clusters_pca,\n",
    "                  cmap=cmap, norm=norm,alpha=.8, lw=lw)\n",
    "        #axis[1].scatter(X_tsne[1:, 0],X_tsne[1:, 1], c=clusters_tsne[1:],cmap=cmap, norm=norm,alpha=.8, lw=lw)\n",
    "        \n",
    "        #ax2 = figure.add_axes([0.95, 0.1, 0.03, 0.8])\n",
    "        ax2 = figure.add_axes([0.99, 0.1, 0.03, 0.8])\n",
    "        cb = mpl.colorbar.ColorbarBase(ax2, cmap=cmap, norm=norm, spacing='proportional', ticks=[b+0.5 for b in bounds], boundaries=bounds, format='%1i')\n",
    "\n",
    "        axis.set_title(f\"PCA\")\n",
    "\n",
    "        figure.suptitle(f\"Strategy: {style} | Ratio: {ratio} | Cluster: {number_cluster}\")\n",
    "        for cluster, index in indexes_pca:\n",
    "            axis.annotate(all_data[index[0]], (X_pca[index[0],0], X_pca[index[0],1]))\n",
    "            try:\n",
    "                axis.annotate(all_data[index[1]], (X_pca[index[1],0], X_pca[index[1],1]))\n",
    "            except:\n",
    "                continue\n",
    "        axis.axhline(0)\n",
    "        axis.axvline(0)\n",
    "        axis.legend()\n",
    "        cluster= {}\n",
    "        for cluster_name, indexes  in indexes_pca:\n",
    "            cluster[cluster_name] = [all_data[i] for i in indexes]\n",
    "        plt.show()\n",
    "        return cluster'''\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize(Kmean, all_data,X_pca,clusters_pca,random_pca=[], style=\"channel\", ratio=\"15\", plot=False, original =False, random =False):\n",
    "        data = {\"pca1\":[],\"pca2\":[],\"cluster\":[],\"dataset_name\":[]}\n",
    "        extra = [ ]\n",
    "        for idx, (pca, cluster) in enumerate(zip(X_pca,clusters_pca)):\n",
    "            if original and idx == 0:\n",
    "                data[\"pca1\"].append(pca[0])\n",
    "                data[\"pca2\"].append(pca[1])\n",
    "                data[\"cluster\"].append(\"original\")\n",
    "                data[\"dataset_name\"].append(\"original\")\n",
    "                extra.append(\"original\")\n",
    "            else:\n",
    "                data[\"pca1\"].append(pca[0])\n",
    "                data[\"pca2\"].append(pca[1])\n",
    "                data[\"cluster\"].append(cluster)\n",
    "                data[\"dataset_name\"].append(all_data[idx])\n",
    "        if random:\n",
    "             for idx, pca in enumerate(random_pca):\n",
    "                data[\"pca1\"].append(pca[0])\n",
    "                data[\"pca2\"].append(pca[1])\n",
    "                data[\"cluster\"].append(\"random\")\n",
    "                data[\"dataset_name\"].append(all_data[idx])\n",
    "                extra.append(\"random\")\n",
    "        indexes_pca = [(x,np.where(clusters_pca == x)[0]) for x in set(clusters_pca)]\n",
    "        lw = 2\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.scatterplot(data=data, x=\"pca1\", y=\"pca2\", hue=\"cluster\",hue_order=list(set(clusters_pca))+list(set(extra)), palette=\"deep\", style=\"cluster\")\n",
    "            #plt.title(f\"Strategy: {style} | Ratio: {ratio}% | Cluster: {number_cluster}\")\n",
    "            centers = Kmean.cluster_centers_\n",
    "            plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.6, edgecolors='w')\n",
    "            def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "                    ax = ax or plt.gca()\n",
    "                    if covariance.shape == (2, 2):\n",
    "                        U, s, Vt = np.linalg.svd(covariance)\n",
    "                        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "                        width, height = 2 * np.sqrt(s)\n",
    "                    else:\n",
    "                        angle = 0\n",
    "                        width, height = 2 * np.sqrt(covariance)\n",
    "                    \n",
    "                    for nsig in range(1, 4):\n",
    "                        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))\n",
    "            #for i, mean in enumerate(Kmean.cluster_centers_):\n",
    "            #    cov = np.cov(data[data['cluster'] == i][['pca1', 'pca2']].T)\n",
    "            #    draw_ellipse(mean, cov, alpha=0.2, color='k')\n",
    "\n",
    "            \n",
    "\n",
    "            plt.grid()\n",
    "            plt.axhline(0, color='black')\n",
    "            plt.axvline(0, color='black')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        cluster= {}\n",
    "        for cluster_name, indexes  in indexes_pca:\n",
    "            cluster[cluster_name] = [all_data[i] for i in indexes]\n",
    "        return cluster, pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hotelling.stats import hotelling_t2\n",
    "\n",
    "def hotelling_t2_test(data_A, data_B):\n",
    "    # Calculate mean and covariance matrices for each dataset\n",
    "    mean_A = np.mean(data_A, axis=0)\n",
    "    mean_B = np.mean(data_B, axis=0)\n",
    "\n",
    "    cov_A = np.cov(data_A, rowvar=False)\n",
    "    cov_B = np.cov(data_B, rowvar=False)\n",
    "\n",
    "    # Number of samples in each dataset\n",
    "    n_A = len(data_A)\n",
    "    n_B = len(data_B)\n",
    "\n",
    "    # Pooled covariance matrix\n",
    "    pooled_cov = ((n_A - 1) * cov_A + (n_B - 1) * cov_B) / (n_A + n_B - 2)\n",
    "\n",
    "    # Calculate the T-squared statistic\n",
    "    t_squared = n_A * n_B / (n_A + n_B) * np.dot(np.dot((mean_A - mean_B).T, np.linalg.inv(pooled_cov)), (mean_A - mean_B))\n",
    "\n",
    "    # Degrees of freedom for the F-distribution\n",
    "    df1 = data_A.shape[1]\n",
    "    df2 = n_A + n_B - data_A.shape[1] - 1\n",
    "\n",
    "    # p-value from the F-distribution\n",
    "    p_value = 1 - f.cdf(t_squared * (n_A + n_B - 2) / (n_A * df1), df1, df2)\n",
    "\n",
    "    return t_squared, p_value\n",
    "\n",
    "def masking(test_list, item):\n",
    "    res_list = []\n",
    "    for i in range(0, len(test_list)):\n",
    "        if test_list[i] == item:\n",
    "            res_list.append(True)\n",
    "        else:\n",
    "            res_list.append(False)\n",
    "    return res_list\n",
    "\n",
    "def hotelling_t2_test_clusters(X, clusters):\n",
    "    # Identify unique clusters\n",
    "    unique_clusters = list(set(clusters))\n",
    "    # Perform Hotelling's T-squared test for each pair of clusters\n",
    "    data = np.zeros((len(unique_clusters),len(unique_clusters)))\n",
    "    save_p_value = []\n",
    "    save_mean_value = []\n",
    "    for idx_i, i in enumerate(unique_clusters):\n",
    "        p_value_list = []\n",
    "        mean_value_list = []\n",
    "        for idx_j, j in enumerate(unique_clusters):\n",
    "            if idx_j > idx_i :\n",
    "                p_value_list.append(0)\n",
    "                mean_value_list.append(0)\n",
    "                continue\n",
    "            # Extract data for each cluster\n",
    "            \n",
    "            cluster_i_data = X[masking(clusters,i)]\n",
    "            cluster_j_data = X[masking(clusters,j)]\n",
    "            t_squared_stat, f_value, p_value, _ = hotelling_t2(cluster_i_data, cluster_j_data)\n",
    "            p_value_list.append(p_value)\n",
    "            mean_value_list.append(t_squared_stat)\n",
    "        save_p_value.append(p_value_list)\n",
    "        save_mean_value.append(mean_value_list)\n",
    "\n",
    "    figure, axis = plt.subplots(figsize=(12, 8), ncols=2)\n",
    "    p_value = np.array(save_p_value)\n",
    "    mean_value = np.array(save_mean_value)\n",
    "    mask = np.triu(np.ones_like(p_value))#-np.eye(p_value.shape[0],p_value.shape[1])\n",
    "    # using the upper triangle matrix as mask \n",
    "    sns.heatmap(p_value, annot=True, fmt='.1e' , ax=axis[1],xticklabels=unique_clusters, yticklabels=unique_clusters,mask=mask)\n",
    "    axis[1].set_title(\"P-value\")\n",
    "    sns.heatmap(mean_value, annot=True, fmt='.1e' , ax=axis[0],xticklabels=unique_clusters, yticklabels=unique_clusters,mask=mask)\n",
    "    axis[0].set_title(\"Statistics\")\n",
    "    figure.suptitle(f\"Hotelling's T-squared Statistic\")\n",
    "\n",
    "    plt.plot()\n",
    "    return p_value, mean_value, unique_clusters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "def k_means_and_pca(X,n_cluster,do_tranform = False):\n",
    "        if do_tranform:\n",
    "            # Standardize the feature matrix (mean center and scale)\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = X #X#scaler.fit_transform(X)\n",
    "        # Perform PCA\n",
    "        n_components = 2  # Number of components to keep\n",
    "        kmeans = KMeans(n_init=\"auto\",n_clusters=n_cluster, random_state=0)  # KMEANS\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        #Random_pca = pca.fit_transform(Random)\n",
    "        clusters_pca = kmeans.fit_predict(X_pca)\n",
    "        return X_pca, clusters_pca, kmeans\n",
    "\n",
    "def k_means_and_spectral_clustering(X,n_cluster,do_tranform = False):\n",
    "        if do_tranform:\n",
    "            # Standardize the feature matrix (mean center and scale)\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = X #X#scaler.fit_transform(X)\n",
    "        # Perform PCA\n",
    "        n_components = 2  # Number of components to keep\n",
    "        sc = SpectralClustering(n_clusters=n_cluster, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        clusters_sc = sc.fit_predict(X_pca)\n",
    "        #Random_pca = pca.fit_transform(Random)\n",
    "        return X_pca, clusters_sc\n",
    "\n",
    "def plot_cluster_distribution(model_label,llm_pruner, original_distribution, dataset_list, ratio=\"15\",norm = \"|W|_0\" ,style=\"channel\",add_original=False,add_random = False, plot=False,hotelling_test=False):\n",
    "    layers = np.arange(0,32,1)[3:31]\n",
    "    \n",
    "    #for module in [\"|W|_0\",\"|W|_F\"]:\n",
    "    best_idx_pca = 10000 \n",
    "    store_pca= []\n",
    "    store_cluster= []\n",
    "    best_clusters_pca = None\n",
    "    best_X_pca = None\n",
    "    best_cluster_size = None\n",
    "    \n",
    "    for NUM_CLUSTER in [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]:\n",
    "        #for NUM_CLUSTER in [3]:\n",
    "        original = np.array(original_distribution[\"distribution\"][norm])[layers,:]\n",
    "        label = [ ]\n",
    "        label_idx = []\n",
    "        Random = []\n",
    "        for idx, dataset_name in enumerate(dataset_list):\n",
    "            dataset = dataset_name.split('/')[-1]\n",
    "            label.append(dataset)\n",
    "            label_idx.append(idx)\n",
    "            if idx == 0:\n",
    "                if add_original:\n",
    "                    X = original.flatten().reshape(-1,1).T\n",
    "                else:\n",
    "                    X_new = (np.array(llm_pruner[style][ratio][dataset][norm])[layers,:])\n",
    "                    X =  X_new.flatten().reshape(-1,1).T\n",
    "                    if add_random:\n",
    "                        X_new =  np.array(llm_pruner[style+\"_random\"][ratio][dataset][norm])[layers,:]\n",
    "                        Random = X_new.flatten().reshape(-1,1).T\n",
    "                continue\n",
    "            try:\n",
    "                X_new =  np.array(llm_pruner[style][ratio][dataset][norm])[layers,:]\n",
    "            except:\n",
    "                print([style],[ratio],[dataset],[norm])\n",
    "                X_new =  np.array(llm_pruner[style][ratio][dataset][norm])[layers,:]\n",
    "            X_new = X_new.flatten().reshape(-1,1).T\n",
    "            X = np.append(X, X_new, axis=0)\n",
    "            if add_random:\n",
    "                if len(Random) != 0:\n",
    "                    X_new =  np.array(llm_pruner[style+\"_random\"][ratio][dataset][norm])[layers,:].flatten().reshape(-1,1).T\n",
    "                    Random = np.append(Random, X_new, axis=0)\n",
    "                else:\n",
    "                    Random =  np.array(llm_pruner[style+\"_random\"][ratio][dataset][norm])[layers,:].flatten().reshape(-1,1).T\n",
    "        if add_random:\n",
    "            dataset_list_label = dataset_list +[\"random\"] * Random.shape[0]\n",
    "            X = np.append(X, Random, axis=0)\n",
    "        \n",
    "        X_pca, clusters_pca, kmeans = k_means_and_pca(X,NUM_CLUSTER)\n",
    "        db_index_pca = davies_bouldin_score(X_pca, clusters_pca)\n",
    "        store_pca.append(db_index_pca)\n",
    "        store_cluster.append(NUM_CLUSTER)\n",
    "        \n",
    "        if db_index_pca < best_idx_pca:\n",
    "            best_idx_pca = db_index_pca  \n",
    "            best_X_pca = X_pca \n",
    "            best_clusters_pca =  clusters_pca\n",
    "            best_cluster_size = NUM_CLUSTER           \n",
    "            optimal_kmean = kmeans\n",
    "            \n",
    "    if plot:\n",
    "        #plt.figure(figsize=(12, 10))\n",
    "        df = pd.DataFrame.from_dict({\"num_cluster\":store_cluster,\"davies_bouldin_score\":store_pca})\n",
    "        df.to_csv(f\"result/kmeans_clustering_hotellings/{model_label}_{style}_{ratio}_davies_bouldin_score.csv\",  index=False)\n",
    "        plt.plot(store_cluster,store_pca,color=\"blue\")\n",
    "        plt.scatter(best_cluster_size,best_idx_pca,marker=\"o\",label=\"Optimal\",color=\"red\")\n",
    "        plt.xlabel(\"Number of Cluster\")\n",
    "        plt.ylabel(\"Davies Bouldin Score\")\n",
    "        #plt.title(\"Finding the Optimal Number of Cluster\")\n",
    "        plt.legend()\n",
    "        plt.plot()\n",
    "        print(\"=\"*1000)  \n",
    "    if hotelling_test:\n",
    "        print(\"Computing statistical testing\")\n",
    "        #t_squared_stat, p_value = hotelling_t2_test(best_X_pca[:len(dataset_list_label),:], best_X_pca[len(dataset_list_label):-1,:])\n",
    "        if add_random:\n",
    "            best_clusters_pca = list(best_clusters_pca)\n",
    "            for i in range(len(Random),len(dataset_list_label)):\n",
    "                best_clusters_pca[i] = \"random\"\n",
    "\n",
    "        if add_original:\n",
    "            best_clusters_pca = list(best_clusters_pca)\n",
    "            best_clusters_pca[0] = \"original\"\n",
    "        p_value, stats, uniq = hotelling_t2_test_clusters(best_X_pca, best_clusters_pca)\n",
    "        for hotelling_test_result, hotelling_label in zip([p_value, stats],[\"p_value\", \"stats\"]):\n",
    "            df = pd.DataFrame(hotelling_test_result, index=uniq, columns=uniq)\n",
    "            rand_label = \"_rand\" if add_random else \"\"\n",
    "            df.to_csv(f\"result/kmeans_clustering_hotellings/{model_label}_{style}_{ratio}{rand_label}_{hotelling_label}.csv\",index=False)\n",
    "        #hotelling_t2_test_clusters(best_X_pca, np.append(best_clusters_pca,np.array([max(best_clusters_pca)+1]*len(Random))))\n",
    "        #print(\"Hotelling's T-squared Statistic:\", t_squared_stat)\n",
    "        #print(\"P-value:\", p_value)\n",
    "    if add_random:\n",
    "        cluster,pd_data = visualize(optimal_kmean, dataset_list_label,best_X_pca[:len(dataset_list)-1,:],best_clusters_pca, random_pca=best_X_pca[len(dataset_list)-1:-1,:], style=style, ratio=ratio, plot=plot,original=add_original,random = add_random)\n",
    "        rand_label = \"_rand\" \n",
    "    else:\n",
    "        cluster,pd_data = visualize(optimal_kmean, dataset_list,best_X_pca[:len(dataset_list),:],best_clusters_pca, style=style, ratio=ratio, plot=plot,original=add_original,random = add_random)\n",
    "        rand_label = \"\" \n",
    "    pd_data.to_csv(f\"result/kmeans_clustering_hotellings/{model_label}_{style}_{ratio}{rand_label}_clustering.csv\",index=False)\n",
    "    return cluster\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cluster = plot_cluster_distribution(\"llama\",llama_distribution, llama_original,dataset_list,ratio=\"20\",style=\"block\",add_original=False,add_random=True,plot=True,hotelling_test=True)\n",
    "model_cluster = plot_cluster_distribution(\"llama_chat\",llama_chat_distribution, llama_chat_original,dataset_list,ratio=\"15\",style=\"block\",add_original=False,add_random=True,plot=True,hotelling_test=True)\n",
    "model_cluster = plot_cluster_distribution(\"vicuna\",vicuna_distribution, vicuna_original,dataset_list,ratio=\"15\",style=\"block\",add_original=False,add_random=True,plot=True,hotelling_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import rand_score, jaccard_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "plt.clf()\n",
    "pruning_strategy = \"block\"\n",
    "value_list =[]\n",
    "value_list_sklearn_ars =[]\n",
    "value_list_sklearn_jaccard_index =[]\n",
    "value_list_sklearn_fms =[]\n",
    "value_list_sklearn_nmi =[]\n",
    "ratio_list = [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]\n",
    "for ratio in ratio_list:\n",
    "    model_cluster = plot_cluster_distribution(llama_distribution, llama_original,dataset_list,ratio=ratio,style=pruning_strategy,add_original=False)\n",
    "    model_dict = get_cluster_dict(dataset_list,model_cluster)\n",
    "    #value = nmi(dataset_list, model_dict, community_dict)\n",
    "    #value_list.append(value)\n",
    "    value_list_sklearn_ars.append(rand_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_jaccard_index.append(jaccard_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list],average='weighted'))\n",
    "    value_list_sklearn_fms.append(fowlkes_mallows_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_nmi.append(normalized_mutual_info_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_jaccard_index, color=\"green\", label=\"rand_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_fms, color=\"orange\", label=\"fowlkes_mallows_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_nmi, color=\"red\", label=\"normalized_mutual_info_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_ars, color=\"blue\", label=\"rand_score\")\n",
    "plt.ylabel(\"rand_score\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Model Pruning Ratio\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(f\"Llama 7b | {pruning_strategy} Strategy\")\n",
    "plt.show()\n",
    "print(\"=\"*100)\n",
    "value_list_sklearn_nmi =[]\n",
    "value_list_sklearn_jaccard_index =[]\n",
    "value_list_sklearn_fms =[]\n",
    "value_list_sklearn_ars =[]\n",
    "for ratio in ratio_list:\n",
    "    model_cluster = plot_cluster_distribution(vicuna_distribution, vicuna_original,dataset_list,ratio=ratio,style=pruning_strategy,add_original=False)\n",
    "    model_dict = get_cluster_dict(dataset_list,model_cluster)\n",
    "    #value = nmi(dataset_list, model_dict, community_dict)\n",
    "    #value_list.append(value)\n",
    "    value_list_sklearn_ars.append(rand_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_jaccard_index.append(jaccard_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list],average='weighted'))\n",
    "    value_list_sklearn_fms.append(fowlkes_mallows_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_nmi.append(normalized_mutual_info_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_jaccard_index, color=\"green\", label=\"rand_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_fms, color=\"orange\", label=\"fowlkes_mallows_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_nmi, color=\"red\", label=\"normalized_mutual_info_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_ars, color=\"blue\", label=\"rand_score\")\n",
    "plt.ylabel(\"rand_score\")\n",
    "plt.xlabel(\"Model Pruning Ratio\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(f\"Vicuna 7b | {pruning_strategy} Strategy\")\n",
    "plt.show()\n",
    "print(\"=\"*100)\n",
    "value_list_sklearn_nmi =[]\n",
    "value_list_sklearn_jaccard_index =[]\n",
    "value_list_sklearn_fms =[]\n",
    "value_list_sklearn_ars =[]\n",
    "for ratio in ratio_list:\n",
    "    model_cluster = plot_cluster_distribution(llama_chat_distribution, llama_chat_original,dataset_list,ratio=ratio,style=pruning_strategy,add_original=False)\n",
    "    model_dict = get_cluster_dict(dataset_list,model_cluster)\n",
    "    #value = nmi(dataset_list, model_dict, community_dict)\n",
    "    #value_list.append(value)\n",
    "    value_list_sklearn_ars.append(rand_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_jaccard_index.append(jaccard_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list],average='weighted'))\n",
    "    value_list_sklearn_fms.append(fowlkes_mallows_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "    value_list_sklearn_nmi.append(normalized_mutual_info_score([ community_dict[dataset] for dataset in dataset_list], [ model_dict[dataset] for dataset in dataset_list]))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_jaccard_index, color=\"green\", label=\"rand_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_fms, color=\"orange\", label=\"fowlkes_mallows_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_nmi, color=\"red\", label=\"normalized_mutual_info_score\")\n",
    "#plt.plot([f\"{ratio}%\" for ratio in ratio_list],value_list_sklearn_ars, color=\"blue\", label=\"rand_score\")\n",
    "plt.ylabel(\"rand_score\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Model Pruning Ratio\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(f\"Llama Chat 7b | {pruning_strategy} Strategy\")\n",
    "plt.show()\n",
    "print(\"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNeuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
