{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from datasets import load_dataset \n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np \n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def get_dataset(dataset_name, dataset_info_list):\n",
    "    if isinstance(dataset_name,list):\n",
    "        if dataset_name[0] == \"tasksource/mmlu\":\n",
    "            try:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"test[0:100]\", num_proc=8) \n",
    "            except:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"test\", num_proc=8) \n",
    "        elif dataset_name[0] == \"tasksource/bigbench\":\n",
    "            try:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"train[0:100]\", num_proc=8,trust_remote_code=True) \n",
    "            except:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"train\", num_proc=8,trust_remote_code=True) \n",
    "    else:\n",
    "        if dataset_name == \"EleutherAI/truthful_qa_mc\":\n",
    "            traindata = load_dataset(dataset_name, split=\"validation[0:100]\", num_proc=8) \n",
    "        else:\n",
    "            traindata = load_dataset(dataset_name, split=\"train[0:100]\", num_proc=8) \n",
    "    if isinstance(dataset_name,list):\n",
    "        key = dataset_info_list[dataset_name[0]][\"keys\"]\n",
    "    else:\n",
    "        key = dataset_info_list[dataset_name][\"keys\"]\n",
    "    return traindata[key[0]]\n",
    "\n",
    "def set_random_seed(seed=0):\n",
    "    #random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append([data,subset])\n",
    "    return dataname\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_info_list = json.load(openfile)\n",
    "dataset_name_list = get_dataset_list(dataset_info_list)\n",
    "questions = []\n",
    "questions_name = []\n",
    "for dataset_name in dataset_name_list:\n",
    "    data = get_dataset(dataset_name, dataset_info_list)\n",
    "    combinations = \" \".join(data)\n",
    "    questions.append(combinations)\n",
    "    if len(dataset_name) == 2:\n",
    "        questions_name.append(dataset_name[-1])\n",
    "    else:\n",
    "        questions_name.append(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Corpus and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.corpus import words\n",
    "\n",
    "def tokenize_text(text, name):\n",
    "    for val in [\":\",\",\",\"?\",\".\",\"!\",\"\\n\"]:\n",
    "            text = text.replace(val,\"\")\n",
    "    #if name in [\"entailed_polarity_hindi\",\"indic_cause_and_effect\",\"kannada\"]:\n",
    "    if text.split(\" \")[0] not in words.words():   \n",
    "        tokens = text.split(\" \")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token.isalnum() not in stop_words]\n",
    "        #print(name, end=\", \")\n",
    "        return tokens\n",
    "    # Tokenize using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    #tokens = [ps.stem(token) for token in tokens if token.isalnum() not in stop_words]\n",
    "    tokens = [token for token in tokens if token.isalnum() not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import copy\n",
    "questions_copy = copy.deepcopy(questions)\n",
    "for idx in range(len(questions)):\n",
    "    questions_copy[idx] = questions_copy[idx].lower()  \n",
    "    #questions_copy[idx] = tokenizer.tokenize(questions_copy[idx])  \n",
    "    questions_copy[idx] = tokenize_text(questions_copy[idx], questions_name[idx])\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "#questions = [[token for token in q if not token.isnumeric()] for q in questions]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "#questions = [[token for token in q if len(token) > 1] for q in questions]\n",
    "\n",
    "bigram = Phrases(questions_copy, min_count=20)\n",
    "for idx in range(len(questions_copy)):\n",
    "    for token in bigram[questions_copy[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            questions_copy[idx].append(token)\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(questions_copy)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 20% of the documents.\n",
    "#dictionary.filter_extremes(no_below=5, no_above=0.3)\n",
    "dictionary.filter_extremes(no_above=0.50)\n",
    "corpus = [dictionary.doc2bow(q) for q in questions_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Set training parameters.\n",
    "num_topics = 20\n",
    "#chunksize = 2000\n",
    "passes = 500\n",
    "iterations = 10000\n",
    "#eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    iterations=iterations,\n",
    "    random_state = 0\n",
    ")'''\n",
    "# Function to compute coherence values for different number of topics\n",
    "def compute_coherence_values(dictionary,texts, corpus, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    id2word = dictionary.id2token\n",
    "    optimal_model = None\n",
    "    best_coherece = 0\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            passes=500,\n",
    "            iterations=10000,\n",
    "            random_state=0\n",
    "        )\n",
    "        coherence_model = CoherenceModel(model=model,texts=texts,corpus=corpus,dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        if coherence_values[-1] > best_coherece:\n",
    "            best_coherece = coherence_values[-1]\n",
    "            optimal_model = model\n",
    "    return optimal_model, coherence_values\n",
    "\n",
    "# Set the range of topics to explore\n",
    "start, limit, step = 4, 5, 1\n",
    "#start, limit, step = 7, 8, 1\n",
    "#start, limit, step = 10, 11, 1\n",
    "\n",
    "# Get coherence values\n",
    "model, coherence_values = compute_coherence_values(dictionary=dictionary,texts=questions_copy, corpus=corpus, start=start, limit=limit, step=step)\n",
    "\n",
    "# Plotting\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values, label=\"coherence value\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number of topics\n",
    "optimal_num_topics = x[coherence_values.index(max(coherence_values))]\n",
    "print(f\"Optimal number of topics: {optimal_num_topics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Assign Topics to Datasets.\n",
    "topics = [model[doc] for doc in corpus]\n",
    "dominant  = lambda x: max(x, key=lambda item: item[1])[0]\n",
    "# Step 7: Categorize Datasets.\n",
    "dominant_topic = np.array([dominant(t) for t in topics])\n",
    "dominant_topic= dominant_topic.astype(int)\n",
    "# Visualization (Optional):\n",
    "# You can visualize the results using various libraries such as matplotlib or seaborn.\n",
    "#print(\"Dominant Topic: \", len(dominant_topic) ,dominant_topic)\n",
    "# Example: Bar chart of dominant topics\n",
    "value, topic_index, topic_counts = np.unique(dominant_topic, return_index=True,return_counts=True)\n",
    "plt.bar([v+1 for v in value], topic_counts)\n",
    "plt.xticks([v+1 for v in value])\n",
    "plt.xlabel('Dominant Topic')\n",
    "plt.ylabel('Number of Datasets')\n",
    "plt.title('Distribution of Dominant Topics in Datasets')\n",
    "plt.plot()\n",
    "'''for index, topic in model.show_topics(formatted=False, num_words= 30):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, [w[0] for w in topic]))'''\n",
    "questions_name = np.array(questions_name)\n",
    "for topic, count in zip(value,topic_counts):\n",
    "    indexes = [i for i in range(len(dominant_topic)) if dominant_topic[i] == topic]\n",
    "    print(\"TOPIC \",topic+1, \" count: \", count)\n",
    "    print(\"Topic Distribution: \", [topics[i] for i in indexes])\n",
    "    print(\"DATASET \", questions_name[indexes])\n",
    "    print(\"+\"*100)\n",
    "#plt.savefig(\"Test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "# Convert the gensim LDA model to a format compatible with pyLDAvis\n",
    "vis_data = gensimvis.prepare(model, corpus,dictionary=dictionary,sort_topics=False)\n",
    "#print(vis_data[1][vis_data[1][\"Category\"]!=\"Default\"])\n",
    "'''for term, freq, category,total in zip(vis_data[1]['Term'],vis_data[1]['Freq'],vis_data[1][\"Category\"],vis_data[1][\"Total\"]):\n",
    "    if category != \"Default\":\n",
    "        print(term, freq, category, total)'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.display(vis_data)\n",
    "#db_index_pca = davies_bouldin_score(vis_data[0]['x'], vis_data[0]['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data,\"visualization.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "for t in range(model.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "#from bokeh.plotting import figure, output_file, show\n",
    "#from bokeh.models import Label\n",
    "#from bokeh.io import output_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "#[model[doc] for doc in corpus]\n",
    "'''for i, row_list in enumerate(model[dictionary]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])'''\n",
    "for i, row_list in enumerate(corpus):\n",
    "    topic_weights.append([w for i, w in model[row_list]])\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights)#.fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "#arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "print(len(topic_num))\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plt.figure()\n",
    "plt.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "uq_topics, uq_idx = np.unique(topic_num, return_index=True)\n",
    "print(uq_idx)\n",
    "for topic_id, idx in zip(value,uq_idx):\n",
    "    print(topic_id,idx)\n",
    "    plt.scatter(x=tsne_lda[idx,0], y=tsne_lda[idx,1], color=mycolors[topic_num][idx], label=f\"Topic #{topic_id+1}: {questions_name[idx]}\")\n",
    "plt.title(\"t-SNE Clustering of {} LDA Topics\".format(optimal_num_topics))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''for topic, count in zip(value,topic_counts):\n",
    "    indexes = [i for i in range(len(dominant_topic)) if dominant_topic[i] == topic]\n",
    "    print(\"TOPIC \",topic+1, \" count: \", count)\n",
    "    print(\"Topic Distribution: \", [topics[i] for i in indexes])\n",
    "    print(\"DATASET \", questions_name[indexes])\n",
    "    print(\"+\"*100)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData(data, threshold):\n",
    "    from collections import Counter\n",
    "    uniquedata ={}\n",
    "    for d in data:\n",
    "        uniquedata[d] = np.unique(data[d])\n",
    "    # Combine all items into a single list of words\n",
    "    all_words = [word for skills_list in uniquedata.values() for word in skills_list]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Calculate the threshold for words to be removed (appearing in more than 90% of the data)\n",
    "    threshold = len(uniquedata) * threshold\n",
    "\n",
    "    # Identify common words to be removed\n",
    "    common_words = [word for word, count in word_counts.items() if count > threshold]\n",
    "\n",
    "    # Remove common words from each item\n",
    "    filtered_data = {key: [word for word in value if word not in common_words] for key, value in data.items()}\n",
    "    return filtered_data\n",
    "with open(\"result/dataCategory.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataCategory = json.load(openfile)\n",
    "total_skill= 0 \n",
    "for d in dataCategory:\n",
    "    total_skill += len(dataCategory[d])\n",
    "print(total_skill)\n",
    "dataCategory = filterData(dataCategory, 0.50)\n",
    "total_skill= 0 \n",
    "for d in dataCategory:\n",
    "    total_skill += len(dataCategory[d])\n",
    "print(total_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSkill(uniqueCategory, topic):\n",
    "    dict_of_lists = {}\n",
    "    for idx, comm in enumerate(topic):\n",
    "        print(comm)\n",
    "        dict_of_lists[comm] = {}\n",
    "        for node in topic[comm]:\n",
    "            items =  np.unique(np.array(uniqueCategory[node]))\n",
    "            frequency = [(x,uniqueCategory[node].count(x)) for x in items]\n",
    "            get_node = sorted(frequency,key=lambda x: x[1], reverse=True)[0:20]\n",
    "            get_node = [key[0] for key in get_node]\n",
    "            print(\"\\t\",node,\"\\t\",get_node)\n",
    "            dict_of_lists[comm][node] = get_node\n",
    "    return dict_of_lists\n",
    "def topSkills(dict_of_lists, top=10):\n",
    "    from collections import Counter\n",
    "\n",
    "    # Flatten the lists into a single list\n",
    "    all_elements = [item for sublist in dict_of_lists.values() for item in sublist]\n",
    "\n",
    "    # Count the occurrences of each element\n",
    "    element_counts = Counter(all_elements)\n",
    "\n",
    "    # Get the 10 most common elements\n",
    "    most_common_elements = element_counts.most_common(top)\n",
    "\n",
    "    '''print(\"10 Most Common Elements:\")\n",
    "    for element, count in most_common_elements:\n",
    "        print(f\"{element}: {count} occurrences\")'''\n",
    "    return most_common_elements\n",
    "questions_name = np.array(questions_name)\n",
    "topicDataset = {}\n",
    "for topic, count in zip(value,topic_counts):\n",
    "    indexes = [i for i in range(len(dominant_topic)) if dominant_topic[i] == topic]\n",
    "    topicDataset[topic+1] = list(questions_name[indexes])\n",
    "topicSkills = getSkill(dataCategory,topicDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicNumber in topicSkills:\n",
    "    skills = topSkills(topicSkills[topicNumber], 20)\n",
    "    print(topicNumber, [f\"({skill[0]} {skill[1]})\" for skill in skills])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
