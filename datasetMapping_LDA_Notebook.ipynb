{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from datasets import load_dataset \n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np \n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def get_dataset(dataset_name, dataset_info_list):\n",
    "    if isinstance(dataset_name,list):\n",
    "        if dataset_name[0] == \"tasksource/mmlu\":\n",
    "            try:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"test[0:100]\", num_proc=8) \n",
    "            except:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"test\", num_proc=8) \n",
    "        elif dataset_name[0] == \"tasksource/bigbench\":\n",
    "            try:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"train[0:100]\", num_proc=8) \n",
    "            except:\n",
    "                traindata = load_dataset(dataset_name[0],dataset_name[1], split=\"train\", num_proc=8) \n",
    "    else:\n",
    "        if dataset_name == \"EleutherAI/truthful_qa_mc\":\n",
    "            traindata = load_dataset(dataset_name, split=\"validation[0:100]\", num_proc=8) \n",
    "        else:\n",
    "            traindata = load_dataset(dataset_name, split=\"train[0:100]\", num_proc=8) \n",
    "    if isinstance(dataset_name,list):\n",
    "        key = dataset_info_list[dataset_name[0]][\"keys\"]\n",
    "    else:\n",
    "        key = dataset_info_list[dataset_name][\"keys\"]\n",
    "    return traindata[key[0]]\n",
    "\n",
    "def set_random_seed(seed=0):\n",
    "    #random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append([data,subset])\n",
    "    return dataname\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_info_list = json.load(openfile)\n",
    "dataset_name_list = get_dataset_list(dataset_info_list)\n",
    "questions = []\n",
    "questions_name = []\n",
    "for dataset_name in dataset_name_list:\n",
    "    data = get_dataset(dataset_name, dataset_info_list)\n",
    "    questions.append(\" \".join(data))\n",
    "    if len(dataset_name) == 2:\n",
    "        questions_name.append(dataset_name[-1])\n",
    "    else:\n",
    "        questions_name.append(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Corpus and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [ps.stem(token) for token in tokens if token.isalnum() not in stop_words]\n",
    "    #tokens = [token for token in tokens if token.isalnum() not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import copy\n",
    "\n",
    "new_copy = copy.deepcopy(questions)\n",
    "for idx in range(len(questions)):\n",
    "    new_copy[idx] = new_copy[idx].lower()  \n",
    "    #new_copy[idx] = tokenizer.tokenize(new_copy[idx])  \n",
    "    new_copy[idx] = tokenize_text(new_copy[idx])\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "#questions = [[token for token in q if not token.isnumeric()] for q in questions]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "#questions = [[token for token in q if len(token) > 1] for q in questions]\n",
    "\n",
    "bigram = Phrases(new_copy, min_count=30)\n",
    "for idx in range(len(new_copy)):\n",
    "    for token in bigram[new_copy[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            new_copy[idx].append(token)\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(new_copy)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(q) for q in new_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dictionary))\n",
    "print(dictionary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Set training parameters.\n",
    "num_topics = 20\n",
    "#chunksize = 2000\n",
    "passes = 500\n",
    "iterations = 10000\n",
    "#eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    iterations=iterations,\n",
    "    random_state = 0\n",
    ")'''\n",
    "# Function to compute coherence values for different number of topics\n",
    "def compute_coherence_values(dictionary,texts, corpus, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    id2word = dictionary.id2token\n",
    "    optimal_model = None\n",
    "    best_coherece = 0\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=num_topics,\n",
    "            passes=500,\n",
    "            iterations=10000,\n",
    "            random_state=0\n",
    "        )\n",
    "        coherence_model = CoherenceModel(model=model,texts=texts,corpus=corpus,dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        if coherence_values[-1] > best_coherece:\n",
    "            best_coherece = coherence_values[-1]\n",
    "            optimal_model = model\n",
    "    return optimal_model, coherence_values\n",
    "\n",
    "# Set the range of topics to explore\n",
    "start, limit, step = 2, 12, 1\n",
    "#start, limit, step = 10, 11, 1\n",
    "\n",
    "# Get coherence values\n",
    "model, coherence_values = compute_coherence_values(dictionary=dictionary,texts=new_copy, corpus=corpus, start=start, limit=limit, step=step)\n",
    "\n",
    "# Plotting\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values, label=\"coherence value\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number of topics\n",
    "optimal_num_topics = x[coherence_values.index(max(coherence_values))]\n",
    "print(f\"Optimal number of topics: {optimal_num_topics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Assign Topics to Datasets.\n",
    "topics = [model[doc] for doc in corpus]\n",
    "dominant  = lambda x: max(x, key=lambda item: item[1])[0]\n",
    "# Step 7: Categorize Datasets.\n",
    "dominant_topic = np.array([dominant(t) for t in topics])\n",
    "dominant_topic= dominant_topic.astype(int)\n",
    "# Visualization (Optional):\n",
    "# You can visualize the results using various libraries such as matplotlib or seaborn.\n",
    "#print(\"Dominant Topic: \", len(dominant_topic) ,dominant_topic)\n",
    "# Example: Bar chart of dominant topics\n",
    "value, topic_index, topic_counts = np.unique(dominant_topic, return_index=True,return_counts=True)\n",
    "plt.bar([v+1 for v in value], topic_counts)\n",
    "plt.xticks([v+1 for v in value])\n",
    "plt.xlabel('Dominant Topic')\n",
    "plt.ylabel('Number of Datasets')\n",
    "plt.title('Distribution of Dominant Topics in Datasets')\n",
    "plt.plot()\n",
    "'''for index, topic in model.show_topics(formatted=False, num_words= 30):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, [w[0] for w in topic]))'''\n",
    "questions_name = np.array(questions_name)\n",
    "for topic, count in zip(value,topic_counts):\n",
    "    indexes = [i for i in range(len(dominant_topic)) if dominant_topic[i] == topic]\n",
    "    print(\"TOPIC \",topic+1, \" count: \", count)\n",
    "    print(\"Topic Distribution: \", [topics[i] for i in indexes])\n",
    "    print(\"DATASET \", questions_name[indexes])\n",
    "    print(\"+\"*100)\n",
    "#plt.savefig(\"Test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "# Convert the gensim LDA model to a format compatible with pyLDAvis\n",
    "vis_data = gensimvis.prepare(model, corpus,dictionary=dictionary,sort_topics=False)\n",
    "#print(vis_data[1][vis_data[1][\"Category\"]!=\"Default\"])\n",
    "'''for term, freq, category,total in zip(vis_data[1]['Term'],vis_data[1]['Freq'],vis_data[1][\"Category\"],vis_data[1][\"Total\"]):\n",
    "    if category != \"Default\":\n",
    "        print(term, freq, category, total)'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.display(vis_data)\n",
    "#db_index_pca = davies_bouldin_score(vis_data[0]['x'], vis_data[0]['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data,\"visualization.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
