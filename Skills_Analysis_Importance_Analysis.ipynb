{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "from collections import Counter\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "with open(\"result/dataMultidisciplinaryCognitiveSkillsFrameworkRestrict.json\", 'r') as openfile:\n",
    "    #with open(\"result/dataCategory.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataCategory = json.load(openfile)\n",
    "cognitive_skills_community = {\n",
    "                    \"cognitive_process_memory\":[ \n",
    "                        \"sustained_attention\", \"selective_attention\", \"divided_attention\", \"vigilance_attention\",\"attention_shifting\",\n",
    "                        \"processing_speed\", \"visual_processing_speed\", \"auditory_processing_speed\",\n",
    "                        \"prospective_memory\", \"working_memory\", \"episodic_memory\", \"semantic_memory\", \"procedural_memory\", \"iconic_memory\", \"echoic_memory\", \"spatial_memory\"],\n",
    "                    \"executive_function\":[ \n",
    "                        \"planning\", \"organization\", \"goal_setting\",\"time_management\", \n",
    "                        \"problem_solving\", \"mental_flexibility\", \"strategic_thinking\",\"adaptability\",\n",
    "                        \"impulse_control\", \"decision_making\",\"emotional_regulation\",\"risk_assessment\",\n",
    "                        \"abstract_thinking\", \"reasoning\", \"cognitive_flexibility\", \"creativity\"], #concept_formation\n",
    "                    \"language_communication\":[\n",
    "                         \"expressive_language\", \"receptive_language\", \"naming\", \"fluency\", \"comprehension\", \"repetition\", \"reading\", \"writing\", \n",
    "                         \"pragmatics\", \"discourse_ability\", \"linguistic_analysis\", \"narrative_skills\"],\n",
    "                    \"social_cognition\":\n",
    "                        [\"recognition_of_social_cues\", \"theory_of_mind\", \"empathy\", \"social_judgment\",\"intercultural_competence\",\"conflict_resolution\",\"self_awareness\",\"relationship_management\"]\n",
    "}\n",
    "\n",
    "all_skill_label = []\n",
    "for func, skill_list in cognitive_skills_community.items():\n",
    "    all_skill_label += skill_list\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append(subset)\n",
    "    return dataname\n",
    "def returnCognitive(x):\n",
    "    for function, skill_list in cognitive_skills_community.items():\n",
    "        if x in skill_list:\n",
    "            return function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Frequency of Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten the skills list and count frequency of each skill\n",
    "all_skills = [skill for skills in dataCategory.values() for skill in skills]\n",
    "skill_counts = Counter(all_skills)\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Create a list of unique skills and datasets\n",
    "unique_skills = list(set(skill for skills in dataCategory.values() for skill in skills))\n",
    "unique_datasets = list(dataCategory.keys())\n",
    "\n",
    "# Step 2: Create a dataset-skill matrix\n",
    "matrix = pd.DataFrame(0, index=unique_datasets, columns=unique_skills)\n",
    "\n",
    "# Step 3: Fill the matrix with counts of skills per dataset\n",
    "for dataset, skills in dataCategory.items():\n",
    "    for skill in skills:\n",
    "        matrix.loc[dataset, skill] += 1\n",
    "\n",
    "# Step 4: Create a clustered heatmap with dendrograms\n",
    "# Step 4: Perform hierarchical clustering\n",
    "linkage_matrix = linkage(matrix, method='ward')\n",
    "\n",
    "# Step 5: Determine cluster assignments for rows (datasets)\n",
    "num_clusters = 14  # Define the number of clusters you want\n",
    "row_clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Step 6: Create a color palette to represent different clusters\n",
    "palette = sns.color_palette(\"Set2\", num_clusters)  # Use a color palette (e.g., \"Set2\")\n",
    "row_colors = [palette[cluster - 1] for cluster in row_clusters]  # Map cluster numbers to colors\n",
    "cognitive_category_mapping = {}\n",
    "for category, skills in cognitive_skills_community.items():\n",
    "    for skill in skills:\n",
    "        cognitive_category_mapping[skill] = category\n",
    "long_form_df = matrix.reset_index().melt(id_vars='index', var_name='Skill', value_name='Value')\n",
    "long_form_df.rename(columns={'index': 'Dataset'}, inplace=True)\n",
    "long_form_df['CognitiveCategory'] = long_form_df['Skill'].apply(returnCognitive)\n",
    "\n",
    "# Save to CSV\n",
    "long_form_df.to_csv('./result/lda_Iw/datasetskills_matrix.csv', index=False)\n",
    "\n",
    "sns.clustermap(matrix, method='ward', cmap=\"Blues\", figsize=(15,15), annot=False,  row_colors=row_colors)\n",
    "plt.title(\"Clustered Heatmap of Dataset-Skill Mapping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "community_colors = {\n",
    "    \"cognitive_process_memory\": \"skyblue\",\n",
    "    \"executive_function\": \"lightgreen\",\n",
    "    \"language_communication\": \"lightpink\",\n",
    "    \"social_cognition\": \"lightyellow\"\n",
    "}\n",
    "\n",
    "\n",
    "# Flatten the skills into a single list and count frequencies\n",
    "all_skills = [skill for skills in dataCategory.values() for skill in skills]\n",
    "skill_counter = Counter(all_skills)\n",
    "\n",
    "# Count how many datasets each skill appears in\n",
    "skill_dataset_count = Counter(skill for dataset, skills in dataCategory.items() for skill in set(skills))\n",
    "\n",
    "# Convert data to DataFrame for analysis\n",
    "datasets = list(dataCategory.keys())\n",
    "skills = list(set(all_skills))\n",
    "\n",
    "data = []\n",
    "for dataset in datasets:\n",
    "    row = [1 if skill in dataCategory[dataset] else 0 for skill in skills]\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, index=datasets, columns=skills)\n",
    "\n",
    "skill_freq_dataset_cognitive = {\"skills\":[],\"cognitive_function\":[],\"frequency\":[], \"dataset_freq\":[]}\n",
    "for category, skills in cognitive_skills_community.items():\n",
    "    for skill in skills:\n",
    "        skill_freq_dataset_cognitive[\"skills\"].append(skill)\n",
    "        skill_freq_dataset_cognitive[\"cognitive_function\"].append(category)\n",
    "        skill_freq_dataset_cognitive[\"frequency\"].append(skill_counter[skill])\n",
    "        skill_freq_dataset_cognitive[\"dataset_freq\"].append(skill_dataset_count[skill])\n",
    "\n",
    "skill_freq_dataset_cognitive_df = pd.DataFrame.from_dict(skill_freq_dataset_cognitive)\n",
    "skill_freq_dataset_cognitive_df.to_csv('./result/lda_Iw/skills_frequency.csv', index=False)\n",
    "# Plotting a bar chart of skill frequencies across all datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "print({skill: community_colors[skills] for skills in cognitive_skills_community for skill in cognitive_skills_community[skills]})\n",
    "plt.bar([skill for skills in cognitive_skills_community.values() for skill in skills], [skill_counter[skill] for skills in cognitive_skills_community.values() for skill in skills], color=[community_colors[skills] for skills in cognitive_skills_community for _ in cognitive_skills_community[skills]])\n",
    "plt.xlabel('Skills')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Skills Across Datasets')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting a bar chart of how many datasets each skill appears in\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar([skill for skills in cognitive_skills_community.values() for skill in skills], [skill_dataset_count[skill] for skills in cognitive_skills_community.values() for skill in skills], color=[community_colors[skills] for skills in cognitive_skills_community for _ in cognitive_skills_community[skills]])\n",
    "plt.xlabel('Skills')\n",
    "plt.ylabel('Number of Datasets')\n",
    "plt.title('Number of Datasets Each Skill Appears In')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap to show the presence of skills in datasets\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(df, annot=False, cmap=\"YlGnBu\", cbar=True, linewidths=.5)\n",
    "plt.xlabel('Skills')\n",
    "plt.ylabel('Datasets')\n",
    "plt.title('Heatmap of Skill Presence Across Datasets')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Word Cloud for skill distribution\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(skill_counter)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Skill Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Modules Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze I_w with acc and pruning ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_average(dict):\n",
    "    data = dict[\"0\"]\n",
    "    iterations_block = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    iterations_channel = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    #for style , iterations in zip ([\"block\",\"channel\",\"block_random\",\"channel_random\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "    for style , iterations in zip ([\"block\",\"channel\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "        for iter in iterations:\n",
    "            if iter == \"0\":\n",
    "                continue\n",
    "            for ratio in dict[iter][style]:\n",
    "                for dataset in dict[iter][style][ratio]:\n",
    "                    for norm in dict[iter][style][ratio][dataset]:\n",
    "                        value = np.array(dict[iter][style][ratio][dataset][norm])\n",
    "                        if len( value.shape) != 1:\n",
    "                            shape_model = value.shape\n",
    "                        data[style][ratio][dataset][norm]= (np.array(data[style][ratio][dataset][norm])+value)\n",
    "                        if iter == iterations[-1]:\n",
    "                            data[style][ratio][dataset][norm] = data[style][ratio][dataset][norm]/len(iterations)\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "            # Reading from json file\n",
    "            dataset_list = json.load(openfile)\n",
    "    #Original Distribution\n",
    "    with open(\"result/original_distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "        vicuna_original = json.load(openfile)\n",
    "    with open(\"result/original_distribution_llama_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_original = json.load(openfile)\n",
    "    with open(\"result/original_distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_chat_original = json.load(openfile)\n",
    "    #Pruned Distribution\n",
    "    with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_distribution = json.load(openfile)\n",
    "    with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        vicuna_distribution = json.load(openfile)\n",
    "    with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_chat_distribution= json.load(openfile)\n",
    "    with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        dataset_community= json.load(openfile)\n",
    "    return get_dataset_list(dataset_list), (llama_original,take_average(llama_distribution)), (llama_chat_original,take_average(llama_chat_distribution)), (vicuna_original,take_average(vicuna_distribution))\n",
    "def create_plot_bog_modules(distribution, original_distribution, dataset_list,norm=\"|W|_0\"):\n",
    "    modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "    layer_modules_label=[ str(i)+\"_\"+m  for i in range(3,31) for m in modules]\n",
    "    result_sparsity = {\"pruning_ratio\":[],\"pruning_strategy\":[],\"dataset_name\":[],\"sparsity_ratio\":[],\"Iw\":[]}\n",
    "    result_delta = {\"pruning_ratio\":[],\"pruning_strategy\":[],\"dataset_name\":[],\"delta_acc\":[],\"Iw\":[]} \n",
    "    for pruner_style in [\"block\",\"channel\"]:\n",
    "        for pruner_ratio in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            for dataset_name in dataset_list:\n",
    "                dataset_name = dataset_name.split('/')[-1]\n",
    "                #get distribution of both\n",
    "                pruned_data = distribution[pruner_style][pruner_ratio][dataset_name][norm][3:31,:].flatten()\n",
    "                original_data = np.array(original_distribution[\"distribution\"][norm])[3:31,:].flatten()\n",
    "\n",
    "                #take the difference and normalize\n",
    "                pruned_acc = distribution[pruner_style][pruner_ratio][dataset_name][\"Accuracy\"][2]\n",
    "                original_acc = original_distribution[dataset_name][-1]\n",
    "\n",
    "                sparsity_ratio = [w_pruned/w_org for w_org, w_pruned  in zip(original_data,pruned_data) ]\n",
    "                min_weight_reduction = min(sparsity_ratio)\n",
    "                range_weight_reduction = max(sparsity_ratio) - min_weight_reduction\n",
    "                data = [(1-abs(original_acc-pruned_acc))*(((weight - min_weight_reduction) / range_weight_reduction)) for weight in sparsity_ratio ]\n",
    "                for sp_ratio,iw in zip(sparsity_ratio,data):\n",
    "                    result_sparsity[\"pruning_ratio\"].append(pruner_ratio)\n",
    "                    result_sparsity[\"pruning_strategy\"].append(pruner_style)\n",
    "                    result_sparsity[\"Iw\"].append(iw)\n",
    "                    result_sparsity[\"sparsity_ratio\"].append(sp_ratio)\n",
    "                    result_sparsity[\"dataset_name\"].append(dataset_name)\n",
    "                result_delta[\"pruning_ratio\"].append(pruner_ratio)\n",
    "                result_delta[\"pruning_strategy\"].append(pruner_style)\n",
    "                result_delta[\"Iw\"].append(np.mean(data))\n",
    "                result_delta[\"delta_acc\"].append(abs(original_acc-pruned_acc))\n",
    "                result_delta[\"dataset_name\"].append(dataset_name) \n",
    "    return pd.DataFrame.from_dict(result_sparsity), pd.DataFrame.from_dict(result_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list, llama, llama_chat, vicuna  = get_model()\n",
    "data_llama = create_plot_bog_modules(llama[1], llama[0], dataset_list,norm=\"|W|_0\")\n",
    "data_llama_chat = create_plot_bog_modules(llama_chat[1], llama_chat[0], dataset_list,norm=\"|W|_0\")\n",
    "data_vicuna = create_plot_bog_modules(vicuna[1], vicuna[0], dataset_list,norm=\"|W|_0\") \n",
    "def concatenate(data_llama,data_llama_chat,data_vicuna):\n",
    "    data_llama[\"model\"],data_llama_chat[\"model\"],data_vicuna[\"model\"] = data_llama.shape[0]*[\"llama\"],data_llama_chat.shape[0]*[\"llama_chat\"],data_vicuna.shape[0]*[\"vicuna\"]\n",
    "    return pd.concat([data_llama,data_llama_chat,data_vicuna])\n",
    "data_all_acc =concatenate(data_llama[1],data_llama_chat[1],data_vicuna[1])\n",
    "data_all_pruning =concatenate(data_llama[0],data_llama_chat[0],data_vicuna[0])  \n",
    "data_all_acc.to_csv(\"./result/lda_Iw/accuracy.csv\",index=False)\n",
    "data_all_pruning.to_csv(\"./result/lda_Iw/performance.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $I_w$ vs $\\Delta$ Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plots = sns.lmplot(\n",
    "    data=data_all_acc, x=\"delta_acc\", y=\"Iw\",\n",
    "    hue=\"pruning_strategy\", col=\"model\",\n",
    ")\n",
    "plt.ylabel(r\"Average $I_w$\")\n",
    "plt.xlabel(r\"$\\Delta acc$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.scatter(data_all_pruning[\"sparsity_ratio\"], data_all_pruning[\"Iw\"])\n",
    "plt.ylabel(r\"$I_w$\")\n",
    "plt.xlabel(r\"$\\Delta acc$\")\n",
    "plt.show()'''\n",
    "def plot_histogram_sparsity_ratio(df,title):\n",
    "    data_filter_block = df[0][(df[0][\"pruning_ratio\"]==\"25\")&(df[0][\"sparsity_ratio\"]<10.61)&(df[0][\"pruning_strategy\"]==\"block\")]\n",
    "    data_filter_channel = df[0][(df[0][\"pruning_ratio\"]==\"25\")&(df[0][\"sparsity_ratio\"]<10.61)&(df[0][\"pruning_strategy\"]==\"channel\")]\n",
    "    fig, [ax0,ax1] = plt.subplots(ncols=2,figsize=(12,6))\n",
    "    sns.histplot(data=data_filter_block, x = \"sparsity_ratio\",bins=100, kde=True, ax=ax0)\n",
    "    ax0.set_title(\"Block\")\n",
    "    ax0.set_xlabel(r\"$I_w$\")\n",
    "    ax0.set_xscale(\"log\")\n",
    "    #ax0.tick_params(axis='x', rotation=90, size=12)\n",
    "    sns.histplot(data=data_filter_channel, x = \"sparsity_ratio\",bins=100, kde=True, ax=ax1)\n",
    "    ax1.set_title(\"Channel\")\n",
    "    ax1.set_xlabel(r\"$I_w$\")\n",
    "    ax1.set_xscale(\"log\")\n",
    "    #ax1.tick_params(axis='x', rotation=90, size=12)\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "plot_histogram_sparsity_ratio(data_llama,\"Llama\")\n",
    "plot_histogram_sparsity_ratio(data_llama_chat,\"Llama Chat\")\n",
    "plot_histogram_sparsity_ratio(data_vicuna,\"Vicuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of $I_w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_Iw(df,title):\n",
    "    data_filter = df[0][(df[0][\"Iw\"]>0.01)&(df[0][\"Iw\"]<0.99)]\n",
    "    sns.histplot(data=df[0], x = \"Iw\",hue=\"pruning_strategy\",color=\"red\",label='llama', kde=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(r\"$I_w$\")\n",
    "    plt.show()\n",
    "    sns.histplot(data=data_filter, x = \"Iw\",hue=\"pruning_strategy\",color=\"red\",label='llama', kde=True)\n",
    "    plt.xlabel(r\"$I_w$\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "plot_histogram_Iw(data_llama,\"Llama\")\n",
    "plot_histogram_Iw(data_llama_chat,\"Llama Chat\")\n",
    "plot_histogram_Iw(data_vicuna,\"Vicuna\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
