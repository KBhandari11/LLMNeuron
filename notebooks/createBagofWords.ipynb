{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json \n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Import necessary libraries\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "#import itertools\n",
    "from utils.bag_of_words.skill_dataset import *\n",
    "from utils.bag_of_words.network_property import *\n",
    "from utils.bag_of_words.dataset_modules import *\n",
    "from utils.bag_of_words.bipartite_multipartite_projection import *\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "def l_0_norm(vector):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        for sub_element in element:\n",
    "            if sub_element != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "def take_average(dict):\n",
    "    data = dict[\"0\"]\n",
    "    iterations_block = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    iterations_channel = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    #for style , iterations in zip ([\"block\",\"channel\",\"block_random\",\"channel_random\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "    for style , iterations in zip ([\"block\",\"channel\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "        for iter in iterations:\n",
    "            if iter == \"0\":\n",
    "                continue\n",
    "            for ratio in dict[iter][style]:\n",
    "                for dataset in dict[iter][style][ratio]:\n",
    "                    for norm in dict[iter][style][ratio][dataset]:\n",
    "                        value = np.array(dict[iter][style][ratio][dataset][norm])\n",
    "                        if len( value.shape) != 1:\n",
    "                            shape_model = value.shape\n",
    "                        data[style][ratio][dataset][norm]= (np.array(data[style][ratio][dataset][norm])+value)\n",
    "                        if iter == iterations[-1]:\n",
    "                            data[style][ratio][dataset][norm] = data[style][ratio][dataset][norm]/len(iterations)\n",
    "    return data, shape_model\n",
    "\n",
    "def strip(name):\n",
    "    name = name.split(\"/\")[-1]\n",
    "    name = name.split(\"_\")[0]\n",
    "    return name \n",
    "\n",
    "with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    vicuna_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_distribution= json.load(openfile)\n",
    "def loop_over(dict):\n",
    "    if isinstance(dict, list):\n",
    "        print(\"end\")\n",
    "    else: \n",
    "        print(dict.keys())\n",
    "        for keys in dict:\n",
    "            loop_over(dict[keys])\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append(subset)\n",
    "    return dataname\n",
    "    #Dataset List\n",
    "with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        dataset_list = json.load(openfile)\n",
    "#Original Distribution\n",
    "with open(\"result/original_distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    vicuna_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_original = json.load(openfile)\n",
    "#Pruned Distribution\n",
    "with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    vicuna_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_distribution= json.load(openfile)\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_community= json.load(openfile)\n",
    "\n",
    "dataset_list = get_dataset_list(dataset_list)\n",
    "llama_distribution, model_shape = take_average(llama_distribution)\n",
    "vicuna_distribution, model_shape = take_average(vicuna_distribution)\n",
    "llama_chat_distribution, model_shape = take_average(llama_chat_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.skill_dataset import *\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_community= json.load(openfile)\n",
    "with open(\"result/dataMultidisciplinaryCognitiveSkillsFrameworkRestrict.json\", 'r') as openfile:\n",
    "    #with open(\"result/dataCategory.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataCategory = json.load(openfile)\n",
    "cognitive_skills_community = {\n",
    "                    \"cognitive_process_memory\":[ \n",
    "                        \"sustained_attention\", \"selective_attention\", \"divided_attention\", \"vigilance_attention\",\"attention_shifting\",\n",
    "                        \"processing_speed\", \"visual_processing_speed\", \"auditory_processing_speed\",\n",
    "                        \"prospective_memory\", \"working_memory\", \"episodic_memory\", \"semantic_memory\", \"procedural_memory\", \"iconic_memory\", \"echoic_memory\", \"spatial_memory\"],\n",
    "                    \"executive_function\":[ \n",
    "                        \"planning\", \"organization\", \"goal_setting\",\"time_management\", \n",
    "                        \"problem_solving\", \"mental_flexibility\", \"strategic_thinking\",\"adaptability\",\n",
    "                        \"impulse_control\", \"decision_making\",\"emotional_regulation\",\"risk_assessment\",\n",
    "                        \"abstract_thinking\", \"reasoning\", \"cognitive_flexibility\", \"creativity\"], #concept_formation\n",
    "                    \"language_communication\":[\n",
    "                         \"expressive_language\", \"receptive_language\", \"naming\", \"fluency\", \"comprehension\", \"repetition\", \"reading\", \"writing\", \n",
    "                         \"pragmatics\", \"discourse_ability\", \"linguistic_analysis\", \"narrative_skills\"],\n",
    "                    \"social_cognition\":\n",
    "                        [\"recognition_of_social_cues\", \"theory_of_mind\", \"empathy\", \"social_judgment\",\"intercultural_competence\",\"conflict_resolution\",\"self_awareness\",\"relationship_management\"]\n",
    "}\n",
    "dataCategory1 = dataCategory#filterData(dataCategory, 1.0)#0.4\n",
    "all_skill_label = []\n",
    "for func, skill_list in cognitive_skills_community.items():\n",
    "    all_skill_label += skill_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create bag of words: Dataset vs Skills\n",
    "where rows are neurons, and columns mean the neuron activity on each dataset after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataCategory1 = dataCategory#filterData(dataCategory, 1.0)#0.4\n",
    "A_dataset_skill, skill_label = create_plot_bog_skills(dataCategory1, dataset_list, plot=False)\n",
    "'''dataCategory2 = td_idf_filter(dataCategory)\n",
    "A_dataset_skill, skills = create_plot_bog_skills(dataCategory2, dataset_list, plot=False)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create bag of words: Dataset vs Neurons\n",
    "where rows are datasets, and columns mean the sparsity value on each dataset after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a,b = create_plot_bog_modules(llama_distribution,llama_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"15\",norm=\"|W|_0\",plot=True, alpha=0.01)\n",
    "a,b = create_plot_bog_modules(llama_distribution,llama_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"25\",norm=\"|W|_0\",plot=True, alpha=0.01)\n",
    "a,b = create_plot_bog_modules(llama_distribution,llama_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"40\",norm=\"|W|_0\",plot=True, alpha=0.01)\n",
    "#a,b = create_plot_bog_modules(llama_chat_distribution,llama_chat_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",plot=True, alpha=0.01)\n",
    "#a,b = create_plot_bog_modules(vicuna_distribution,vicuna_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",plot=True, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bipartite Network from Adjacency Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite: Dataset vs Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.bipartite_multipartite_projection import *\n",
    "AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory1, dataset_list, plot=False)\n",
    "networkx_draw_bipartite(AB_dataset_skill,dataset_list,skill_label, A_node_color=\"tab:blue\",B_node_color=\"tab:red\",path=\"./graph/skills_dataset.csv\")\n",
    "BC_dataset_modules,  module_label = create_plot_bog_modules(llama_distribution,llama_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha=0.01,plot=False)\n",
    "networkx_draw_bipartite(BC_dataset_modules,dataset_list,module_label, A_node_color=\"tab:blue\",B_node_color=\"tab:green\",path=\"./graph/llama_block_20_dataset_modules.csv\")\n",
    "#networkx_draw_tripartite(AB_dataset_skill.T,BC_dataset_modules,skill_label, [d.split(\"/\")[-1] for d  in dataset_list],module_label,\"tab:red\",\"tab:blue\",\"tab:green\",title=\"\")\n",
    "plot_AC_bipartite(AB_dataset_skill, skill_label,llama_distribution,llama_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha1=0.01,alpha2=0.01,path=\"./graph/skills_modules.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite: Skills vs Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory1, dataset_list, plot=False)\n",
    "plot_AC_bipartite(AB_dataset_skill, skill_label,llama_distribution,llama_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha1=0.01,alpha2=0.01)\n",
    "plot_AC_bipartite(AB_dataset_skill, skill_label,vicuna_distribution, vicuna_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha1=0.01,alpha2=0.01)\n",
    "plot_AC_bipartite(AB_dataset_skill, skill_label,llama_chat_distribution,llama_chat_original, dataset_list,pruner_style=\"channel\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha1=0.01,alpha2=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Multipartite Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multipartite Network: Modules vs Dataset vs Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_dataset_skill, skillNode = create_plot_bog_skills(dataCategory1, dataset_list, plot=False)\n",
    "AB_skill_dataset =AB_dataset_skill.T\n",
    "BC_dataset_modules,  moduleNode = create_plot_bog_modules(llama_distribution,llama_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha=0.01,plot=False)\n",
    "networkx_draw_tripartite(AB_skill_dataset,BC_dataset_modules,skillNode, [d.split(\"/\")[-1] for d  in dataset_list],moduleNode,\"tab:red\",\"tab:blue\",\"tab:green\",title=\"\")\n",
    "BC_dataset_modules,  moduleNode = create_plot_bog_modules(vicuna_distribution,vicuna_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha=0.01,plot=False)\n",
    "networkx_draw_tripartite(AB_skill_dataset,BC_dataset_modules,skillNode, [d.split(\"/\")[-1] for d  in dataset_list],moduleNode,\"tab:red\",\"tab:blue\",\"tab:green\",title=\"\")\n",
    "BC_dataset_modules,  moduleNode = create_plot_bog_modules(llama_chat_distribution,llama_chat_original, dataset_list,pruner_style=\"block\", pruner_ratio=\"20\",norm=\"|W|_0\",alpha=0.01,plot=False)\n",
    "networkx_draw_tripartite(AB_skill_dataset,BC_dataset_modules,skillNode, [d.split(\"/\")[-1] for d  in dataset_list],moduleNode,\"tab:red\",\"tab:blue\",\"tab:green\",title=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Projection Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing community detection between different networks for different sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.bipartite_multipartite_projection import *\n",
    "from utils.bag_of_words.network_property import *\n",
    "from utils.bag_of_words.permutation_test import *\n",
    "from sklearn.metrics import jaccard_score, normalized_mutual_info_score, rand_score,adjusted_rand_score, adjusted_mutual_info_score,mutual_info_score,adjusted_mutual_info_score\n",
    "\n",
    "def add_isolated(label, all_label):\n",
    "    new_label = {}\n",
    "    total_comm = list(set(label.values()))\n",
    "    for node in all_label:\n",
    "        if node in label:\n",
    "            new_label[node] = label[node]\n",
    "        else:\n",
    "            new_label[node] = len(total_comm) +10\n",
    "    return new_label\n",
    "def get_community_for_alpha(dataCategory, dataset_list, distribution, original, pruner_style=\"block\", sparsity_ratio=\"15\",alpha1=None, alpha2=None, random_seed=True, modules_vs_modules=True):\n",
    "    AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory, dataset_list, plot=False)\n",
    "    BC_dataset_modules, module_label = create_plot_bog_modules(distribution,original, dataset_list,pruner_style=pruner_style, pruner_ratio=sparsity_ratio,norm=\"|W|_0\",plot=False, alpha=alpha1, random_seed=random_seed)\n",
    "    A_skill_modules =  np.dot(AB_dataset_skill.T,BC_dataset_modules)\n",
    "    sparse_network = spectral_sparsification(A_skill_modules, alpha2, random_seed=random_seed)\n",
    "\n",
    "    if modules_vs_modules:\n",
    "        _ ,A_modules_modules  = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_modules_modules,module_label,module_label )\n",
    "    else:\n",
    "        A_skills_skills , _ = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_skills_skills,skill_label,skill_label )\n",
    "    return G, network_property, (sparse_network,skill_label,module_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data for community comparison between different sparsity value for Module vs Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "pruner_style =\"block\"\n",
    "modules_vs_modules = False # if False comparing skills vs skills\n",
    "if modules_vs_modules:\n",
    "    modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "    all_label=[ str(i)+\"_\"+m  for i in range(3,31) for m in modules]\n",
    "else:\n",
    "    all_label = all_skill_label\n",
    "\n",
    "data = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio1\":[],\"sparsity_ratio2\":[], \"jaccard_index\":[], \"nmi\":[], \"rand_score\":[], \"adjusted_rand_score\":[]}\n",
    "for pruner_style in [\"block\",\"channel\"]:\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "        for sparsity_ratio1 in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            _, property_1, _ = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio1,alpha1=0.01,alpha2=0.01, modules_vs_modules=modules_vs_modules)\n",
    "            partition1 = add_isolated(property_1[\"partition\"],all_label)\n",
    "            partition1 = np.array([comm for  _, comm in partition1.items()])\n",
    "            for sparsity_ratio2 in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "                #module_p_values = permutation_test(original_network_list)\n",
    "                G , property_2,_ = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio2,alpha1=0.01,alpha2=0.01, modules_vs_modules=modules_vs_modules)\n",
    "                partition2 = add_isolated(property_2[\"partition\"],all_label)\n",
    "                partition2 = np.array([comm for  _, comm in partition2.items()])\n",
    "                jaccard = jaccard_score(partition1,partition2, average=\"micro\")\n",
    "                nmi = normalized_mutual_info_score(partition1,partition2)\n",
    "                adjust_rand_score = adjusted_rand_score(partition1,partition2)\n",
    "                rand_scores = rand_score(partition1,partition2)\n",
    "                #jaccard = jaccard_score(np.array([comm for  _, comm in property_1[\"partition\"].items()]), np.array([comm for  _, comm in property_2[\"partition\"].items()]), average=None)\n",
    "                #print(sparsity_ratio1,sparsity_ratio2,jaccard)\n",
    "                data[\"model\"].append(model)\n",
    "                data[\"pruner_style\"].append(pruner_style)\n",
    "                data[\"sparsity_ratio1\"].append(sparsity_ratio1)\n",
    "                data[\"sparsity_ratio2\"].append(sparsity_ratio2)\n",
    "                data[\"jaccard_index\"].append(jaccard)\n",
    "                data[\"nmi\"].append(nmi)\n",
    "                data[\"rand_score\"].append(rand_scores)\n",
    "                data[\"adjusted_rand_score\"].append(adjust_rand_score)\n",
    "\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(data)\n",
    "data1[\"sparsity_ratio1\"] = pd.to_numeric(data1[\"sparsity_ratio1\"])\n",
    "data1[\"sparsity_ratio2\"] = pd.to_numeric(data1[\"sparsity_ratio2\"])\n",
    "for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "    for score in [\"rand_score\",\"adjusted_rand_score\",\"jaccard_index\",\"nmi\"]:\n",
    "        #for score in [\"rand_score\"]:\n",
    "        data2 = data1[data1.model == model]\n",
    "        block = data2[data2.pruner_style == \"block\"]\n",
    "        channel = data2[data2.pruner_style == \"channel\"]\n",
    "        block = block.pivot(index=\"sparsity_ratio1\", columns=\"sparsity_ratio2\", values=score)\n",
    "        channel = channel.pivot(index=\"sparsity_ratio1\", columns=\"sparsity_ratio2\", values=score)\n",
    "        mask_b = np.triu(np.ones_like(block.to_numpy()))-np.eye(block.to_numpy().shape[0])\n",
    "        mask_c = np.triu(np.ones_like(channel.to_numpy()))-np.eye(channel.to_numpy().shape[0])\n",
    "        fig, [ax0,ax1] = plt.subplots(figsize = (10,5),ncols= 2)\n",
    "        sns.heatmap(block,ax=ax0,annot=True, mask=mask_b)\n",
    "        ax0.set_title(\"Block\")\n",
    "        sns.heatmap(channel,ax=ax1,annot=True, mask=mask_c)\n",
    "        ax1.set_title(\"Channel\")\n",
    "        fig.suptitle(f\"Model: {model} | Metric: {score}\")\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Community within Skills vs Skills to Cognitive Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "pruner_style =\"block\"\n",
    "def add_isolated(label, all_label):\n",
    "    new_label = {}\n",
    "    total_comm = list(set(label.values()))\n",
    "    for node in all_label:\n",
    "        if node in label:\n",
    "            new_label[node] = label[node]\n",
    "        else:\n",
    "            new_label[node] = len(total_comm)\n",
    "            total_comm.append(len(total_comm))\n",
    "    return new_label\n",
    "def get_ground_truth(all_node, cognitive_skills_community):\n",
    "    ground_truth = {}\n",
    "    for node in all_node:\n",
    "        for comm_idx, comm in enumerate(cognitive_skills_community):\n",
    "            if node in cognitive_skills_community[comm]:\n",
    "                ground_truth[node] = comm_idx\n",
    "    return ground_truth\n",
    "cognitive_function_index_dict= get_ground_truth(all_skill_label, cognitive_skills_community)\n",
    "cognitive_function_partition = np.array([cognitive_function_index_dict[skill] for skill in all_skill_label])\n",
    "data = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"jaccard_index\":[], \"nmi\":[], \"adjusted_nmi\":[], \"rand_score\":[], \"adjusted_rand_score\":[],\"community\":[],\"cognitive_function\":[]}\n",
    "for pruner_style in [\"block\",\"channel\"]:\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "        for sparsity_ratio1 in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            G, property_1, _ = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio1,alpha1=0.01,alpha2=0.01, modules_vs_modules=False)\n",
    "            partition1 = add_isolated(property_1[\"partition\"],all_skill_label)\n",
    "            partition1 = np.array([comm for  _, comm in partition1.items()])\n",
    "\n",
    "            print(partition1)\n",
    "            print(cognitive_function_partition)\n",
    "            \n",
    "            jaccard = jaccard_score(cognitive_function_partition,partition1, average=\"micro\")\n",
    "            nmi = normalized_mutual_info_score(cognitive_function_partition,partition1)\n",
    "            adjust_nmi= adjusted_mutual_info_score(cognitive_function_partition,partition1)\n",
    "            adjust_rand_score = adjusted_rand_score(cognitive_function_partition,partition1)\n",
    "            rand_scores = rand_score(cognitive_function_partition,partition1)\n",
    "            #jaccard = jaccard_score(np.array([comm for  _, comm in property_1[\"partition\"].items()]), np.array([comm for  _, comm in property_2[\"partition\"].items()]), average=None)\n",
    "            #print(sparsity_ratio1,sparsity_ratio2,jaccard)\n",
    "            data[\"model\"].append(model)\n",
    "            data[\"pruner_style\"].append(pruner_style)\n",
    "            data[\"sparsity_ratio\"].append(sparsity_ratio1)\n",
    "            data[\"jaccard_index\"].append(jaccard)\n",
    "            data[\"nmi\"].append(nmi)\n",
    "            data[\"adjusted_nmi\"].append(adjust_nmi)\n",
    "            data[\"rand_score\"].append(rand_scores)\n",
    "            data[\"adjusted_rand_score\"].append(adjust_rand_score)\n",
    "            data[\"community\"].append(list(partition1))\n",
    "            data[\"cognitive_function\"].append(list(cognitive_function_partition))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.projection_community import get_community_for_alpha\n",
    "pruner_style = \"block\"\n",
    "model_idx ,model = 0, \"llama\"\n",
    "sparsity_ratio1 = \"20\"\n",
    "def add_isolated(label, all_label):\n",
    "    new_label = {}\n",
    "    total_comm = list(set(label.values()))\n",
    "    for node in all_label:\n",
    "        if node in label:\n",
    "            new_label[node] = label[node]\n",
    "        else:\n",
    "            new_label[node] = len(total_comm)\n",
    "            total_comm.append(len(total_comm))\n",
    "    return new_label\n",
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "G, network_property, (sparse_network,skill_label,module_label), (AB_dataset_skill, BC_dataset_modules, A_skill_modules) = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio1,alpha1=0.01,alpha2=0.01, modules_vs_modules=False)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(G, \"./graph/llama_block_20_skill_skill.csv\", data=[\"weight\"],delimiter=',')\n",
    "node_data = []\n",
    "for node, community_id in network_property[\"partition\"].items():\n",
    "    node_data.append({\"Id\":node,\"size\":4,\"community\":community_id})\n",
    "df = pd.DataFrame(node_data) \n",
    "df.to_csv( \"./graph/llama_block_20_nodes_skill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(data)\n",
    "data1.to_csv(\"result/compare_cog_functions_skills_community.csv\")\n",
    "#data1[\"sparsity_ratio1\"] = pd.to_numeric(data1[\"sparsity_ratio1\"])\n",
    "for score in [\"rand_score\",\"adjusted_rand_score\",\"jaccard_index\",\"nmi\",\"adjusted_nmi\"]:\n",
    "    #for score in [\"rand_score\"]:\n",
    "    block = data1[data1.pruner_style == \"block\"]\n",
    "    channel = data1[data1.pruner_style == \"channel\"]\n",
    "\n",
    "    fig, [ax0,ax1] = plt.subplots(figsize = (20,4),ncols= 2)\n",
    "    sns.lineplot(data=block, x=\"sparsity_ratio\", y=score, hue=\"model\", ax=ax0)\n",
    "    ax0.set_title(\"Block\")\n",
    "    sns.lineplot(data=channel, x=\"sparsity_ratio\", y=score, hue=\"model\", ax=ax1)\n",
    "    ax1.set_title(\"Channel\")\n",
    "    fig.suptitle(f\" Metric: {score}\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare localization with random network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.bipartite_multipartite_projection import *\n",
    "from utils.bag_of_words.network_property import *\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import stats\n",
    "\n",
    "def create_random_network(G):\n",
    "    actual_degrees = [d for v, d in G.degree()]\n",
    "    G_rand = nx.configuration_model(actual_degrees)\n",
    "    G_rand = nx.Graph(G_rand)\n",
    "    G_rand.remove_edges_from(nx.selfloop_edges(G_rand))\n",
    "    result = {}\n",
    "    result[\"average_degree\"] = sum(dict(G_rand.degree()).values()) / len(G_rand)\n",
    "    result[\"average_cluster\"] = nx.average_clustering(G_rand)  # Average clustering coefficient\n",
    "    result[\"density\"] = nx.density(G_rand)\n",
    "    result[\"global_efficiency\"]  = nx.global_efficiency(G_rand)\n",
    "    result[\"assortativity_coefficient\"]   = nx.degree_assortativity_coefficient(G_rand)\n",
    "    if nx.is_connected(G_rand):\n",
    "        result[\"diameter\"]  = nx.diameter(G_rand)\n",
    "        result[\"average_path_length\"] = nx.average_shortest_path_length(G_rand)\n",
    "    else:\n",
    "        S = G_rand.subgraph(max(nx.connected_components(G_rand), key=len))\n",
    "        result[\"diameter\"]  = nx.diameter(S)\n",
    "        result[\"average_path_length\"] = nx.average_shortest_path_length(S)\n",
    "    return result\n",
    "\n",
    "def collect_all_data(n_sample, dataCategory, dataset_list, distribution_dist, original_dist, pruner_style , sparsity_ratio ,alpha1 ,alpha2, random_seed=False, modules_vs_modules=True):\n",
    "    data_org = {\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "    data_rand = {\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "    for _ in range(n_sample):\n",
    "        G , prop_org,_ = get_community_for_alpha(dataCategory, dataset_list, distribution_dist, original_dist, pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=alpha1,alpha2=alpha2,random_seed=random_seed, modules_vs_modules=modules_vs_modules)\n",
    "        prop_rand = create_random_network(G)\n",
    "        for prop in data_org:\n",
    "            data_org[prop].append(prop_org[prop])\n",
    "            data_rand[prop].append(prop_rand[prop])\n",
    "    return data_org, data_rand\n",
    "\n",
    "\n",
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    plt.hist(degrees)\n",
    "    plt.show()\n",
    "def statistic(x, y, axis):\n",
    "    return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "\n",
    "def plot_heatmap_pvalues(data_pvalue, data_stats):\n",
    "    data1_pvalue = pd.DataFrame(data_pvalue)\n",
    "    data1_stats = pd.DataFrame(data_stats)\n",
    "    #data_pvalue = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "    data1_pvalue[\"sparsity_ratio\"] = pd.to_numeric(data1_pvalue[\"sparsity_ratio\"])\n",
    "    data1_stats[\"sparsity_ratio\"] = pd.to_numeric(data1_stats[\"sparsity_ratio\"])\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "            data2_pvalue = data1_pvalue[data1_pvalue.model == model]\n",
    "            data2_stats = data1_stats[data1_stats.model == model]\n",
    "    # Pivot the DataFrame to suit heatmap format (adjusting for actual p-values and categories)\n",
    "            pivoted_data_pval = data2_pvalue.pivot_table(index=['pruner_style', 'sparsity_ratio'], \n",
    "                                    values=['average_degree', 'average_cluster', 'density', \n",
    "                                            'global_efficiency', 'diameter', 'assortativity_coefficient', \n",
    "                                            'average_path_length'],\n",
    "                                    aggfunc=np.max)  # You may use np.min or another function if it makes more sense\n",
    "            pivoted_data_stat = data2_stats.pivot_table(index=['pruner_style', 'sparsity_ratio'], \n",
    "                                    values=['average_degree', 'average_cluster', 'density', \n",
    "                                            'global_efficiency', 'diameter', 'assortativity_coefficient', \n",
    "                                            'average_path_length'],\n",
    "                                    aggfunc=np.max)  # You may use np.min or another function if it makes more sense\n",
    "            # Creating a heatmap\n",
    "            #pivoted_data_stat.sort_index(level=['pruner_style', 'sparsity_ratio'], ascending=[True, False])\n",
    "            fig, [ax0, ax1] = plt.subplots(figsize=(18, 6), ncols=2)\n",
    "            sns.heatmap(pivoted_data_pval, annot=True,ax=ax0, cmap='coolwarm', fmt=\".2g\")#, yticklabels=label)\n",
    "            ax0.set_title('Heatmap of P-Values by Pruner Style and Sparsity Ratio')\n",
    "            ax0.set_ylabel('Pruner Style, Sparsity Ratio')\n",
    "            ax0.set_xlabel('Metrics')\n",
    "            ax0.set_yticklabels(ax0.get_yticklabels(), rotation=30)\n",
    "            ax0.set_xticklabels(ax0.get_xticklabels(), rotation=30)\n",
    "            sns.heatmap(pivoted_data_stat, annot=True,ax=ax1, cmap=ListedColormap(['white']),linecolor=\"black\", linewidths = 3,fmt=\".2g\",cbar=False)#, yticklabels=label)\n",
    "            ax1.set_title('Heatmap of Statistics by Pruner Style and Sparsity Ratio')\n",
    "            ax1.set_ylabel('Pruner Style, Sparsity Ratio')\n",
    "            ax1.set_xlabel('Metrics')\n",
    "            #ax1.set_xticklabels(rotation=45)\n",
    "            ax1.set_xticklabels(ax1.get_xticklabels(), rotation=30)\n",
    "            ax1.set_yticklabels(ax1.get_yticklabels(), rotation=30)\n",
    "            fig.suptitle(model)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules_vs_Modules Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "data_modules_pvalue = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "data_modules_stats = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "for pruner_style in [\"block\",\"channel\"]:\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "        for sparsity_ratio in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            original_sample, random_sample = collect_all_data(50,dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=0.01,alpha2=0.01, random_seed=False)\n",
    "            for metric, random_values in random_sample.items():\n",
    "                res = stats.permutation_test((random_values, original_sample[metric]),statistic, n_resamples=10000, vectorized=True, alternative='two-sided')\n",
    "                #res = stats.ttest_1samp(random_values, popmean=property[metric])\n",
    "                data_modules_pvalue[metric].append(res.pvalue)\n",
    "                data_modules_stats[metric].append(res.statistic)\n",
    "                '''plt.hist(random_values, bins=30, alpha=0.75, label='Random Networks')\n",
    "                plt.axvline(property[metric], color='red', linestyle='dashed', linewidth=2, label='Your Network')\n",
    "                plt.title(f'Comparison of {metric}')\n",
    "                plt.xlabel(f'{metric}')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "                plt.show()'''\n",
    "            data_modules_pvalue[\"model\"].append(model)\n",
    "            data_modules_pvalue[\"pruner_style\"].append(pruner_style)\n",
    "            data_modules_pvalue[\"sparsity_ratio\"].append(sparsity_ratio)\n",
    "            data_modules_stats[\"model\"].append(model)\n",
    "            data_modules_stats[\"pruner_style\"].append(pruner_style)\n",
    "            data_modules_stats[\"sparsity_ratio\"].append(sparsity_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap_pvalues(data_modules_pvalue, data_modules_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills vs Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "data_skill_pvalue = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "data_skill_stats = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[],\"average_degree\":[],\"average_cluster\":[],\"density\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "for pruner_style in [\"block\",\"channel\"]:\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "        for sparsity_ratio in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            original_sample, random_sample = collect_all_data(50,dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=0.01,alpha2=0.01, random_seed=False, modules_vs_modules=False)\n",
    "            for metric, random_values in random_sample.items():\n",
    "                res = stats.permutation_test((random_values, original_sample[metric]),statistic, n_resamples=10000, vectorized=True, alternative='two-sided')\n",
    "                #res = stats.ttest_1samp(random_values, popmean=property[metric])\n",
    "                data_skill_pvalue[metric].append(res.pvalue)\n",
    "                data_skill_stats[metric].append(res.statistic)\n",
    "                '''plt.hist(random_values, bins=30, alpha=0.75, label='Random Networks')\n",
    "                plt.axvline(property[metric], color='red', linestyle='dashed', linewidth=2, label='Your Network')\n",
    "                plt.title(f'Comparison of {metric}')\n",
    "                plt.xlabel(f'{metric}')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "                plt.show()'''\n",
    "            data_skill_pvalue[\"model\"].append(model)\n",
    "            data_skill_pvalue[\"pruner_style\"].append(pruner_style)\n",
    "            data_skill_pvalue[\"sparsity_ratio\"].append(sparsity_ratio)\n",
    "            data_skill_stats[\"model\"].append(model)\n",
    "            data_skill_stats[\"pruner_style\"].append(pruner_style)\n",
    "            data_skill_stats[\"sparsity_ratio\"].append(sparsity_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap_pvalues(data_skill_pvalue, data_skill_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules vs Modules Graph Compare (to the Ground Truth)\n",
    "## Chi Square Test to compare skill frequency test between communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from scipy.stats import chi2_contingency, entropy,fisher_exact\n",
    "\n",
    "def add_isolated(label, all_label):\n",
    "    new_label = {}\n",
    "    total_comm = list(set(label.values()))\n",
    "    for node in all_label:\n",
    "        if node in label:\n",
    "            new_label[node] = label[node]\n",
    "        else:\n",
    "            new_label[node] = len(total_comm) +10\n",
    "    return new_label\n",
    "def get_community_for_alpha(dataCategory, dataset_list, distribution, original, pruner_style=\"block\", sparsity_ratio=\"15\",alpha1=None, alpha2=None, random_seed=True, modules_vs_modules=True):\n",
    "    AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory, dataset_list, plot=False)\n",
    "    BC_dataset_modules, module_label = create_plot_bog_modules(distribution,original, dataset_list,pruner_style=pruner_style, pruner_ratio=sparsity_ratio,norm=\"|W|_0\",plot=False, alpha=alpha1)\n",
    "    A_skill_modules =  np.dot(AB_dataset_skill.T,BC_dataset_modules)\n",
    "    sparse_network = spectral_sparsification(A_skill_modules, alpha2)\n",
    "    sparse_network =  sparse_network #min_max(sparse_network)\n",
    "    if modules_vs_modules:\n",
    "        _ ,A_modules_modules  = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_modules_modules,module_label,module_label )\n",
    "    else:\n",
    "        A_skills_skills , _ = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_skills_skills,skill_label,skill_label )\n",
    "    return G, network_property, (sparse_network,skill_label,module_label), (AB_dataset_skill, BC_dataset_modules, A_skill_modules)\n",
    "\n",
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]\n",
    "pruner_style =\"block\"\n",
    "def get_ground_truth(all_node, cognitive_skills_community):\n",
    "    ground_truth = {}\n",
    "    for node in all_node:\n",
    "        for comm_idx, comm in enumerate(cognitive_skills_community):\n",
    "            if node in cognitive_skills_community[comm]:\n",
    "                ground_truth[node] = comm_idx\n",
    "    return ground_truth\n",
    "def get_skills_shared_by_modules(G_module_module,community_detected,skills_modules,skills_label, module_label, all_skill_label):\n",
    "    community = {}\n",
    "    G_skill_module = create_biparitite(skills_modules,skills_label, module_label)\n",
    "    for comm in community_detected:\n",
    "        H_module_module = G_module_module.subgraph(community_detected[comm])\n",
    "        shared_skills = []\n",
    "        for u,v in list(H_module_module.edges):\n",
    "            u_skills = set(G_skill_module.neighbors(u))\n",
    "            v_skills = set(G_skill_module.neighbors(v))\n",
    "            shared_skills += list(u_skills.intersection(v_skills))\n",
    "        freq_skills = collections.Counter(shared_skills)\n",
    "        community[comm] = [freq_skills[skill] if skill in freq_skills else 0 for skill in all_skill_label]\n",
    "    return community\n",
    "def get_modules_shared_by_skills(G_skills_skills,community_detected,skills_modules,skills_label, module_label, all_label):\n",
    "    community = {}\n",
    "    G_skill_module = create_biparitite(skills_modules,skills_label, module_label)\n",
    "    for comm in community_detected:\n",
    "        H_skills_skills = G_skills_skills.subgraph(community_detected[comm])\n",
    "        shared_modules = []\n",
    "        for u,v in list(H_skills_skills.edges):\n",
    "            u_modules = set(G_skill_module.neighbors(u))\n",
    "            v_modules = set(G_skill_module.neighbors(v))\n",
    "            shared_modules += list(u_modules.intersection(v_modules))\n",
    "        freq_modules = collections.Counter(shared_modules)\n",
    "        community[comm] = [freq_modules[skill] if skill in freq_modules else 0 for skill in all_label]\n",
    "    return community\n",
    "def calculate_kl_divergence(community_freq, epsilon=1e-16):\n",
    "    row = []\n",
    "    for comm1 in community_freq:\n",
    "        distribution_p = np.array(community_freq[comm1]) / sum(community_freq[comm1])\n",
    "        smoothed_p = np.where(distribution_p == 0, epsilon, distribution_p)\n",
    "        col = []\n",
    "        for comm2 in community_freq:\n",
    "            distribution_q = np.array(community_freq[comm2]) / sum(community_freq[comm2])\n",
    "            smoothed_q = np.where(distribution_q == 0, epsilon, distribution_q)\n",
    "            col.append(entropy(smoothed_p, smoothed_q))\n",
    "        row.append(col)\n",
    "    return np.array(row)\n",
    "def create_frequency_skills(dataCategory, all_skill_label):\n",
    "    freq ={}\n",
    "    for data, skills in dataCategory.items():\n",
    "        freq_skills = collections.Counter(skills)\n",
    "        freq[data] = [freq_skills[skill] if skill in freq_skills else 0 for skill in all_skill_label]\n",
    "    return freq\n",
    "def check_kl_divergence(community_detected_frequency, sample_dict, epsilon=1e-16):\n",
    "    comm_list = list(community_detected_frequency.keys())\n",
    "    distributions = np.array([freq for _, freq in community_detected_frequency.items()],dtype='float64')\n",
    "    data = {comm:[] for comm in community_detected_frequency}\n",
    "    for samp in sample_dict:\n",
    "        sample = np.array(sample_dict[samp],dtype='float64')\n",
    "        \n",
    "        # Normalize the distributions and sample if not already normalized\n",
    "        distributions /= distributions.sum(axis=1, keepdims=True)\n",
    "        sample /= sample.sum()\n",
    "\n",
    "        # Compute Kullback-Leibler Divergence from the sample to each distribution\n",
    "        kl_divergences = [entropy(sample, dist) for dist in distributions]\n",
    "\n",
    "        # Find the distribution with the minimum KL divergence\n",
    "        closest_distribution_index = np.argmin(kl_divergences)\n",
    "        data[comm_list[closest_distribution_index]].append(samp)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi Square test for skills within Skills vs Skills or Modules vs Modules dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(2)\n",
    "def sum_cogn_function(fequencies, index):\n",
    "    data = []\n",
    "    for cog_functions in np.unique(index):\n",
    "        data.append(sum(np.take(fequencies,  np.where(index==cog_functions)[0], 0)))\n",
    "    return np.array(data)\n",
    "modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "layer_modules_label=[ str(i)+\"_\"+m  for i in range(3,31) for m in modules]\n",
    "random_seed = True\n",
    "modules_vs_modules = True # if comparing skills vs skills\n",
    "if not modules_vs_modules:\n",
    "    modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "    all_label_modules=[ str(i)+\"_\"+m  for i in range(3,31) for m in modules]\n",
    "else:\n",
    "    all_label_skills = all_skill_label\n",
    "    ground_truth = get_ground_truth(all_label_skills,cognitive_skills_community)\n",
    "    ground_truth = np.array([comm for  _, comm in ground_truth.items()])\n",
    "#plot_community_model(G,property[\"partition\"])\n",
    "\n",
    "data = {\"pruner_style\":[],\"model\":[],\"sparsity_ratio\":[], \"statistic\":[], \"p\":[], \"dof\":[], \"entropy\":[],\"data\":[]}\n",
    "for pruner_style in [\"block\",\"channel\"]:\n",
    "    for model_idx ,model in enumerate([\"llama\",\"llama_chat\",\"vicuna\"]):\n",
    "        for sparsity_ratio in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            G, property_1, (skills_modules, skill_label, module_label), (AB_dataset_skill, BC_dataset_modules, A_skill_modules) = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=0.01,alpha2=0.01,random_seed=True, modules_vs_modules=modules_vs_modules)\n",
    "            if modules_vs_modules:\n",
    "                community_detected_frequency = get_skills_shared_by_modules(G,property_1[\"community\"],skills_modules,skill_label, module_label, all_label_skills)\n",
    "            else:\n",
    "                community_detected_frequency = get_modules_shared_by_skills(G,property_1[\"community\"],skills_modules,skill_label, module_label, all_label_modules)\n",
    "            \n",
    "            '''for idx, c in enumerate(community_detected_frequency):\n",
    "                print(\"Given\",c, community_detected_frequency[c])\n",
    "                print(\"Expected\",c, res.expected_freq[idx])\n",
    "            print(\"*\"*100)'''\n",
    "        \n",
    "            entropy_matrix = calculate_kl_divergence(community_detected_frequency)\n",
    "            feq_skills = np.array([freq for _, freq in community_detected_frequency.items()])\n",
    "            feq_cog_function = np.array([sum_cogn_function(freq,ground_truth) for _, freq in community_detected_frequency.items()])\n",
    "            feq = feq_cog_function\n",
    "            res = chi2_contingency(feq)\n",
    "            data[\"model\"].append(model)\n",
    "            data[\"pruner_style\"].append(pruner_style)\n",
    "            data[\"sparsity_ratio\"].append(sparsity_ratio)\n",
    "            data[\"statistic\"].append(res.statistic)\n",
    "            data[\"p\"].append(res.pvalue)\n",
    "            data[\"dof\"].append(res.dof)\n",
    "            data[\"entropy\"].append(np.sum(entropy_matrix))\n",
    "            data[\"data\"].append(feq)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"result/compare_chi_square_cog_functions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot(data, measure= \"p\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df_block = df[df.pruner_style == \"block\"]\n",
    "    df_channel = df[df.pruner_style == \"channel\"]\n",
    "    fig, [ax0, ax1] = plt.subplots(figsize=(20, 6), ncols=2)\n",
    "    sns.lineplot(data=df_block, x=\"sparsity_ratio\", y=measure, hue=\"model\",ax=ax0 ,marker=\"o\")\n",
    "    ax0.set_title(\"Block Based Pruning\")\n",
    "    ax0.set_xlabel('Sparsity Ratio (%)')\n",
    "    ax0.set_ylabel(f'{measure}')\n",
    "    ax0.legend(title='Model')\n",
    "    #ax0.set_ylim([0, 1])\n",
    "    ax0.grid()\n",
    "    sns.lineplot(data=df_channel, x=\"sparsity_ratio\", y=measure, hue=\"model\",ax=ax1 ,marker=\"o\")\n",
    "    ax1.set_title(\"Channel Based Pruning\")\n",
    "    ax1.set_xlabel('Sparsity Ratio (%)')\n",
    "    ax1.set_ylabel(f'{measure}')\n",
    "    ax1.legend(title='Model')\n",
    "    #ax1.set_ylim([0, 1])\n",
    "    ax1.grid()\n",
    "    fig.suptitle(f'Chi Squared {measure.capitalize()} vs Sparsity Ratio for Different Models')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "line_plot(data, measure=\"statistic\")\n",
    "line_plot(data, measure=\"p\")\n",
    "line_plot(data, measure=\"dof\")\n",
    "line_plot(data, measure=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check all the distribution of data cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "random_seed = False\n",
    "plot_graph = True\n",
    "all_label_skills = all_skill_label\n",
    "ground_truth = get_ground_truth(all_label_skills,cognitive_skills_community)\n",
    "ground_truth = np.array([comm for  _, comm in ground_truth.items()])\n",
    "sparsity_ratio = \"25\"\n",
    "pruner_style = \"block\"\n",
    "model_idx, model = 0, \"llama\"\n",
    "#for sparsity_ratio in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "for sparsity_ratio in [\"25\"]:\n",
    "    print(sparsity_ratio)\n",
    "    G, property_1, (skills_modules, skill_label, module_label) = get_community_for_alpha(dataCategory1, dataset_list, distribution_dist[model_idx], original_dist[model_idx], pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=0.01,alpha2=0.01,random_seed=random_seed)\n",
    "    community_detected_frequency = get_skills_shared_by_modules(G,property_1[\"community\"],skills_modules,skill_label, module_label, all_label_skills)\n",
    "    dataset_frequency = create_frequency_skills(dataCategory1, all_label_skills)\n",
    "    community_detected_frequency_plot = copy.deepcopy(community_detected_frequency)\n",
    "    community_detected_frequency_plot[\"Skills\"] = all_label_skills\n",
    "    community_detected_frequency_plot[\"Cognitive Function\"] = []\n",
    "    for skill in all_label_skills:\n",
    "        for cog_function, skills_within_cog_function in cognitive_skills_community.items():\n",
    "            if skill in skills_within_cog_function:\n",
    "                community_detected_frequency_plot[\"Cognitive Function\"].append(cog_function)\n",
    "    df = pd.DataFrame.from_dict(community_detected_frequency_plot)\n",
    "    kl_based_data = check_kl_divergence(community_detected_frequency,dataset_frequency)\n",
    "    for comm in kl_based_data:\n",
    "        print(comm, kl_based_data[comm])\n",
    "    if plot_graph:\n",
    "        #plot_community(G,property_1[\"partition\"])\n",
    "        fig, axis = plt.subplots(figsize=(15,5*(len(community_detected_frequency))), nrows= len(community_detected_frequency), sharex=True)\n",
    "        color = matplotlib.colormaps['Accent'].colors\n",
    "        for idx, comm in enumerate(community_detected_frequency):\n",
    "            try:\n",
    "                sns.histplot(df, x=\"Skills\", weights=comm,color=color[int(comm)],hue=\"Cognitive Function\", ax=axis[idx], discrete=True,common_norm=True,\n",
    "                        edgecolor='black',\n",
    "                        kde=True, kde_kws={\"bw_adjust\":.15,'cut': 0}, line_kws={'linewidth': 4})\n",
    "            except:\n",
    "                sns.histplot(df, x=\"Skills\", weights=comm,color=color[int(comm)],hue=\"Cognitive Function\", ax=axis[idx], discrete=True,common_norm=True,\n",
    "                    edgecolor='black')\n",
    "            axis[idx].set_title(f\"Community {comm}\", fontsize=24)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title\n",
    "        plt.show()\n",
    "    print(\"++\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Network Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skill_skill = {\"model\":[],\"sparsity_value\": [], \"strategy\":[], \"average_degree\":[],\"average_cluster\":[],\"modularity\":[],\"density\":[],\"num_community\":[],\"global_efficiency\":[],\"diameter\":[],\"assortativity_coefficient\":[],\"average_path_length\":[]}\n",
    "print(data_skill_skill.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.bipartite_multipartite_projection import *\n",
    "from utils.bag_of_words.network_property import *\n",
    "\n",
    "AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory1, dataset_list, plot=False)\n",
    "llama = (\"llama\",llama_distribution,llama_original)\n",
    "vicuna = (\"vicuna\",vicuna_distribution,vicuna_original)\n",
    "llama_chat = (\"llama_chat\",llama_chat_distribution,llama_chat_original)\n",
    "global_property = ['average_degree', 'average_cluster', 'modularity', 'density', 'num_community', 'global_efficiency', 'diameter', 'assortativity_coefficient', 'average_path_length']\n",
    "data_skill_skill =  {\"model\":[],\"sparsity_value\": [], \"strategy\":[]}\n",
    "data_module_module = {\"model\":[],\"sparsity_value\": [], \"strategy\":[]}\n",
    "for properties in global_property:\n",
    "    data_skill_skill[properties] = []\n",
    "    data_module_module[properties] = []\n",
    "\n",
    "for strategy in [\"block\",\"channel\"]:\n",
    "    for model_name,pruned_distribution, original_distribution in [llama,vicuna,llama_chat]:\n",
    "        for sparsity_value in [\"3\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\"]:\n",
    "            BC_dataset_modules, module_label = create_plot_bog_modules(pruned_distribution,original_distribution, dataset_list,pruner_style=strategy, pruner_ratio=sparsity_value,norm=\"|W|_0\",plot=False, alpha=0.01)\n",
    "            A_skill_modules =  np.dot(AB_dataset_skill.T,BC_dataset_modules)\n",
    "            A_skill_modules = spectral_sparsification(A_skill_modules, alpha=0.01)\n",
    "            A_skill_skill,A_modules_modules  = get_projection(A_skill_modules, plot_projection= False)\n",
    "            _, property_module = get_network_property(A_modules_modules,module_label,module_label)\n",
    "            _, property_skill = get_network_property(A_skill_skill,skill_label,skill_label)\n",
    "\n",
    "            ## collect data\n",
    "            data_module_module[\"model\"].append(model_name)\n",
    "            data_module_module[\"sparsity_value\"].append(sparsity_value)\n",
    "            data_module_module[\"strategy\"].append(strategy)\n",
    "\n",
    "            data_skill_skill[\"model\"].append(model_name)\n",
    "            data_skill_skill[\"sparsity_value\"].append(sparsity_value)\n",
    "            data_skill_skill[\"strategy\"].append(strategy)\n",
    "\n",
    "            for properties in global_property:\n",
    "                data_module_module[properties].append(property_module[properties])\n",
    "                data_skill_skill[properties].append(property_skill[properties])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules vs Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_plot(data, property):\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    data_block = data[data[\"strategy\"] == \"block\"]\n",
    "    data_channel = data[data[\"strategy\"] == \"channel\"]\n",
    "    fig, (block_ax, channel_ax) = plt.subplots(figsize=(12,4) ,ncols=2)\n",
    "    sns.lineplot(data=data_block, x=\"sparsity_value\", y=property, hue=\"model\", ax=block_ax)\n",
    "    block_ax.set_title(\"Block Strategy\")\n",
    "    sns.lineplot(data=data_channel, x=\"sparsity_value\", y=property, hue=\"model\", ax=channel_ax)\n",
    "    channel_ax.set_title(\"Channel Strategy\")\n",
    "    fig.suptitle(property)\n",
    "    plt.plot()\n",
    "for properties in global_property:\n",
    "    plot_line_plot(data_module_module, properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills vs Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for properties in global_property:\n",
    "    plot_line_plot(data_skill_skill, properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Network Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.projection_community import *\n",
    "pruner_style=\"block\"\n",
    "sparsity_ratio=\"20\"\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_random_seed(0)\n",
    "G, property, (dataset_modules,skills_modules, skill_label, module_label) = get_community_for_alpha(dataCategory1, dataset_list, llama_distribution, llama_original, pruner_style=pruner_style, sparsity_ratio=sparsity_ratio,alpha1=0.01,alpha2=0.01)\n",
    "\n",
    "nx.write_edgelist(G, \"graph/modules_modules.csv\", data=[\"weight\"])\n",
    "data = {\"node\":[],\"community\":[],\"layer\":[]}\n",
    "\n",
    "modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "all_label_modules=[ str(i)+\"_\"+m  for i in range(0,31) for m in modules]\n",
    "community = property[\"partition\"]\n",
    "for node in all_label_modules:\n",
    "    data[\"node\"].append(node)\n",
    "    if node in community:   \n",
    "        data[\"community\"].append(community[node])\n",
    "    else:\n",
    "        data[\"community\"].append(-100) \n",
    "    data[\"layer\"].append(node.split(\"_\")[0])\n",
    "    \n",
    "node_list = pd.DataFrame.from_dict(data)\n",
    "node_list.to_csv(\"graph/modules_modules_node.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance between train certain Modules, all Modules and None Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def fill_nan(df):\n",
    "    df.dropna()\n",
    "    #df[\"finetune\"] = df[\"finetune\"].fillna(\"None\")\n",
    "    #df[\"rank\"] = df[\"rank\"].fillna(\"None\")\n",
    "    return df\n",
    "#df = pd.read_csv('result/randomize_accuracy/temp.csv')\n",
    "df_0 = pd.read_csv('result/randomize_accuracy/randomize_data_new_kl_0.csv')\n",
    "df_2 = pd.read_csv('result/randomize_accuracy/randomize_data_new_kl_2.csv')\n",
    "con_df= df_2#pd.concat([fill_nan(df_0), fill_nan(df_2)])\n",
    "# Group by all columns except for 'accuracy' and 'l2', then calculate the mean for 'accuracy' and 'l2'\n",
    "df = con_df.groupby(['iteration','model', 'pruning_style', 'community', 'pruning_ratio', 'dataset', 'rank', 'modules', 'modules_size', 'finetune',\"min_loss\"]).agg({'accuracy': 'mean'}).reset_index()\n",
    "#,iteration,model,pruning_style,community,pruning_ratio,dataset,accuracy,rank,modules,modules_size,finetune,l2\n",
    "#df[\"run_name\"] = df[\"run_name\"].fillna(\"None\")\n",
    "df = df.sort_values(['rank',\"finetune\"])\n",
    "#df = df[(df[\"modules_size\"].astype(int)>10) | (df[\"finetune\"]== \"All\")]\n",
    "df.to_csv(\"result/compare_impact_finetuning.csv\",index=False)\n",
    "\n",
    "'''for model in [\"llama_chat\"]:\n",
    "    df_model = df[df[\"model\"] == model ]\n",
    "    for strategy in [\"channel\"]:\n",
    "        df_strategy = df_model[df_model[\"pruning_style\"] == strategy ]\n",
    "        for community in [0,1,2,3,4]:\n",
    "            df_comm = df_strategy[df_strategy[\"community\"] == community ]\n",
    "            # Plot the data using seaborn\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            mod_size = np.unique(df_comm[\"modules_size\"].tolist())[-1]\n",
    "            print(mod_size)\n",
    "            #sns.barplot(x='dataset_name', y='accuracy', hue='run_name', data=df_comm)\n",
    "            sns.barplot(x='dataset', y='accuracy', hue='finetune',hue_order=[\"Community\",\"Random\",\"All\"], data=df_comm)\n",
    "            \n",
    "            plt.xlabel('Dataset')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Comparison of Accuracy Values Strategy {strategy} using {model} model for {community} community ({mod_size} modules) ')\n",
    "            plt.xticks(rotation=20)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Display the plot\n",
    "            plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def perform_stat_test(data1,data2, data1_label, data2_label, alternate='two-sided'):\n",
    "    \n",
    "    # Perform Shapiro-Wilk test to check for normality of the differences\n",
    "    '''\n",
    "    print(\"\\t\\tShapiro-Wilk test\")\n",
    "    stat, p_value_normality = stats.shapiro(data1 - data2)\n",
    "    print(f\"\\t\\tNormality Test Statistic: {stat}, P-value: {p_value_normality}\")\n",
    "    if p_value_normality > 0.05:\n",
    "        print(\"\\t\\tDifferences are normally distributed.\")\n",
    "        print(\"\\t\\tPaired t-test\")\n",
    "        # Perform the paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(data1, data2)\n",
    "\n",
    "        print(f\"\\t\\tPaired t-test T-statistic: {t_stat}, Two-tailed P-value: {p_value}\")\n",
    "\n",
    "        # Interpret the results\n",
    "        if p_value < 0.05:\n",
    "            print(f\"\\t\\tReject the null hypothesis: There is a significant difference between {data1_label} and {data2_label} accuracies.\")\n",
    "       \n",
    "        else:\n",
    "            print(f\"\\t\\tFail to reject the null hypothesis: There is no significant difference between {data1_label} and {data2_label} accuracies.\")\n",
    "        print(\"=\"*100)\n",
    "    else:\n",
    "        print(\"\\t\\tDifferences are not normally distributed. Consider non-parametric tests if needed.\")'''\n",
    "        \n",
    "    print(\"\\t\\tWilcox Test\")\n",
    "    # This code is ready to run, provided you input the actual accuracy data.\n",
    "    #uivalent within the margin of delta.\")\n",
    "    difference = np.around(data1 - data2,decimals=3)\n",
    "    #difference[np.where(difference == 0)] = 1e-3\n",
    "    stat, p_value = stats.wilcoxon(difference,alternative=alternate, correction=True, method=\"approx\")\n",
    "\n",
    "    print(f\"\\t\\tWilcoxon signed-rank test Statistic: {stat}, P-value: {p_value}\")\n",
    "\n",
    "    # Interpret the results\n",
    "    if p_value < 0.05:\n",
    "        print(f\"\\t\\tReject the null hypothesis: There is a significant difference between {data1_label} and {data2_label} accuracies.\")\n",
    "        if alternate == \"greater\" or alternate == \"smaller\":\n",
    "            print(f\"\\t\\t\\t {data1_label} Accuracy is {alternate} than {data2_label} accuracy.\")\n",
    "        else:\n",
    "            print(f\"\\t\\t\\t {data1_label} Accuracy is not similar than {data2_label} accuracy.\")\n",
    "    else:\n",
    "        print(f\"\\t\\tFail to reject the null hypothesis: There is no significant difference between {data1_label} and {data2_label} accuracies.\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    return stat,p_value \n",
    "\n",
    "# Bland-Altman Plot\n",
    "def bland_altman_plot(data1, data2, data1_label, data2_label, alternate):\n",
    "    data1, data2 = np.array(data1),np.array(data2)\n",
    "    mean = (data1 + data2) / 2\n",
    "    diff = data1 - data2\n",
    "    md = diff.mean()\n",
    "    sd = diff.std()\n",
    "    wilcoxon_statistic, p_value = perform_stat_test(data1,data2, data1_label, data2_label, alternate=alternate) \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(mean, diff, alpha=0.5)\n",
    "    plt.axhline(md, color='gray', linestyle='--')\n",
    "    plt.axhline(md + 1.96*sd, color='red', linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='red', linestyle='--')\n",
    "\n",
    "    plt.text(0.05, 0.95, f'Wilcoxon Statistic: {wilcoxon_statistic}\\nP-Value: {p_value:.5f}', \n",
    "         transform=plt.gca().transAxes, \n",
    "         fontsize=12, \n",
    "         verticalalignment='top',\n",
    "         bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.xlabel(f'Mean of Accuracy {data1_label} and Accuracy {data2_label}')\n",
    "    plt.ylabel(f'Accuracy {data1_label} - Accuracy {data2_label}')\n",
    "    plt.title(f'Bland-Altman Plot ({data1_label} vs {data2_label})')\n",
    "    plt.show()\n",
    "    return pd.DataFrame.from_dict({data1_label:data1, data2_label:data2})\n",
    "\n",
    "for m in [\"all\",\"llama\",\"llama_chat\",\"vicuna\"]:\n",
    "    if m == \"all\":\n",
    "        df_model = df\n",
    "    else:\n",
    "        df_model = df[df[\"model\"] == m]\n",
    "    for pruning_strategy in [\"block\",\"channel\"]:\n",
    "        df_pruning = df_model[df_model[\"pruning_style\"] == pruning_strategy ]\n",
    "\n",
    "        acc = df_pruning.groupby('finetune').aggregate(lambda tdf: tdf.tolist())[\"accuracy\"]\n",
    "        dataset = df_pruning.groupby('finetune').aggregate(lambda tdf: tdf.tolist())[\"dataset\"][0]\n",
    "        all, comm, rand = acc.iloc[0], acc.iloc[1], acc.iloc[2]\n",
    "        print(f\"{m} | {pruning_strategy}\")\n",
    "        \n",
    "        data = bland_altman_plot(comm, all, \"Community\", \"All\",alternate='greater' )\n",
    "        data.to_csv(f\"result/bland_altman/{m}_{pruning_strategy}_comm_all.csv\",index=False)\n",
    "        data = bland_altman_plot(rand, all, \"Random\", \"All\",alternate='two-sided' )\n",
    "        data.to_csv(f\"result/bland_altman/{m}_{pruning_strategy}_rand_all.csv\",index=False)\n",
    "        data = bland_altman_plot(comm, rand, \"Community\", \"Rand\",alternate='greater' )\n",
    "        data.to_csv(f\"result/bland_altman/{m}_{pruning_strategy}_comm_rand.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "\n",
    "for m in [\"all\",\"llama\",\"vicuana\"]:\n",
    "    if m == \"all\":\n",
    "        df_model = df\n",
    "    else:\n",
    "        df_model = df[df[\"model\"] == m]\n",
    "    for pruning_strategy in [\"block\",\"channel\"]:\n",
    "        df_pruning = df_model[df_model[\"pruning_style\"] == pruning_strategy ]\n",
    "        acc = df_pruning.groupby('finetune').aggregate(lambda tdf: tdf.tolist())[\"accuracy\"]\n",
    "        dataset = df_pruning.groupby('finetune').aggregate(lambda tdf: tdf.tolist())[\"dataset\"][0]\n",
    "        all, comm,  rand = acc.iloc[0], acc.iloc[1], acc.iloc[2]\n",
    "        all, comm, rand = np.array(all),np.array(comm),np.array(rand)\n",
    "\n",
    "        print(\"MODEL: \", m,pruning_strategy)\n",
    "        print(f\"\\t {pruning_strategy}: Accuracy Community vs All \")\n",
    "        perform_stat_test(comm,all,\"Community\", \"All\", alternate=\"greater\")\n",
    "        print(f\"\\t {pruning_strategy}:Accuracy Random vs All \")\n",
    "        perform_stat_test(rand,all,\"Random\", \"All\",alternate='two-sided')\n",
    "        print(f\"\\t {pruning_strategy}:Accuracy Community vs Random \")\n",
    "        perform_stat_test(comm,rand,\"Community\", \"Random\",alternate=\"greater\")\n",
    "\n",
    "\n",
    "    #print(\"\\tAccuracy Community vs None \")\n",
    "    #perform_stat_test(comm,none,\"Community\", \"None\", alternate=\"greater\")\n",
    "    #print(\"\\tAccuracy Random vs None \")\n",
    "    #perform_stat_test(rand,none,\"Random\", \"None\",alternate='two-sided')      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNeuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
