{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import csv \n",
    "import json\n",
    "import re \n",
    "from ast import literal_eval\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "from collections import OrderedDict\n",
    "\n",
    "import functools\n",
    "from argparse import Namespace\n",
    "from utils.dataset import getData\n",
    "from utils.evaluation import evaluate, mcq_token_index,computeLogits\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaTokenizerFast\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from torch.optim import AdamW,Adadelta\n",
    "from utils.bag_of_words.projection_community import create_projection_network\n",
    "\n",
    "from pynvml import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def free_mem(model):\n",
    "    del_grad(model)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def get_model(model_name,just_tokenizer=False):\n",
    "    if model_name == \"llama\":\n",
    "        base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "    elif model_name == \"llama_chat\":\n",
    "        base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    elif model_name == \"vicuna\":\n",
    "        base_model = \"lmsys/vicuna-7b-v1.5\"\n",
    "    torch.cuda.empty_cache()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    if just_tokenizer:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        return tokenizer\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        #low_cpu_mem_usage=True, \n",
    "        #device_map=\"auto\"\n",
    "    )\n",
    "    #tokenizer.add_bos_token = False\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    #freezing the LM_HEAD\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def l_0_norm(vector):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        for sub_element in element:\n",
    "            if sub_element != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "def take_average(dict):\n",
    "    data = dict[\"0\"]\n",
    "    iterations_block = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    iterations_channel = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    #for style , iterations in zip ([\"block\",\"channel\",\"block_random\",\"channel_random\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "    for style , iterations in zip ([\"block\",\"channel\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "        for iter in iterations:\n",
    "            if iter == \"0\":\n",
    "                continue\n",
    "            for ratio in dict[iter][style]:\n",
    "                for dataset in dict[iter][style][ratio]:\n",
    "                    for norm in dict[iter][style][ratio][dataset]:\n",
    "                        value = np.array(dict[iter][style][ratio][dataset][norm])\n",
    "                        if len( value.shape) != 1:\n",
    "                            shape_model = value.shape\n",
    "                        data[style][ratio][dataset][norm]= (np.array(data[style][ratio][dataset][norm])+value)\n",
    "                        if iter == iterations[-1]:\n",
    "                            data[style][ratio][dataset][norm] = data[style][ratio][dataset][norm]/len(iterations)\n",
    "    return data, shape_model\n",
    "def strip(name):\n",
    "    name = name.split(\"/\")[-1]\n",
    "    name = name.split(\"_\")[0]\n",
    "    return name \n",
    "\n",
    "def loop_over(dict):\n",
    "    if isinstance(dict, list):\n",
    "        print(\"end\")\n",
    "    else: \n",
    "        print(dict.keys())\n",
    "        for keys in dict:\n",
    "            loop_over(dict[keys])\n",
    "        \n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append(subset)\n",
    "    return dataname\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find the layers of a certain type in a module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): PyTorch module.\n",
    "        layers (list): List of layer types to find.\n",
    "        name (str): Name of the module.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layers of the given type(s) within the module.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res\n",
    "\n",
    "def create_distribution_llm_pruner(model):\n",
    "    layers = model.model.layers\n",
    "    distribution_2 = []\n",
    "    count = 0 \n",
    "    total_params = 0\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "        layer_values_2 = []\n",
    "        for name in subset:\n",
    "            W = subset[name].weight.data\n",
    "            count += (W==0).sum().item()\n",
    "            total_params += W.numel()\n",
    "            layer_values_2.append(torch.linalg.matrix_norm(W, ord=float(\"Inf\")).item()) #|W|_inf norm\n",
    "        distribution_2.append(layer_values_2)\n",
    "    return  np.array(distribution_2)\n",
    "\n",
    " \n",
    "def freeze_all_model(model):\n",
    "    #all_self_modules = [f\"{m.split('_')[1]}_proj\" for m in modules_list]+[f\"self_{m.split('_')[1]}_proj\" for m in modules_list]\n",
    "    modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "    all_modules = [f\"{i}_{m}\"  for i in range(3,31) for m in modules]\n",
    "    layers = model.model.layers\n",
    "    for idx_layer in range(len(layers)):\n",
    "        layer = layers[idx_layer]\n",
    "        for name1, child1 in layer.named_children():\n",
    "            for name2, child2 in child1.named_children():\n",
    "                if f\"{idx_layer}_{name1.split('_')[-1]}.{name2.split('_')[0]}\" in all_modules:\n",
    "                    for param in child2.parameters():\n",
    "                        param.requires_grad = True  \n",
    "                else:\n",
    "                    for param in child2.parameters():\n",
    "                        param.requires_grad = False  \n",
    "    return model\n",
    "\n",
    "def freeze_subset_model(model, modules_list):\n",
    "    #all_self_modules = [f\"{m.split('_')[1]}_proj\" for m in modules_list]+[f\"self_{m.split('_')[1]}_proj\" for m in modules_list]\n",
    "    layers = model.model.layers\n",
    "    for idx_layer in range(len(layers)):\n",
    "        layer = layers[idx_layer]\n",
    "        for name1, child1 in layer.named_children():\n",
    "            for name2, child2 in child1.named_children():\n",
    "                if f\"{idx_layer}_{name1.split('_')[-1]}.{name2.split('_')[0]}\" in modules_list:\n",
    "                    for param in child2.parameters():\n",
    "                        param.requires_grad = True  \n",
    "                else:\n",
    "                    for param in child2.parameters():\n",
    "                        param.requires_grad = False  \n",
    "    return model\n",
    "\n",
    "\n",
    "def del_grad(model):\n",
    "    #all_self_modules = [f\"{m.split('_')[1]}_proj\" for m in modules_list]+[f\"self_{m.split('_')[1]}_proj\" for m in modules_list]\n",
    "    layers = model.model.layers\n",
    "    for idx_layer in range(len(layers)):\n",
    "        layer = layers[idx_layer]\n",
    "        for name1, child1 in layer.named_children():\n",
    "            for name2, child2 in child1.named_children():\n",
    "                for param in child2.parameters():\n",
    "                    if param.requires_grad:\n",
    "                        del param.grad\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_l2_norm(model):\n",
    "    l2_norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            l2_norm += torch.norm(param).item()\n",
    "    return l2_norm\n",
    "\n",
    "def get_high_datasets(ranked_dataset, top_skill= 50): \n",
    "    return ranked_dataset[:top_skill]\n",
    "def flatten_comprehension(matrix):\n",
    "     return [item for row in matrix for item in row]\n",
    "\n",
    "def get_all_dataset_list(dataset_info_list, dataset_list):\n",
    "    dataname = []\n",
    "    for d in dataset_list:\n",
    "        for data in dataset_info_list:\n",
    "            if \"subset\" not in dataset_info_list[data].keys():\n",
    "                if  data == d:\n",
    "                    dataname.append(data)\n",
    "                    continue\n",
    "            else:\n",
    "                if d in dataset_info_list[data][\"subset\"]:\n",
    "                    dataname.append([data,d])\n",
    "                    continue\n",
    "    return dataname\n",
    "\n",
    "def get_modulesCommunityDataset(sparsity_ratio):\n",
    "    with open(\"./dataset_info.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        dataset_info_list = json.load(openfile)\n",
    "    dataset_list = get_dataset_list(dataset_info_list)\n",
    "    #Original Distribution\n",
    "    with open(\"result/original_distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "        vicuna_original = json.load(openfile)\n",
    "    with open(\"result/original_distribution_llama_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_original = json.load(openfile)\n",
    "    with open(\"result/original_distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_chat_original = json.load(openfile)\n",
    "    #Pruned Distribution\n",
    "    with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_distribution = json.load(openfile)\n",
    "    with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        vicuna_distribution = json.load(openfile)\n",
    "    with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        llama_chat_distribution= json.load(openfile)\n",
    "    with open(\"result/dataMultidisciplinaryCognitiveSkillsFrameworkRestrict.json\", 'r') as openfile:\n",
    "        dataCategory = json.load(openfile)\n",
    "\n",
    "    llama_distribution, _ = take_average(llama_distribution)\n",
    "    vicuna_distribution, _ = take_average(vicuna_distribution)\n",
    "    llama_chat_distribution, _ = take_average(llama_chat_distribution)\n",
    "    \n",
    "    distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "    original_dist = [llama_original,llama_chat_original,vicuna_original]    \n",
    "    modules_community_dataset = create_projection_network(dataCategory,dataset_list, distribution_dist, original_dist, sparsity_ratio = sparsity_ratio)\n",
    "    return modules_community_dataset,dataset_info_list, dataset_list\n",
    "\n",
    "def adjust_number(element,subset, all_elements, new_subset):\n",
    "    parts = element.split('_')\n",
    "    number_part = int(parts[0]) \n",
    "    rest = '_'.join(parts[1:])   \n",
    "    count = 0\n",
    "    while True:\n",
    "        adjusted_number = random.choice(list(np.arange(number_part - 1-count, number_part + 1+count)))\n",
    "        new_element = f'{adjusted_number}_{rest}'\n",
    "        if new_element in all_elements and new_element not in new_subset and new_element not in subset:\n",
    "            return new_element\n",
    "        count +=1\n",
    "\n",
    "def create_random_modules_set(all_modules, modules_list):\n",
    "    new_subset = []\n",
    "    for elem in modules_list:\n",
    "        new_elem = adjust_number(elem,modules_list,all_modules, new_subset)\n",
    "        new_subset.append(new_elem)\n",
    "    return new_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_weights(state_dict, modules):\n",
    "    weights = {}\n",
    "    for module_name in modules:\n",
    "        for key in state_dict.keys():\n",
    "            if \"layers\" not in key:\n",
    "                continue\n",
    "            key_split= key.split(\".\")[2:5]\n",
    "            if \"gate\" in key:\n",
    "                format_module_key = f\"{key_split[0]}_{key_split[2].split('_')[0]}\"\n",
    "            else:\n",
    "                format_module_key = f\"{key_split[0]}_{key_split[1].split('_')[-1]}.{key_split[2].split('_')[0]}\"\n",
    "            if module_name in format_module_key:  # Match affected module names\n",
    "                weights[module_name] = state_dict[key].detach().cpu()\n",
    "    del state_dict\n",
    "    return weights\n",
    "\n",
    "def compare_weights(base, fine_tuned):\n",
    "    differences = {}\n",
    "    for key in base.keys():\n",
    "        if key in fine_tuned:\n",
    "            base_param = base[key].numpy().flatten()\n",
    "            fine_tuned_param = fine_tuned[key].numpy().flatten()\n",
    "            #print(\"mean_absolute_difference\", np.mean(np.abs(base_param - fine_tuned_param)))\n",
    "            differences[key] = {\n",
    "                \"cosine_similarity\": 1 - cosine(base_param, fine_tuned_param),\n",
    "                \"euclidean_distance\": np.linalg.norm(base_param - fine_tuned_param),\n",
    "                \"mean_absolute_difference\": np.mean(np.abs(base_param - fine_tuned_param))\n",
    "            }\n",
    "            '''print( {\n",
    "                \"cosine_similarity\": 1 - cosine(base_param, fine_tuned_param),\n",
    "                \"euclidean_distance\": np.linalg.norm(base_param - fine_tuned_param),\n",
    "             #   \"mean_absolute_difference\": np.mean(np.abs(base_param - fine_tuned_param))\n",
    "            })'''\n",
    "    return differences\n",
    "\n",
    "def plot_differences(differences, title, metric1, metric2=None, metric3=None):\n",
    "    modules = list(differences.keys())\n",
    "    if metric2 == None or metric3 == None:\n",
    "        values = [differences[module][metric1] for module in modules]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(modules, values)\n",
    "        plt.title(f\"{title} - {metric1}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel(metric1)\n",
    "        plt.xlabel(\"Modules\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        values_metric1 = [differences[module][metric1] for module in modules]\n",
    "        values_metric2 = [differences[module][metric2] for module in modules]\n",
    "        values_metric3 = [differences[module][metric3] for module in modules]\n",
    "\n",
    "        fig, [ax1, ax2,ax3] = plt.subplots(figsize=(20, 6),ncols=3)\n",
    "        ax1.bar(modules, values_metric1)\n",
    "        ax1.set_title(f\"{title} - {metric1}\")\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(),rotation=90)\n",
    "        ax1.set_ylabel(metric1)\n",
    "        ax1.set_xlabel(\"Modules\")\n",
    "\n",
    "        ax2.bar(modules, values_metric2)\n",
    "        ax2.set_title(f\"{title} - {metric2}\")\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(),rotation=90)\n",
    "        ax2.set_ylabel(metric2)\n",
    "        ax2.set_xlabel(\"Modules\")\n",
    "\n",
    "        ax3.bar(modules, values_metric3)\n",
    "        ax3.set_title(f\"{title} - {metric3}\")\n",
    "        ax3.set_xticklabels(ax3.get_xticklabels(),rotation=90)\n",
    "        ax3.set_ylabel(metric3)\n",
    "        ax3.set_xlabel(\"Modules\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_random_seed(2)\n",
    "sparsity_ratio = \"20\"\n",
    "modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "all_modules = [f\"{i}_{m}\"  for i in range(3,31) for m in modules]\n",
    "model_path = \"/data/Kushal/MLNeuron/checkpoint\"\n",
    "#modules_community_dataset,dataset_info_list, dataset_list = get_modulesCommunityDataset(sparsity_ratio)\n",
    "data = pd.read_csv(\"result/randomize_accuracy/randomize_data_new_kl_longer_2.csv\")\n",
    "data[\"modules\"] = data[\"modules\"].apply(literal_eval)\n",
    "grouped_dict = data.groupby(['model', 'pruning_style', 'community', 'pruning_ratio', 'dataset', \"modules_size\", \"rank\",  \"training_dataset_size\",\"validation_dataset_size\" ])\n",
    "for _,grouped in grouped_dict:\n",
    "    model_name = grouped[\"model\"].iloc[0] \n",
    "    pruner_style = grouped[\"pruning_style\"].iloc[0] \n",
    "    comm_name = grouped[\"community\"].iloc[0] \n",
    "    dataset_name_label = grouped[\"dataset\"].iloc[0]\n",
    "     \n",
    "    module_list = grouped.loc[grouped['finetune'] == 'Community', 'modules'].iloc[0] \n",
    "    random_module_list = grouped.loc[grouped['finetune'] == 'Random', 'modules'].iloc[0] \n",
    "\n",
    "\n",
    "    base_model_data,_ = get_model(model_name)\n",
    "    base_model = base_model_data.state_dict()\n",
    "    base_weights = extract_weights(base_model, module_list+random_module_list)\n",
    "    print(\"Base\",base_weights.keys())\n",
    "    del base_model \n",
    "\n",
    "    \n",
    "    fine_tuned_all = torch.load(f\"{model_path}/{model_name}_{pruner_style}_{comm_name}_{dataset_name_label}_All_{5000}_{5}.pt\", map_location=torch.device(\"cpu\"))\n",
    "    all_weights = extract_weights(fine_tuned_all, module_list)\n",
    "    print(\"All\",all_weights.keys())\n",
    "    del fine_tuned_all \n",
    "    \n",
    "    fine_tuned_community =torch.load(f\"{model_path}/{model_name}_{pruner_style}_{comm_name}_{dataset_name_label}_Community_{5000}_{5}.pt\", map_location=torch.device(\"cpu\"))\n",
    "    community_weights = extract_weights(fine_tuned_community, module_list)\n",
    "    print(\"Community\",community_weights.keys())\n",
    "    del fine_tuned_community\n",
    "    \n",
    "    fine_tuned_random = torch.load(f\"{model_path}/{model_name}_{pruner_style}_{comm_name}_{dataset_name_label}_Random_{5000}_{5}.pt\", map_location=torch.device(\"cpu\"))\n",
    "    random_weights = extract_weights(fine_tuned_random, random_module_list)\n",
    "    print(\"Random\",random_weights.keys()) \n",
    "    del fine_tuned_random \n",
    "    \n",
    "    #Compare Weights\n",
    "    diff_all = compare_weights(base_weights, all_weights)\n",
    "    plot_differences(diff_all, \"All Fine-Tuned vs Base\", \"euclidean_distance\",\"cosine_similarity\",\"mean_absolute_difference\")\n",
    "    diff_community = compare_weights(base_weights, community_weights)\n",
    "    plot_differences(diff_community, \"Community Fine-Tuned vs Base\", \"euclidean_distance\",\"cosine_similarity\",\"mean_absolute_difference\")\n",
    "    diff_random = compare_weights(base_weights, random_weights)\n",
    "    plot_differences(diff_random, \"Random Fine-Tuned vs Base\", \"euclidean_distance\",\"cosine_similarity\",\"mean_absolute_difference\")\n",
    "    save_dictionary = {\n",
    "                    \"model\":model_name, \n",
    "                    \"pruning_style\":pruner_style,\n",
    "                    \"community\":comm_name,\n",
    "                    \"pruning_ratio\":sparsity_ratio,\n",
    "                    \"dataset\":dataset_name_label,\n",
    "                    \"modules\":module_list,\n",
    "                    \"random_modules\":random_module_list,\n",
    "                    #\"base_weight\":base_weights,\n",
    "                    #\"community_weight\":community_weights,\n",
    "                    #\"random_weight\":random_weights,\n",
    "                    #\"all_weights\":all_weights,\n",
    "                    \"diff_all\":diff_all,\n",
    "                    \"diff_community\":diff_community,\n",
    "                    \"diff_random\":diff_random \n",
    "                    }\n",
    "    with open(\"/home/bhandk/MLNeuron/result/randomize_accuracy/weight_comparison.csv\", mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=save_dictionary.keys())\n",
    "        writer.writerow({key: save_dictionary[key] for key in save_dictionary.keys()})\n",
    "print(\"\\n\",\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "def compare_modules(weights1, weights2):\n",
    "    differences = {}\n",
    "    for module_name in weights1.keys():\n",
    "        if module_name in weights2:\n",
    "            # Flatten weights for comparison\n",
    "            w1 = weights1[module_name].numpy().flatten()\n",
    "            w2 = weights2[module_name].numpy().flatten()\n",
    "            differences[module_name] = {\n",
    "                \"cosine_similarity\": 1 - cosine(w1, w2),\n",
    "                \"euclidean_distance\": np.linalg.norm(w1 - w2),\n",
    "            }\n",
    "    return differences\n",
    "\n",
    "differences_all_vs_community = compare_modules(weights_all, weights_community)\n",
    "differences_all_vs_random = compare_modules(weights_all, weights_random)\n",
    "differences_community_vs_random = compare_modules(weights_community, weights_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_differences(differences, title):\n",
    "    modules = list(differences.keys())\n",
    "    cosine_similarities = [differences[module][\"cosine_similarity\"] for module in modules]\n",
    "    euclidean_distances = [differences[module][\"euclidean_distance\"] for module in modules]\n",
    "\n",
    "    # Plot Cosine Similarity\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(modules, cosine_similarities)\n",
    "    plt.title(f\"{title} - Cosine Similarity\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Euclidean Distance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(modules, euclidean_distances)\n",
    "    plt.title(f\"{title} - Euclidean Distance\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_differences(differences_all_vs_community, \"All vs Community\")\n",
    "plot_differences(differences_all_vs_random, \"All vs Random\")\n",
    "plot_differences(differences_community_vs_random, \"Community vs Random\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNeuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
