{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import json \n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "#import itertools\n",
    "from utils.bag_of_words.skill_dataset import create_plot_bog_skills\n",
    "\n",
    "from utils.bag_of_words.bipartite_multipartite_projection import create_plot_bog_modules,spectral_sparsification,get_projection\n",
    "from utils.bag_of_words.network_property import get_network_property\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "def l_0_norm(vector):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        for sub_element in element:\n",
    "            if sub_element != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "def take_average(dict):\n",
    "    data = dict[\"0\"]\n",
    "    iterations_block = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    iterations_channel = list([\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    #for style , iterations in zip ([\"block\",\"channel\",\"block_random\",\"channel_random\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "    for style , iterations in zip ([\"block\",\"channel\"],[iterations_block,iterations_channel,iterations_block,iterations_channel]):\n",
    "        for iter in iterations:\n",
    "            if iter == \"0\":\n",
    "                continue\n",
    "            for ratio in dict[iter][style]:\n",
    "                for dataset in dict[iter][style][ratio]:\n",
    "                    for norm in dict[iter][style][ratio][dataset]:\n",
    "                        value = np.array(dict[iter][style][ratio][dataset][norm])\n",
    "                        if len( value.shape) != 1:\n",
    "                            shape_model = value.shape\n",
    "                        data[style][ratio][dataset][norm]= (np.array(data[style][ratio][dataset][norm])+value)\n",
    "                        if iter == iterations[-1]:\n",
    "                            data[style][ratio][dataset][norm] = data[style][ratio][dataset][norm]/len(iterations)\n",
    "    return data, shape_model\n",
    "\n",
    "def strip(name):\n",
    "    name = name.split(\"/\")[-1]\n",
    "    name = name.split(\"_\")[0]\n",
    "    return name \n",
    "\n",
    "with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    vicuna_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_distribution= json.load(openfile)\n",
    "def loop_over(dict):\n",
    "    if isinstance(dict, list):\n",
    "        print(\"end\")\n",
    "    else: \n",
    "        print(dict.keys())\n",
    "        for keys in dict:\n",
    "            loop_over(dict[keys])\n",
    "def get_dataset_list(dataset_list):\n",
    "    dataname = []\n",
    "    for data in dataset_list:\n",
    "        if \"subset\" not in dataset_list[data].keys():\n",
    "            dataname.append(data)\n",
    "        else:\n",
    "            for subset in dataset_list[data][\"subset\"]:\n",
    "                dataname.append(subset)\n",
    "    return dataname\n",
    "    #Dataset List\n",
    "with open(\"/home/bhandk/MLNeuron/dataset_info.json\", 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        dataset_list = json.load(openfile)\n",
    "#Original Distribution\n",
    "with open(\"result/original_distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    vicuna_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_original = json.load(openfile)\n",
    "with open(\"result/original_distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_original = json.load(openfile)\n",
    "#Pruned Distribution\n",
    "with open(\"result/distribution_llama_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_vicuna_7b.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    vicuna_distribution = json.load(openfile)\n",
    "with open(\"result/distribution_llama_7b-chat.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    llama_chat_distribution= json.load(openfile)\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_community= json.load(openfile)\n",
    "\n",
    "dataset_list = get_dataset_list(dataset_list)\n",
    "llama_distribution, model_shape = take_average(llama_distribution)\n",
    "vicuna_distribution, model_shape = take_average(vicuna_distribution)\n",
    "llama_chat_distribution, model_shape = take_average(llama_chat_distribution)\n",
    "\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataset_community= json.load(openfile)\n",
    "with open(\"result/dataMultidisciplinaryCognitiveSkillsFrameworkRestrict.json\", 'r') as openfile:\n",
    "    #with open(\"result/dataCategory.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataCategory = json.load(openfile)\n",
    "cognitive_skills_community = {\n",
    "                    \"cognitive_process_memory\":[ \n",
    "                        \"sustained_attention\", \"selective_attention\", \"divided_attention\", \"vigilance_attention\",\"attention_shifting\",\n",
    "                        \"processing_speed\", \"visual_processing_speed\", \"auditory_processing_speed\",\n",
    "                        \"prospective_memory\", \"working_memory\", \"episodic_memory\", \"semantic_memory\", \"procedural_memory\", \"iconic_memory\", \"echoic_memory\", \"spatial_memory\"],\n",
    "                    \"executive_function\":[ \n",
    "                        \"planning\", \"organization\", \"goal_setting\",\"time_management\", \n",
    "                        \"problem_solving\", \"mental_flexibility\", \"strategic_thinking\",\"adaptability\",\n",
    "                        \"impulse_control\", \"decision_making\",\"emotional_regulation\",\"risk_assessment\",\n",
    "                        \"abstract_thinking\", \"reasoning\", \"cognitive_flexibility\", \"creativity\"], #concept_formation\n",
    "                    \"language_communication\":[\n",
    "                         \"expressive_language\", \"receptive_language\", \"naming\", \"fluency\", \"comprehension\", \"repetition\", \"reading\", \"writing\", \n",
    "                         \"pragmatics\", \"discourse_ability\", \"linguistic_analysis\", \"narrative_skills\"],\n",
    "                    \"social_cognition\":\n",
    "                        [\"recognition_of_social_cues\", \"theory_of_mind\", \"empathy\", \"social_judgment\",\"intercultural_competence\",\"conflict_resolution\",\"self_awareness\",\"relationship_management\"]\n",
    "}\n",
    "all_skill_label = []\n",
    "for func, skill_list in cognitive_skills_community.items():\n",
    "    all_skill_label += skill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max(X):\n",
    "    max, mean,  min = X.max(),X.mean(),X.min() \n",
    "    X_std = (X - mean) / (max - min)\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "def get_community_for_alpha(dataCategory, dataset_list, distribution, original, pruner_style=\"block\", sparsity_ratio=\"15\",alpha1=None, alpha2=None, random_seed=True, modules_vs_modules=True):\n",
    "    AB_dataset_skill, skill_label = create_plot_bog_skills(dataCategory, dataset_list, plot=False)\n",
    "    BC_dataset_modules, module_label = create_plot_bog_modules(distribution,original, dataset_list,pruner_style=pruner_style, pruner_ratio=sparsity_ratio,norm=\"|W|_0\",plot=False, alpha=alpha1)\n",
    "    A_skill_modules =  np.dot(AB_dataset_skill.T,BC_dataset_modules)\n",
    "    sparse_network = spectral_sparsification(A_skill_modules, alpha2)\n",
    "    sparse_network =  sparse_network #min_max(sparse_network)\n",
    "    if modules_vs_modules:\n",
    "        _ ,A_modules_modules  = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_modules_modules,module_label,module_label )\n",
    "    else:\n",
    "        A_skills_skills , _ = get_projection(sparse_network, plot_projection= False)\n",
    "        G, network_property = get_network_property(A_skills_skills,skill_label,skill_label )\n",
    "    return G, network_property, (sparse_network,skill_label,module_label), (AB_dataset_skill, BC_dataset_modules, A_skill_modules)\n",
    "def collect_edge_weights(G, communities_dict):\n",
    "    community_weights = {\"community\":[],\"node\":[],\"neighbor\":[],\"weight\":[]}\n",
    "    community_node_weighted_sum = {\"community\":[],\"node\":[],\"weight\":[],\"degree\":[]} \n",
    "    for community, nodes in communities_dict.items():\n",
    "        #if len(nodes) < 10:\n",
    "        #    continue\n",
    "        subGraph = nx.subgraph(G,nodes)\n",
    "        for node in nodes:\n",
    "            sum = []\n",
    "            for neighbor in subGraph.neighbors(node):\n",
    "                if subGraph.has_edge(node, neighbor):\n",
    "                    community_weights[\"community\"].append(community)\n",
    "                    community_weights[\"node\"].append(node)\n",
    "                    community_weights[\"neighbor\"].append(neighbor)\n",
    "                    community_weights[\"weight\"].append(subGraph[node][neighbor].get('weight', 1))\n",
    "                    sum.append(community_weights[\"weight\"][-1])\n",
    "            community_node_weighted_sum[\"community\"].append(community)\n",
    "            community_node_weighted_sum[\"node\"].append(node)\n",
    "            community_node_weighted_sum[\"weight\"].append(np.sum(sum))\n",
    "            community_node_weighted_sum[\"degree\"].append(len(sum))\n",
    "    return pd.DataFrame.from_dict(community_weights),pd.DataFrame.from_dict(community_node_weighted_sum)\n",
    "def within_module_z_score(G, communities):\n",
    "    z_scores = {\"node\":[],\"community\":[],\"z_scores\":[]}\n",
    "    for comm_id, nodes in communities.items():\n",
    "        degrees = np.array([G.degree(node) for node in nodes])\n",
    "        mean_degree = np.mean(degrees)\n",
    "        std_degree = np.std(degrees) if len(degrees) > 1 else 1.0\n",
    "        for node in nodes:\n",
    "            z_scores[\"node\"].append(node)\n",
    "            z_scores[\"community\"].append(comm_id)\n",
    "            z_scores[\"z_scores\"].append((G.degree(node) - mean_degree) / std_degree)\n",
    "    return pd.DataFrame.from_dict(z_scores) \n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    part_coeff = {\"node\":[],\"community\":[],\"part_coeff\":[]}\n",
    "    total_degree = dict(G.degree())\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        community_id = partition[node]\n",
    "        degree = total_degree[node]\n",
    "        comm_degrees = sum([G.degree(neigh) for neigh in G.neighbors(node) if partition[neigh] == community_id])\n",
    "        \n",
    "        if degree > 0:\n",
    "            score = 1 - (comm_degrees / degree) ** 2\n",
    "        else:\n",
    "            score = 0\n",
    "        part_coeff[\"node\"].append(node)\n",
    "        part_coeff[\"community\"].append(community_id)\n",
    "        part_coeff[\"part_coeff\"].append(score)\n",
    "\n",
    "    return  pd.DataFrame.from_dict(part_coeff) \n",
    "def compute_correlation_community(df, model, pruning_strategy):\n",
    "    grouped = df.groupby('community')\n",
    "\n",
    "    # Store the correlation results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each community group\n",
    "    for community, group in grouped:\n",
    "        z_scores = group['z_scores']\n",
    "        part_coeff = group['part_coeff']\n",
    "        \n",
    "        # Calculate Pearson and Spearman correlation\n",
    "        pearson_corr, _ = pearsonr(z_scores, part_coeff)\n",
    "        spearman_corr, _ = spearmanr(z_scores, part_coeff)\n",
    "        \n",
    "        # Append results to a list\n",
    "        results.append({\n",
    "            'community': community,\n",
    "            'pearson_correlation': pearson_corr,\n",
    "            'spearman_correlation': spearman_corr,\n",
    "            'model':model,\n",
    "            'pruning_strategy':pruning_strategy \n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "distribution_dist = [llama_distribution,llama_chat_distribution,vicuna_distribution]\n",
    "original_dist = [llama_original,llama_chat_original,vicuna_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_data(nodes_skills, nodes_dataset, nodes_modules, name):\n",
    "    results = []\n",
    "    if nodes_dataset != None: \n",
    "        node_shape,node_label, node_data = [2,3,4],[\"modules\",\"dataset\",\"skill\"],[nodes_modules,nodes_dataset,nodes_skills] \n",
    "    else:\n",
    "        node_shape,node_label, node_data = [2,4],[\"modules\",\"skill\"],[nodes_modules,nodes_skills] \n",
    "    for id, (shape, node_type, node_list) in enumerate(zip(node_shape, node_label, node_data)):\n",
    "        for node in node_list:\n",
    "            if node_type == \"skill\":\n",
    "                label = node\n",
    "            elif node_type == \"modules\" and nodes_dataset == None:\n",
    "                label = node\n",
    "            else:  \n",
    "                label = None\n",
    "            results.append({\"Id\":node,\"type\":node_type,\"size\":id+2,\"[z]\":shape-1,\"Label\":label})\n",
    "    node_data =pd.DataFrame(results)\n",
    "    node_data.to_csv(f\"./graph/{name}\",index=False)\n",
    "def network_data(adjacency_matrix_AB, nodes_A, nodes_B,edge_type, name):\n",
    "    min_val = np.min(adjacency_matrix_AB)\n",
    "    max_val = np.max(adjacency_matrix_AB)\n",
    "\n",
    "    normalized_matrix = (adjacency_matrix_AB - min_val) / (max_val - min_val)\n",
    "    assert adjacency_matrix_AB.shape[0] == len(nodes_A)\n",
    "    assert adjacency_matrix_AB.shape[1] == len(nodes_B)\n",
    "    \n",
    "    if \"_datasets_skills\" in name:\n",
    "        normalized_matrix[normalized_matrix <= 0.3] = 0\n",
    "    elif \"_skill_modules\" in name:\n",
    "        normalized_matrix = spectral_sparsification(normalized_matrix, 0.5)\n",
    "    else:\n",
    "        normalized_matrix = spectral_sparsification(normalized_matrix, 0.05)\n",
    "    results = []\n",
    "    for node_i in range(adjacency_matrix_AB.shape[0]):\n",
    "        for node_j in range(adjacency_matrix_AB.shape[1]):\n",
    "            if normalized_matrix[node_i,node_j] == 0:\n",
    "                continue \n",
    "            #results.append({\"Source\":nodes_A[node_i], \"Target\":nodes_B[node_j],\"Weight\":normalized_matrix[node_i,node_j],\"Edge_Type\":edge_type})\n",
    "            results.append({\"Source\":nodes_A[node_i], \"Target\":nodes_B[node_j],\"Edge_Type\":edge_type})\n",
    "    edge_data =pd.DataFrame(results)\n",
    "    edge_data.to_csv(f\"./graph/{name}\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bag_of_words.projection_community import create_projection_network,all_skill_label\n",
    "sparsity_ratio = \"20\"\n",
    "set_random_seed(int(2))\n",
    "modules_community_dataset = create_projection_network(dataCategory,dataset_list, distribution_dist, original_dist, sparsity_ratio = sparsity_ratio,get_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "for idx, network_data in enumerate(modules_community_dataset[\"network_data\"]):\n",
    "    (skill_label,dataset_list,module_label),(skill_dataset, dataset_modules,skills_modules) = network_data\n",
    "    C = np.dot(skills_modules.T,skills_modules)\n",
    "    eigenvalues, eigenvectors = LA.eigh(C)\n",
    "    pruner_style = modules_community_dataset[\"pruner_style\"][idx]\n",
    "    model = modules_community_dataset[\"model\"][idx]\n",
    "    # Compute cosine similarity between all pairs of eigenvectors\n",
    "    num_vectors = len(eigenvectors)\n",
    "    cosine_similarities = np.zeros((num_vectors, num_vectors))\n",
    "\n",
    "    for i in range(num_vectors):\n",
    "        for j in range(num_vectors):\n",
    "            cosine_similarities[i, j] = cosine_similarity(eigenvectors[i], eigenvectors[j])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cosine_similarities, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label=\"Cosine Similarity\")\n",
    "    plt.title(\"Heatmap of Cosine Similarities Between Eigenvectors\")\n",
    "    plt.xticks(range(num_vectors), [f\"Vec_{i}\" for i in range(num_vectors)], rotation=45)\n",
    "    plt.yticks(range(num_vectors), [f\"Vec_{i}\" for i in range(num_vectors)])\n",
    "    plt.xlabel(\"Eigenvectors\")\n",
    "    plt.ylabel(\"Eigenvectors\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    ax = sns.heatmap(C, cmap='viridis',norm=LogNorm())\n",
    "    plt.xlabel('Modules')\n",
    "    plt.ylabel('Modules')\n",
    "    plt.title(f\"{model}|{pruner_style}\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(eigenvalues, bins=10, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of EigenValues|{model}|{pruner_style}')\n",
    "    plt.xlabel('EigenValues')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    print(model, pruner_style)\n",
    "    for e in eigenvalues:\n",
    "        print(e)\n",
    "    print(\"+\"*100)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, cc in modules_community_dataset[\"network_property\"][0][\"community\"].items():\n",
    "    print(c, cc)\n",
    "_, all_label_skills = all_skill_label()\n",
    "print(modules_community_dataset[\"frequency_skill\"][0])\n",
    "#module_dataset_kl = get_high_datasets(modules_community_dataset[\"community\"][\"kl\"][idx][comm_name][\"dataset\"][\"all\"], top_skill=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def create_matrix(community):\n",
    "    modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "    all_layer = [str(i) for i in range(3,31)]\n",
    "    data = []\n",
    "    for l_idx ,l in enumerate(all_layer):\n",
    "        for m_idx,  m in enumerate(modules):\n",
    "            if f\"{l}_{m}\" in community:\n",
    "                c = community[f\"{l}_{m}\"]\n",
    "            else:\n",
    "                c = -1\n",
    "            data.append({\"Layer\":int(l),\n",
    "                    \"Modules\":m_idx, \n",
    "                    \"Community\":int(c)})\n",
    "    return pd.DataFrame(data), modules, all_layer\n",
    "\n",
    "def density(df):\n",
    "    density_df = df.groupby(['Layer', 'Community']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Normalize to get density\n",
    "    density_df = density_df.div(density_df.sum(axis=1), axis=0)\n",
    "\n",
    "    # Calculate Shannon Diversity Index for each layer\n",
    "    diversity_index = -np.sum(density_df * np.log(density_df + 1e-10), axis=1)\n",
    "\n",
    "    # Plot Density Heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Community Density per Layer\")\n",
    "    sns.heatmap(density_df.T, cmap=\"viridis\", cbar=True)\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Community\")\n",
    "    plt.show() \n",
    "\n",
    "for idx, model in enumerate(modules_community_dataset[\"model\"]):\n",
    "    pruner_style = modules_community_dataset[\"pruner_style\"][idx]\n",
    "    partition = modules_community_dataset[\"network_property\"][idx][\"partition\"]\n",
    "    num_colors = len(modules_community_dataset[\"network_property\"][idx][\"community\"])\n",
    "    \n",
    "    data,m,l = create_matrix(partition)\n",
    "    if -1 in data[\"Community\"].unique():\n",
    "        isolated = True\n",
    "    else: \n",
    "        False\n",
    "    pivot_data = data.pivot(index=\"Layer\", columns=\"Modules\", values=\"Community\").sort_index(ascending=True)\n",
    "    pivot_data.index = pivot_data.index.astype(int)\n",
    "    sorted_data = pivot_data.sort_index()\n",
    "    if isolated:\n",
    "        cmap = sns.color_palette(\"tab10\", num_colors+1)\n",
    "        cmap.insert(0, (0, 0, 0))\n",
    "    else:\n",
    "        cmap = sns.color_palette(\"tab10\", num_colors)\n",
    "    # Create a custom colormap from the discrete colors\n",
    "    discrete_cmap = mcolors.ListedColormap(cmap)\n",
    "    # Plotting the heatmap with hierarchical y-axis labels for layers and module types\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(sorted_data, yticklabels=True,vmin=-1,cmap=discrete_cmap)\n",
    "    plt.title(f\"Communities: {model}|{pruner_style}\")\n",
    "    plt.xlabel(\"Module Type\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.show()\n",
    "    density(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Skill Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict, Counter\n",
    "for idx, model in enumerate(modules_community_dataset[\"model\"]):\n",
    "    pruner_style = modules_community_dataset[\"pruner_style\"][idx]\n",
    "    communities = modules_community_dataset[\"network_property\"][idx][\"community\"]\n",
    "    skill_frequencies = modules_community_dataset[\"frequency_skill\"][idx]\n",
    "    G = modules_community_dataset[\"graph\"][idx]\n",
    "\n",
    "    skill_involvement = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for community_id, skills in skill_frequencies.items():\n",
    "        for skill_index, freq in enumerate(skills):\n",
    "            skill_involvement[community_id][skill_index] += freq\n",
    "\n",
    "    # Step 2: Aggregate skill involvement by layer and attention type across communities\n",
    "    layer_skill_involvement = defaultdict(lambda: defaultdict(int))\n",
    "    attn_type_skill_involvement = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for community_id, modules in communities.items():\n",
    "        skill_freqs = skill_frequencies[community_id]\n",
    "        \n",
    "        for module in modules:\n",
    "            layer = int(module.split('_')[0])  # Extract the layer number\n",
    "            attn_type = module.split('_')[1]  # Extract attention type (q, k, v, o)\n",
    "            \n",
    "            for skill_index, freq in enumerate(skill_freqs):\n",
    "                # Aggregate by layer\n",
    "                layer_skill_involvement[layer][skill_index] += freq\n",
    "                \n",
    "                # Aggregate by attention type\n",
    "                attn_type_skill_involvement[attn_type][skill_index] += freq\n",
    "\n",
    "    # Convert skill involvement data to DataFrames for easier visualization\n",
    "    layer_skill_df = pd.DataFrame(layer_skill_involvement).fillna(0).T\n",
    "    attn_type_skill_df = pd.DataFrame(attn_type_skill_involvement).fillna(0).T\n",
    "\n",
    "    # Step 3: Visualization\n",
    "    # Heatmap for Skill Involvement by Layer\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(layer_skill_df, cmap=\"YlGnBu\", annot=False, fmt=\".0f\")\n",
    "    plt.title(\"Skill Involvement by Layer\")\n",
    "    plt.xlabel(\"Skill Index\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.show()\n",
    "\n",
    "    # Heatmap for Skill Involvement by Attention Type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn_type_skill_df, cmap=\"YlGnBu\", annot=False, fmt=\".0f\")\n",
    "    plt.title(\"Skill Involvement by Attention Type\")\n",
    "    plt.xlabel(\"Skill Index\")\n",
    "    plt.ylabel(\"Attention Type\")\n",
    "    plt.show()\n",
    "\n",
    "    # Step 4: Community-wise skill involvement\n",
    "    community_skill_df = pd.DataFrame(skill_involvement).fillna(0)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(community_skill_df, cmap=\"YlGnBu\", annot=False, fmt=\".0f\")\n",
    "    plt.title(\"Skill Involvement by Community\")\n",
    "    plt.xlabel(\"Community ID\")\n",
    "    plt.ylabel(\"Skill Index\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "set_random_seed(int(2))\n",
    "plt.clf()\n",
    "modules=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "layer_modules_label=[ str(i)+\"_\"+m  for i in range(3,31) for m in modules]\n",
    "data_zscore_part = []\n",
    "data_dist = []\n",
    "data_correlation = []\n",
    "for idx, model in enumerate(modules_community_dataset[\"model\"]):\n",
    "    pruner_style = modules_community_dataset[\"pruner_style\"][idx]\n",
    "    G = modules_community_dataset[\"graph\"][idx]\n",
    "    network_property = modules_community_dataset[\"network_property\"][idx] \n",
    "    if idx%3 == 0:\n",
    "        fig, ax = plt.subplots(figsize=(16,4),ncols=3, sharey=True)\n",
    "        fig_node, ax_node = plt.subplots(figsize=(16,4),ncols=3,sharey=True)\n",
    "        fig.suptitle(f'Edge Weight Distribution | {pruner_style}')\n",
    "        fig_node.suptitle(f'Scatter Plot  | {pruner_style}') \n",
    "    if model== \"llama\" and pruner_style == \"block\":\n",
    "        ((skill_label,dataset_list,module_label),(dataset_shape, dataset_modules,skills_modules))= modules_community_dataset[\"network_data\"][idx]  \n",
    "        node_data(skill_label, dataset_list, module_label, f\"{model}_{pruner_style}_{sparsity_ratio}_nodes.csv\")\n",
    "        network_data(dataset_shape, dataset_list,skill_label,0,f\"{model}_{pruner_style}_{sparsity_ratio}_datasets_skills.csv\")\n",
    "        network_data(dataset_modules, dataset_list, module_label,1, f\"{model}_{pruner_style}_{sparsity_ratio}_datasets_modules.csv\")\n",
    "        network_data(skills_modules, skill_label, module_label,1, f\"{model}_{pruner_style}_{sparsity_ratio}_skill_modules.csv\")\n",
    "        node_data(skill_label, None, module_label, f\"{model}_{pruner_style}_{sparsity_ratio}_nodes_wo_dataset.csv\")\n",
    "    z_scores = within_module_z_score(G, network_property[\"community\"])\n",
    "    part_coeff = participation_coefficient(G, network_property[\"partition\"])\n",
    "    z_score_part_coeff = z_scores.merge(part_coeff, on=[\"node\",\"community\"], how=\"inner\")\n",
    "    z_score_part_coeff[\"pruner_style\"] = [pruner_style]*z_score_part_coeff.shape[0] \n",
    "    z_score_part_coeff[\"model\"] = [model]*z_score_part_coeff.shape[0] \n",
    "    \n",
    "    community_weights,community_node_weighted_sum = collect_edge_weights(G, network_property[\"community\"])\n",
    "    community_weights[\"pruner_style\"] = [pruner_style]*community_weights.shape[0] \n",
    "    community_weights[\"model\"] = [model]*community_weights.shape[0] \n",
    "    sns.histplot(data=community_weights, x=\"weight\", hue=\"community\", kde=True, bins=100, log_scale=True, palette=\"tab10\",ax= ax[idx-floor(int(idx/3)*3)])\n",
    "    ax[idx-floor(int(idx/3)*3)].set_title(model)\n",
    "    ax[idx-floor(int(idx/3)*3)].set_xlabel('Edge Weight')\n",
    "    ax[idx-floor(int(idx/3)*3)].set_ylabel('Frequency')\n",
    "    \n",
    "    sns.scatterplot(data=z_score_part_coeff,x=\"z_scores\",y=\"part_coeff\",hue=\"community\",ax= ax_node[idx-floor(int(idx/3)*3)], palette=\"tab10\")\n",
    "    for comm,color in zip(z_score_part_coeff[\"community\"].unique(),sns.color_palette(\"tab10\", len(z_score_part_coeff[\"community\"].unique()))):\n",
    "        sns.regplot(data=z_score_part_coeff[z_score_part_coeff[\"community\"]==comm],x=\"z_scores\",y=\"part_coeff\",ax= ax_node[idx-floor(int(idx/3)*3)], color=color,lowess=True)\n",
    "    ax_node[idx-floor(int(idx/3)*3)].set_title(model)\n",
    "    ax_node[idx-floor(int(idx/3)*3)].set_ylabel('Participant Coefficient')\n",
    "    ax_node[idx-floor(int(idx/3)*3)].set_xlabel('Z_Score')\n",
    "    \n",
    "    data_zscore_part.append(z_score_part_coeff)\n",
    "    data_dist.append(community_weights)\n",
    "    data_correlation.append(compute_correlation_community(z_score_part_coeff, model, pruner_style))\n",
    "\n",
    "    '''sns.lineplot(data=community_node_weighted_sum, y=\"weight\",x=\"degree\", hue=\"community\",  palette=\"tab10\",ax= ax_node[pruner_idx])\n",
    "    ax_node[pruner_idx].set_title(pruner_style)\n",
    "    ax_node[pruner_idx].set_xlabel('Total Node Weight')\n",
    "    ax_node[pruner_idx].set_ylabel('Frequency')'''\n",
    "    if idx in  [2, 5]:\n",
    "        plt.show()\n",
    "data_zscore_part_concat = pd.concat(data_zscore_part, ignore_index=True)\n",
    "data_zscore_part_concat.to_csv(\"./result/z_scores_part_coefficient.csv\", index=False)\n",
    "data_dist_concat = pd.concat(data_dist, ignore_index=True)\n",
    "data_dist_concat.to_csv(\"./result/distribution_comm.csv\", index=False)\n",
    "data_corr_concat = pd.concat(data_correlation, ignore_index=True)\n",
    "data_corr_concat.to_csv(\"./result/z_scores_part_correlation.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot distribution for Pearson Correlation\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data_corr_concat['pearson_correlation'], kde=True, color='blue', bins=10)\n",
    "plt.title('Distribution of Pearson Correlation')\n",
    "plt.xlabel('Pearson Correlation')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution for Spearman Correlation\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data_corr_concat['spearman_correlation'], kde=True, color='green', bins=10)\n",
    "plt.title('Distribution of Spearman Correlation')\n",
    "plt.xlabel('Spearman Correlation')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def fill_nan(df):\n",
    "    df.dropna()\n",
    "    #df[\"finetune\"] = df[\"finetune\"].fillna(\"None\")\n",
    "    #df[\"rank\"] = df[\"rank\"].fillna(\"None\")\n",
    "    return df\n",
    "df_2 = pd.read_csv('result/randomize_accuracy/randomize_data_new_kl_2.csv')\n",
    "df_2[\"min_loss\"] = df_2[\"loss\"].apply(lambda x: [float(xx) for xx in literal_eval(x)][-1])\n",
    "#df_2[\"min_loss\"] = df_2[\"min_loss\"].apply(lambda x: float(x))\n",
    "# Group by all columns except for 'accuracy' and 'l2', then calculate the mean for 'accuracy' and 'l2'\n",
    "finetuned = df_2.groupby(['iteration','model', 'pruning_style', 'community', 'pruning_ratio', 'dataset', 'rank', 'modules', 'modules_size', 'finetune']).agg({'accuracy': 'mean','min_loss':\n",
    "                                                                                                                                                              'mean'}).reset_index()\n",
    "#average_accuracy_by_group = finetuned.groupby([ 'model','pruning_style','community','finetune'])['accuracy'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(data=finetuned, x=\"pruning_style\",hue=\"finetune\", y=\"accuracy\",hue_order=[ \"Community\",\"Random\", \"All\"]) \n",
    "plt.title(f\"ALL\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(data=finetuned, x=\"pruning_style\",hue=\"finetune\", y=\"min_loss\",hue_order=[ \"Community\",\"Random\", \"All\"]) \n",
    "plt.title(f\"ALL\")\n",
    "plt.show()\n",
    "#average_accuracy_by_group.rename(columns={'accuracy': 'average_accuracy'}, inplace=True)\n",
    "average_accuracy_by_group =finetuned\n",
    "'''with pd.option_context('display.max_rows', None,):\n",
    "    print(average_accuracy_by_group.head(1000))'''\n",
    "for model in [\"llama\",\"llama_chat\",\"vicuna\"]:\n",
    "    for pruning_style in [\"block\",\"channel\"]:\n",
    "        filtered_pd = average_accuracy_by_group[(average_accuracy_by_group[\"model\"]==model) & (average_accuracy_by_group[\"pruning_style\"]==pruning_style)]\n",
    "        fig, [ax0, ax1]= plt.subplots(figsize=(20,7), ncols= 2)\n",
    "        sns.boxplot(data=filtered_pd, x=\"finetune\", y=\"accuracy\",ax=ax0, order=[\"Community\",\"Random\", \"All\"]) \n",
    "        sns.boxplot(data=filtered_pd, x=\"community\", hue=\"finetune\", y=\"accuracy\",ax=ax1,hue_order=[\"Community\",\"Random\", \"All\"]) \n",
    "        fig.suptitle(f\"Accuracy: {model} | {pruning_style}\")\n",
    "        plt.show()\n",
    "print(\"*\"*100)\n",
    "for model in [\"llama\",\"llama_chat\",\"vicuna\"]:\n",
    "    for pruning_style in [\"block\",\"channel\"]:\n",
    "        filtered_pd = average_accuracy_by_group[(average_accuracy_by_group[\"model\"]==model) & (average_accuracy_by_group[\"pruning_style\"]==pruning_style)]\n",
    "        fig, [ax0, ax1]= plt.subplots(figsize=(20,7), ncols= 2)\n",
    "        sns.boxplot(data=filtered_pd, x=\"finetune\", y=\"min_loss\",ax=ax0, order=[\"Community\",\"Random\", \"All\"]) \n",
    "        sns.boxplot(data=filtered_pd, x=\"community\", hue=\"finetune\", y=\"min_loss\",ax=ax1,hue_order=[\"Community\",\"Random\", \"All\"]) \n",
    "        fig.suptitle(f\"Loss: {model} | {pruning_style}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Community Analysis Correlation vs Accuracy and Loss of the Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy_by_group = finetuned.groupby([ 'model','pruning_style','community','finetune'])['accuracy'].mean().reset_index()\n",
    "average_loss_by_group = finetuned.groupby([ 'model','pruning_style','community','finetune'])['min_loss'].mean().reset_index()\n",
    "\n",
    "# Merging the DataFrames on 'model', 'community', and 'pruning_style'/'pruning_strategy'\n",
    "merged_df = pd.merge(average_accuracy_by_group, data_corr_concat, how='inner', left_on=['model', 'community', 'pruning_style'], right_on=['model', 'community', 'pruning_strategy'])\n",
    "merged_df_loss = pd.merge(average_loss_by_group, data_corr_concat, how='inner', left_on=['model', 'community', 'pruning_style'], right_on=['model', 'community', 'pruning_strategy'])\n",
    "\n",
    "# Plotting scatter plot of accuracy vs spearman_correlation\n",
    "for pruning_style in [\"block\",\"channel\"]:\n",
    "        fig, [ax0,ax1] = plt.subplots(figsize=(10,4),ncols=2)\n",
    "        filtered_pd_acc = merged_df[ (merged_df[\"pruning_style\"]==pruning_style)&(merged_df[\"finetune\"]==\"Community\")]\n",
    "        filtered_pd_loss = merged_df_loss[ (merged_df_loss[\"pruning_style\"]==pruning_style)&(merged_df[\"finetune\"]==\"Community\")]\n",
    "        sns.scatterplot(data = filtered_pd_loss, y ='min_loss', x = 'spearman_correlation',ax=ax1,hue=\"model\")\n",
    "        sns.scatterplot(data = filtered_pd_acc, y ='accuracy', x = 'spearman_correlation',ax=ax0,hue=\"model\")\n",
    "        ax0.set_title(f'Accuracy vs Spearman Correlation')\n",
    "        ax0.grid(True)\n",
    "        ax1.set_title(f'Loss vs Spearman Correlation')\n",
    "        ax1.grid(True)\n",
    "        fig.suptitle(pruning_style)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Distribution of the Performance of the Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, pearsonr, spearmanr\n",
    "grouped = df_2.groupby(['iteration', 'model', 'pruning_style', 'pruning_ratio'])\n",
    "for group_name, grouped_df in grouped:\n",
    "    print(group_name)\n",
    "    all_df = grouped_df[grouped_df['finetune'] == 'All']  # Replace 'modules' with actual label if different\n",
    "    actual_df = grouped_df[grouped_df['finetune'] == 'Community']  # Replace 'modules' with actual label if different\n",
    "    random_df = grouped_df[grouped_df['finetune'] == 'Random']  # Replace 'modules' with random label if different\n",
    "\n",
    "    \n",
    "    '''# Compare model L2 norm distributions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.kdeplot(data = grouped_df, x='model_l2', hue=\"finetune\", palette=[\"C0\", \"C1\", \"C2\"]) \n",
    "    plt.title(f'Model L2 Norm Distribution | {group_name}')\n",
    "    plt.xlabel('L2 Norm')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()'''\n",
    "\n",
    "    # Correlation between L2 norm and final loss per epoch\n",
    "    all_l2_loss_corr, _ = pearsonr(all_df['model_l2'].tolist()[0], all_df['min_loss'].tolist()[0])\n",
    "    actual_l2_loss_corr, _ = pearsonr(actual_df['model_l2'].tolist()[0], actual_df['min_loss'].tolist()[0])\n",
    "    random_l2_loss_corr, _ = pearsonr(random_df['model_l2'].tolist()[0], random_df['min_loss'].tolist()[0])\n",
    "    print(f'Correlation between L2 norm and final loss(All): {all_l2_loss_corr}')\n",
    "    print(f'Correlation between L2 norm and final loss(Actual): {actual_l2_loss_corr}')\n",
    "    print(f'Correlation between L2 norm and final loss(Random): {random_l2_loss_corr}')\n",
    "\n",
    "    # T-test on accuracy between actual and Randoms\n",
    "    t_stat, p_val = ttest_ind(actual_df['accuracy'], random_df['accuracy'])\n",
    "    print(f'T-test for accuracy difference between actual and Randoms: t-stat={t_stat}, p-value={p_val}')\n",
    "\n",
    "\n",
    "\n",
    "    # Loss and accuracy correlation\n",
    "    all_loss_accuracy_corr, _ = spearmanr(all_df['min_loss'], all_df['accuracy'])\n",
    "    actual_loss_accuracy_corr, _ = spearmanr(actual_df['min_loss'], actual_df['accuracy'])\n",
    "    random_loss_accuracy_corr, _ = spearmanr(random_df['min_loss'], random_df['accuracy'])\n",
    "    print(f'Spearman correlation between loss and accuracy (All): {all_loss_accuracy_corr}')\n",
    "    print(f'Spearman correlation between loss and accuracy (Actual): {actual_loss_accuracy_corr}')\n",
    "    print(f'Spearman correlation between loss and accuracy (Random): {random_loss_accuracy_corr}')\n",
    "    \n",
    "    fig, [axx,ax0,ax1,axll] = plt.subplots(figsize=(24, 6),ncols=4)\n",
    "\n",
    "    # Scatter plot with hue for different modules\n",
    "    #sns.scatterplot(data=grouped_df, x='model_l2', y='accuracy', hue='finetune')\n",
    "    sns.kdeplot(data = grouped_df, x='model_l2', hue=\"finetune\", palette=[\"C0\", \"C1\", \"C2\"],ax=axx) \n",
    "    sns.boxplot(data=grouped_df, x='finetune', y='accuracy', palette=[\"C0\", \"C1\", \"C2\"],ax=axll)\n",
    "\n",
    "    sns.scatterplot(data=grouped_df[grouped_df[\"finetune\"]!=\"All\"], x='model_l2', y='accuracy', hue='finetune', palette=[\"C1\", \"C2\"],ax=ax0)\n",
    "    sns.scatterplot(data=grouped_df[grouped_df[\"finetune\"]!=\"All\"], x='model_l2', y='min_loss', hue='finetune', palette=[\"C1\", \"C2\"],ax=ax1)\n",
    "\n",
    "    # Fit and plot a line of best fit for each module subset\n",
    "    #for finetune, subset in [(\"All\",all_df,\"C0\"),(\"Community\",actual_df,\"C1\" ),(\"Random\", random_df,\"C2\")]:\n",
    "    for finetune, subset, color in [(\"Community\",actual_df,\"C1\" ),(\"Random\", random_df,\"C2\")]:\n",
    "        slope, intercept = np.polyfit(subset['model_l2'], subset['accuracy'], 1)\n",
    "        ax0.plot(subset['model_l2'], slope * subset[\"model_l2\"] + intercept, color=color)\n",
    "        slope, intercept = np.polyfit(subset['model_l2'], subset['min_loss'], 1)\n",
    "        ax1.plot(subset['model_l2'], slope * subset[\"model_l2\"] + intercept, color=color)\n",
    "        ax1.set_ylim(ymin=0)\n",
    "    # Add labels and title\n",
    "    ax0.set_xlabel('Model L2 Norm')\n",
    "    ax0.set_ylabel('Accuracy')\n",
    "    ax1.set_ylabel('Final Loss')\n",
    "    fig.suptitle(f'{group_name}\\nMagnitude of Weight (L2) w.r.t Accuracy and Loss')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"+\"*500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare community and skill distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "blocks=[\"attn.q\", \"attn.k\", \"attn.v\", \"attn.o\",\"gate\",\"mlp.up\", \"mlp.down\"]\n",
    "layers=[ str(i) for i in range(3,31)]\n",
    "for idx, model in enumerate(modules_community_dataset[\"model\"]):\n",
    "    pruner_style = modules_community_dataset[\"pruner_style\"][idx]\n",
    "    G = modules_community_dataset[\"graph\"][idx]\n",
    "    layer_block_data = modules_community_dataset[\"network_property\"][idx][\"community\"]\n",
    "    skill_distribution = modules_community_dataset[\"frequency_skill\"][idx]\n",
    "    skill_distribution[\"skills\"]=all_label_skills\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    skill_df = pd.DataFrame(skill_distribution).set_index(\"skills\")\n",
    "\n",
    "    layer_counts = {}\n",
    "    block_counts = {}\n",
    "\n",
    "    # Count layers and blocks per community\n",
    "    for comm, modules in layer_block_data.items():\n",
    "        layer_counter = Counter()\n",
    "        block_counter = Counter()\n",
    "        \n",
    "        for module in modules:\n",
    "            layer, block = module.split('_', 1)\n",
    "            layer_counter[layer] += 1\n",
    "            block_counter[block] += 1\n",
    "        \n",
    "        # Ensure all possible layers and blocks are represented, filling missing ones with 0\n",
    "        layer_counts[comm] = {layer: layer_counter.get(layer, 0) for layer in layers}\n",
    "        block_counts[comm] = {block: block_counter.get(block, 0) for block in blocks}\n",
    "\n",
    "    # Convert counts to DataFrames\n",
    "    layer_df = pd.DataFrame(layer_counts)\n",
    "    block_df = pd.DataFrame(block_counts).fillna(0)\n",
    "\n",
    "    # Step 1: Calculate correlations of skills and layers for each community\n",
    "    skill_correlation = skill_df.corr(method = \"kendall\")  \n",
    "    layer_correlation = layer_df.corr(method = \"kendall\")      \n",
    "    modules_correlation = block_df.corr(method = \"kendall\")      \n",
    "\n",
    "    # Step 2: Visualize both correlation matrices\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "    # Skill Correlation Heatmap\n",
    "    sns.heatmap(skill_correlation, cmap=\"coolwarm\", center=0, ax=axes[0])\n",
    "    axes[0].set_title(\"Communities Correlation Across Skill\")\n",
    "    axes[0].set_xlabel(\"Communities\")\n",
    "    axes[0].set_ylabel(\"Communities\")\n",
    "\n",
    "    # Layer Correlation Heatmap\n",
    "    sns.heatmap(layer_correlation, cmap=\"coolwarm\", center=0, ax=axes[1])\n",
    "    axes[1].set_title(\"Communities Correlation Across Layer\")\n",
    "    axes[1].set_xlabel(\"Communities\")\n",
    "    axes[1].set_ylabel(\"Communities\")\n",
    "    \n",
    "    sns.heatmap(modules_correlation,  cmap=\"coolwarm\", center=0, ax=axes[2])\n",
    "    axes[2].set_title(\"Communities Correlation Across Modules\")\n",
    "    axes[2].set_xlabel(\"Communities\")\n",
    "    axes[2].set_ylabel(\"Communities\")\n",
    "\n",
    "    fig.suptitle(f\"{pruner_style} | {model} | Kendall Tau\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    '''# Step 4: Calculate correlation of skills with layers and skills with blocks\n",
    "    layer_skill_correlation = skill_df.T.corrwith(layer_df.T, axis=1)\n",
    "    block_skill_correlation = skill_df.T.corrwith(block_df.T, axis=1\n",
    "                                                  )\n",
    "\n",
    "    print(layer_skill_correlation)\n",
    "    # Step 5: Visualize the two correlation matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 10))\n",
    "\n",
    "    # Layer-Skill Correlation Heatmap\n",
    "    sns.heatmap(layer_skill_correlation, annot=True, cmap=\"coolwarm\", center=0, ax=axes[0])\n",
    "    axes[0].set_title(\"Correlation of Skills with Layers\")\n",
    "    axes[0].set_xlabel(\"Layers\")\n",
    "    axes[0].set_ylabel(\"Skills\")\n",
    "\n",
    "    # Block-Skill Correlation Heatmap\n",
    "    sns.heatmap(block_skill_correlation, annot=True, cmap=\"coolwarm\", center=0, ax=axes[1])\n",
    "    axes[1].set_title(\"Correlation of Skills with Blocks\")\n",
    "    axes[1].set_xlabel(\"Blocks\")\n",
    "    axes[1].set_ylabel(\"Skills\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data = {\n",
    "    \"Random\":{\n",
    "        \"epoch\":[0,1,2,3,4],\n",
    "        \"loss\" : ['0.16985096037387848', '0.007806216832250357', '0.007228560745716095', '0.007081119809299707', '0.007032547611743212'],\n",
    "        \"validation_loss\" : ['0.008793061599135399', '0.0076442379504442215', '0.0073838564567267895', '0.007319859694689512', '0.007318983320146799'],\n",
    "        \"validation_accuracy\" : ['0.1818181872367859', '0.23232322931289673', '0.23232322931289673', '0.23232322931289673', '0.23232322931289673'],\n",
    "        \"test_accuracy\": 0.22,\n",
    "        \"magnitude\":24155.91640162468\n",
    "    },\n",
    "    \"Community\": {\n",
    "            \"epoch\":[0,1,2,3,4],\n",
    "            \"loss\" : ['0.1691683530807495', '0.007796146906912327', '0.00724008260294795', '0.007082964759320021', '0.0070303212851285934'],\n",
    "            \"validation_loss\" :  ['0.00879395566880703', '0.007647279649972916', '0.00742013705894351', '0.007313245441764593', '0.007286437321454287'],\n",
    "            \"validation_accuracy\" : ['0.1818181872367859', '0.23232322931289673', '0.23232322931289673', '0.23232322931289673', '0.2525252401828766'],\n",
    "            \"test_accuracy\":0.22,\n",
    "            \"magnitude\":24155.917002677917\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_Random = pd.DataFrame.from_dict(data[\"Random\"])\n",
    "df_Community = pd.DataFrame.from_dict(data[\"Community\"])\n",
    "for val in [\"loss\",\"validation_loss\",\"validation_accuracy\"]:\n",
    "    df_Random[val] = df_Random[val].astype(float)\n",
    "    df_Community[val] = df_Community[val].astype(float)\n",
    "data_for_bar = {\n",
    "    \"Model\": [\"Random\", \"Random\", \"Random\", \"Community\", \"Community\", \"Community\"],\n",
    "    \"Metric\": [\"Validation Accuracy\", \"Test Accuracy\",\"Magnitude\", \"Validation Accuracy\", \"Test Accuracy\",\"Magnitude\"],\n",
    "    \"Accuracy\": [df_Random[\"validation_accuracy\"].max(), data[\"Random\"][\"test_accuracy\"],data[\"Community\"][\"magnitude\"], df_Community[\"validation_accuracy\"].max(),  data[\"Community\"][\"test_accuracy\"],data[\"Community\"][\"magnitude\"]]\n",
    "}\n",
    "df_bar = pd.DataFrame(data_for_bar)\n",
    "fig, [ax_loss, ax_val_loss] = plt.subplots(1, 2, figsize=(14,6))\n",
    "sns.lineplot(data=df_Random, x=\"epoch\",y=\"loss\",color=\"red\",label=\"Random\", ax=ax_loss)\n",
    "sns.scatterplot(data=df_Random, x=\"epoch\",y=\"loss\",color=\"red\",markers='o', ax=ax_loss)\n",
    "sns.lineplot(data=df_Community, x=\"epoch\",y=\"loss\",color=\"blue\",label=\"Community loss\", ax=ax_loss)\n",
    "sns.scatterplot(data=df_Community, x=\"epoch\",y=\"loss\",color=\"blue\",marker='v',s=50, ax=ax_loss)\n",
    "ax_loss.set_ylim(0.0069, 0.008) \n",
    "ax_loss.set_title(\"Finetuning Loss vs Epoch\")\n",
    "\n",
    "sns.lineplot(data=df_Random, x=\"epoch\",y=\"validation_loss\",color=\"red\",label=\"Random valid_loss\", ax=ax_val_loss)\n",
    "sns.scatterplot(data=df_Random, x=\"epoch\",y=\"validation_loss\",color=\"red\",markers='o', ax=ax_val_loss)\n",
    "sns.lineplot(data=df_Community, x=\"epoch\",y=\"validation_loss\",color=\"blue\",label=\"Community valid_loss\",ax=ax_val_loss)\n",
    "sns.scatterplot(data=df_Community, x=\"epoch\",y=\"validation_loss\",color=\"blue\",marker='v',s=50,ax=ax_val_loss)\n",
    "#ax_val_loss.set_ylim(0.0072, 0.09)\n",
    "ax_val_loss.set_title(\"Finetuning Validation Loss vs Epoch\") \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, [ax_accuracy, ax_test_accuracy] = plt.subplots(1, 2, figsize=(14,6)) \n",
    "sns.lineplot(data=df_Random, x=\"epoch\",y=\"validation_accuracy\",color=\"red\",label = \"Random Accuracy\", ax=ax_accuracy)\n",
    "sns.scatterplot(data=df_Random, x=\"epoch\",y=\"validation_accuracy\",color=\"red\",markers='o', ax=ax_accuracy)\n",
    "sns.lineplot(data=df_Community, x=\"epoch\",y=\"validation_accuracy\",color=\"blue\",label = \"Community Accuracy\", ax=ax_accuracy)\n",
    "sns.scatterplot(data=df_Community, x=\"epoch\",y=\"validation_accuracy\",color=\"blue\",marker='v',s=50, ax=ax_accuracy)\n",
    "ax_accuracy.set_title(\"Finetuning Validation Accuracy vs Epoch\") \n",
    "\n",
    "sns.barplot(data=df_bar, x=\"Metric\", y=\"Accuracy\", hue=\"Model\", ax=ax_test_accuracy, palette={\"Random\": \"red\", \"Community\": \"blue\"})\n",
    "ax_test_accuracy.set_title(\"Test Accuracy and Maximum Validation Accuracy Comparison\")\n",
    "ax_test_accuracy.set_ylim(0.0, 0.3) \n",
    "ax_accuracy.set_title(\"Accuracy Comparison\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=(SettingWithCopyWarning))\n",
    "\n",
    "def change_list_type_df(x):\n",
    "    x_list = literal_eval(x) \n",
    "    return [literal_eval(x) for x in x_list]\n",
    "\n",
    "df = pd.read_csv(\"result/randomize_accuracy/randomize_data_new_kl_longer_2.csv\")\n",
    "df[\"loss_training\"] = df[\"loss_training\"].apply(change_list_type_df) \n",
    "df[\"loss_validation\"] = df[\"loss_validation\"].apply(change_list_type_df)  \n",
    "df[\"model_l2_0\"] = df[\"model_l2_0\"].apply(change_list_type_df)  \n",
    "df[\"accuracy_validation\"] = df[\"accuracy_validation\"].apply(change_list_type_df)  \n",
    "# Ensure epochs are present for proper plotting\n",
    "df[\"epoch\"] = range(1, len(df) + 1)  # Modify this if epochs are explicitly defined in your data\n",
    "# Get unique communities\n",
    "unique_dataset = df['dataset'].unique()\n",
    "\n",
    "# Iterate through each community and plot\n",
    "for dataset in unique_dataset:\n",
    "    # Filter data for the current community\n",
    "    df_community = df[df['dataset'] == dataset]\n",
    "    df_Random = df_community[df_community['finetune'] == \"Random\"]\n",
    "    df_Community = df_community[df_community['finetune'] == \"Community\"]\n",
    "    df_All = df_community[df_community['finetune'] == \"All\"]\n",
    "\n",
    "\n",
    "    # Create a new figure for the community\n",
    "    fig, [ax_loss, ax_val_loss] = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Dataset: {dataset}\")\n",
    "\n",
    "    # Plot training loss\n",
    "    sns.lineplot(data=df_Random[\"loss_training\"].tolist()[0], color=\"red\", label=\"Random Training Loss\", ax=ax_loss)\n",
    "    sns.scatterplot(data=df_Random[\"loss_training\"].tolist()[0], color=\"red\", marker='o', ax=ax_loss)\n",
    "    sns.lineplot(data=df_Community[\"loss_training\"].tolist()[0], color=\"blue\", label=\"Community Training Loss\", ax=ax_loss)\n",
    "    sns.scatterplot(data=df_Community[\"loss_training\"].tolist()[0], color=\"blue\", marker='v', s=50, ax=ax_loss)\n",
    "    sns.lineplot(data=df_All[\"loss_training\"].tolist()[0], color=\"green\", label=\"All Training Loss\", ax=ax_loss)\n",
    "    sns.scatterplot(data=df_All[\"loss_training\"].tolist()[0], color=\"green\", marker='v', s=50, ax=ax_loss)\n",
    "    ax_loss.set_title(\"Training Loss vs Epoch\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Loss\")\n",
    "    # Plot validation loss\n",
    "    sns.lineplot(data=df_Random[\"loss_validation\"].tolist()[0], color=\"red\", label=\"Random Validation Loss\", ax=ax_val_loss)\n",
    "    sns.scatterplot(data=df_Random[\"loss_validation\"].tolist()[0], color=\"red\", marker='o', ax=ax_val_loss)\n",
    "    sns.lineplot(data=df_Community[\"loss_validation\"].tolist()[0], color=\"blue\", label=\"Community Validation Loss\", ax=ax_val_loss)\n",
    "    sns.scatterplot(data=df_Community[\"loss_validation\"].tolist()[0], color=\"blue\", marker='v', s=50, ax=ax_val_loss)\n",
    "    sns.lineplot(data=df_All[\"loss_validation\"].tolist()[0], color=\"green\", label=\"All Validation Loss\", ax=ax_val_loss)\n",
    "    sns.scatterplot(data=df_All[\"loss_validation\"].tolist()[0], color=\"green\", marker='v', s=50, ax=ax_val_loss)\n",
    "    ax_val_loss.set_title(\"Validation Loss vs Epoch\")\n",
    "    ax_val_loss.set_xlabel(\"Epoch\")\n",
    "    ax_val_loss.set_ylabel(\"Validation Loss\")\n",
    "\n",
    "    # Show legends and plot\n",
    "    ax_loss.legend()\n",
    "    ax_val_loss.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    columns_to_transform = [\"accuracy_validation_last\", \"accuracy_test\"]\n",
    "    df_community[\"accuracy_validation_last\"]= df_community[\"accuracy_validation\"].apply(lambda x: x[-1])\n",
    "    df_melted = df_community.melt(\n",
    "        id_vars=[\"finetune\"],  # Retain other columns\n",
    "        value_vars=columns_to_transform,  # Columns to transform\n",
    "        var_name='Metric', \n",
    "        value_name='Accuracy'\n",
    "\n",
    "    )\n",
    "    fig, [ax_accuracy, ax_test_accuracy, ax_magnitude] = plt.subplots(1, 3, figsize=(18,4))\n",
    "    \n",
    "    sns.lineplot(data=df_Random[\"accuracy_validation\"].tolist()[0], color=\"red\", label=\"Random Validation Accuracy\", ax=ax_accuracy)\n",
    "    sns.scatterplot(data=df_Random[\"accuracy_validation\"].tolist()[0], color=\"red\", marker='o', ax=ax_accuracy)\n",
    "    sns.lineplot(data=df_Community[\"accuracy_validation\"].tolist()[0], color=\"blue\", label=\"Random Validation Accuracy\", ax=ax_accuracy)\n",
    "    sns.scatterplot(data=df_Community[\"accuracy_validation\"].tolist()[0], color=\"blue\", marker='v', s=50, ax=ax_accuracy)\n",
    "    sns.lineplot(data=df_All[\"accuracy_validation\"].tolist()[0], color=\"green\", label=\"Random Validation Accuracy\", ax=ax_accuracy)\n",
    "    sns.scatterplot(data=df_All[\"accuracy_validation\"].tolist()[0], color=\"green\", marker='v', s=50, ax=ax_accuracy)\n",
    "    ax_accuracy.set_title(\"Finetuning Validation Accuracy vs Epoch\") \n",
    "\n",
    "    sns.barplot(data=df_melted, x=\"Metric\", y=\"Accuracy\", hue=\"finetune\", ax=ax_test_accuracy, palette={\"Random\": \"red\", \"Community\": \"blue\",\"All\":\"green\"})\n",
    "    ax_test_accuracy.set_title(\"Test Accuracy and Maximum Validation Accuracy Comparison\")\n",
    "    #ax_test_accuracy.set_ylim(0.0, 0.3) \n",
    "\n",
    "\n",
    "    sns.barplot(data=df_community, x=\"finetune\", y=\"model_l2\", ax=ax_magnitude, palette={\"Random\": \"red\", \"Community\": \"blue\",\"All\":\"green\"})\n",
    "    ax_magnitude.set_title(\"L2 norm\")\n",
    "    \n",
    "\n",
    "    ax_accuracy.legend()\n",
    "    ax_test_accuracy.legend() \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNeuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
