{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset \n",
    "import matplotlib.pyplot as plt\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np \n",
    "import torch\n",
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from utils.bag_of_words.skill_dataset import *\n",
    "cognitive_skills_community = {\n",
    "                    \"cognitive_process_memory\":[ \n",
    "                        \"sustained_attention\", \"selective_attention\", \"divided_attention\", \"vigilance_attention\",\"attention_shifting\",\n",
    "                        \"processing_speed\", \"visual_processing_speed\", \"auditory_processing_speed\",\n",
    "                        \"prospective_memory\", \"working_memory\", \"episodic_memory\", \"semantic_memory\", \"procedural_memory\", \"iconic_memory\", \"echoic_memory\", \"spatial_memory\"],\n",
    "                    \"executive_function\":[ \n",
    "                        \"planning\", \"organization\", \"goal_setting\",\"time_management\", \n",
    "                        \"problem_solving\", \"mental_flexibility\", \"strategic_thinking\",\"adaptability\",\n",
    "                        \"impulse_control\", \"decision_making\",\"emotional_regulation\",\"risk_assessment\",\n",
    "                        \"abstract_thinking\", \"reasoning\",\" concept_formation\", \"cognitive_flexibility\", \"creativity\"],\n",
    "                    \"language_communication\":[\n",
    "                         \"expressive_language\", \"receptive_language\", \"naming\", \"fluency\", \"comprehension\", \"repetition\", \"reading\", \"writing\", \n",
    "                         \"pragmatics\", \"discourse_ability\", \"expressive_language\", \"receptive_language\", \"linguistic_analysis\", \"narrative_skills\"],\n",
    "                    \"social_cognition\":\n",
    "                        [\"recognition_of_social_cues\", \"theory_of_mind\", \"empathy\", \"social_judgment\",\"intercultural_competence\",\"conflict_resolution\",\"self_awareness\",\"relationship_management\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result/dataMultidisciplinaryCognitiveSkillsFrameworkRestrict.json\", 'r') as openfile:\n",
    "    #with open(\"result/dataNeuropsychologicalDomains.json\", 'r') as openfile:\n",
    "    #with open(\"result/dataCategory.json\", 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    dataCategory = json.load(openfile)\n",
    "total_skill= 0 \n",
    "for d in dataCategory:\n",
    "    total_skill += len(dataCategory[d])\n",
    "print(total_skill)\n",
    "'''dataCategory = filterData(dataCategory, 0.75)#0.3\n",
    "total_skill= 0 \n",
    "for d in dataCategory:\n",
    "    total_skill += len(dataCategory[d])\n",
    "print(total_skill)'''\n",
    "'''\n",
    "dataCategory = td_idf_filter(dataCategory)\n",
    "total_skill= 0 \n",
    "for d in dataCategory:\n",
    "    total_skill += len(dataCategory[d])\n",
    "print(total_skill)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for node_i in dataCategory:\n",
    "    items_i =  np.unique(np.array(dataCategory[node_i]))\n",
    "    frequency_i = [(x,dataCategory[node_i].count(x)) for x in items_i]\n",
    "    get_node_i = sorted(frequency_i,key=lambda x: x[1], reverse=True)[0:20]\n",
    "    print(get_node_i)\n",
    "    get_node_i = [key[0] for key in get_node_i]\n",
    "    print(node_i, get_node_i)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nodes = tokens\n",
    "#Edges = Common tokens between different datasets\n",
    "def create_graph(dataCategory):\n",
    "    filter = None #just get top 50\n",
    "    edges = []\n",
    "    G =nx.Graph()\n",
    "    dataset = list(dataCategory.keys())\n",
    "    other_dataset = list(dataCategory.keys())\n",
    "    for node_i in dataset:\n",
    "        other_dataset.remove(node_i)\n",
    "        if filter != None:\n",
    "            items_i =  np.unique(np.array(dataCategory[node_i]))\n",
    "            frequency_i = [(x,dataCategory[node_i].count(x)) for x in items_i]\n",
    "            get_node_i = sorted(frequency_i,key=lambda x: x[1], reverse=True)[0:filter]\n",
    "            get_node_i = [key[0] for key in get_node_i]\n",
    "        else:\n",
    "            get_node_i = dataCategory[node_i]\n",
    "        for node_j in other_dataset:\n",
    "            if filter != None:\n",
    "                items_j =  np.unique(np.array(dataCategory[node_j]))\n",
    "                frequency_j = [(x,dataCategory[node_j].count(x)) for x in items_j]\n",
    "                get_node_j = sorted(frequency_j,key=lambda x: x[1], reverse=True)[0:filter]\n",
    "                get_node_j = [key[0] for key in get_node_j]\n",
    "            else:\n",
    "                get_node_j = dataCategory[node_j]\n",
    "            common = list(set(get_node_i).intersection(get_node_j))\n",
    "            #if len(common) == 0:\n",
    "            #    continue\n",
    "            if len(common) >= 1: #15\n",
    "                G.add_edge(node_i, node_j, weight=len(common))\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist(G,title):\n",
    "    degrees = sorted([G.degree(n) for n in G.nodes()])\n",
    "    plt.hist(degrees)\n",
    "    plt.title(\"Degree Distribution of \"+title)\n",
    "    plt.show()\n",
    "def draw_regular_network(G):\n",
    "    #G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    print(G)\n",
    "    nx.write_edgelist(G, \"network_words.txt\")\n",
    "    plot_degree_dist(G, f\"{G}\")\n",
    "\n",
    "    print(G)\n",
    "\n",
    "    plt.figure(figsize=(20, 20), dpi=100)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    pos = nx.spring_layout(G, k=0.8, iterations=20)\n",
    "    d = dict(G.degree)\n",
    "    nx.draw(G, pos, node_color='b',nodelist=d.keys(),node_size=[v * 10 for v in d.values()],with_labels = True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{G}\",fontsize = 40)\n",
    "    plt.show()\n",
    "    plot_degree_dist(G,G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "from copy import deepcopy\n",
    "from utils.bag_of_words.community_layout import *\n",
    "from utils.bag_of_words.network_property import *\n",
    "def test(g):\n",
    "    comm, partition = get_community(g)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "    pos = community_layout(deepcopy(g), partition)\n",
    "    plt.figure(figsize=(10,10),dpi=100)\n",
    "    d = dict(g.degree)\n",
    "    #nx.draw(g, pos, node_size=[v * 15 for v in d.values()], node_color=list(partition.values())); plt.show()\n",
    "    nx.draw(g, pos, node_color=list(partition.values()),nodelist=d.keys(),node_size=[v * 5 for v in d.values()],with_labels = True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)\n",
    "    return g, comm \n",
    "for filter in list(range(10,100,5))+[99,100,None]:\n",
    "    if filter!=None:\n",
    "        filter = filter/100\n",
    "        dataCategoryX = filterData(dataCategory, filter)#0.3\n",
    "    else:\n",
    "        dataCategoryX = dataCategory\n",
    "    G = create_graph(dataCategoryX)\n",
    "    communities, partition = get_community(G)\n",
    "    print(filter,\"\\t| \",nx.density(G),\"\\t| \",[f\"{i}: {len(communities[i])}\" for i in communities])\n",
    "'''print(G)\n",
    "G, communities = test(G)\n",
    "print([f\"{i}: {len(communities[i])}\" for i in communities])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills within Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topSkills(dict_of_lists, top=10):\n",
    "    from collections import Counter\n",
    "\n",
    "    # Flatten the lists into a single list\n",
    "    all_elements = [item for sublist in dict_of_lists.values() for item in sublist]\n",
    "\n",
    "    # Count the occurrences of each element\n",
    "    element_counts = Counter(all_elements)\n",
    "\n",
    "    # Get the 10 most common elements\n",
    "    most_common_elements = element_counts.most_common(top)\n",
    "\n",
    "    '''print(\"10 Most Common Elements:\")\n",
    "    for element, count in most_common_elements:\n",
    "        print(f\"{element}: {count} occurrences\")'''\n",
    "    return most_common_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "uniqueCategory = dataCategory #filterData(dataCategory, 1.0)\n",
    "different = set(uniqueCategory.keys()).difference(G.nodes)\n",
    "dict_of_lists = {}\n",
    "for idx, comm in enumerate(communities):\n",
    "    print(comm)\n",
    "    dict_of_lists[comm] = {}\n",
    "    for node in communities[comm]:\n",
    "        items =  np.unique(np.array(uniqueCategory[node]))\n",
    "        frequency = [(x,uniqueCategory[node].count(x)) for x in items]\n",
    "        get_node = sorted(frequency,key=lambda x: x[1], reverse=True)#[0:20]\n",
    "        get_node = [key[0] for key in get_node]\n",
    "        print(\"\\t\",node,\"\\t\",get_node)\n",
    "        dict_of_lists[comm][node] = get_node\n",
    "    \n",
    "    \n",
    "print(idx+1)\n",
    "dict_of_lists[idx+1] = {}\n",
    "for node in different:\n",
    "        items =  np.unique(np.array(uniqueCategory[node]))\n",
    "        frequency = [(x,uniqueCategory[node].count(x)) for x in items]\n",
    "        get_node = sorted(frequency,key=lambda x: x[1], reverse=True)#[0:20]\n",
    "        get_node = [key[0] for key in get_node]\n",
    "        print(\"\\t\",node,\"\\t\",get_node)\n",
    "        dict_of_lists[idx+1][node] = get_node\n",
    "json_object = json.dumps(dict_of_lists, cls=NumpyEncoder)\n",
    "with open(\"result/dataNeuropsychologicalDomainsCluster.json\", \"w\") as outfile:\n",
    "#with open(\"result/dataCategoryCluster.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comm in dict_of_lists:\n",
    "    skills = topSkills(dict_of_lists[comm], 50)\n",
    "    print(comm, [f\"({skill[0]} {skill[1]})\" for skill in skills])\n",
    "    print(comm, list(dict_of_lists[comm].keys()))\n",
    "    print(\"+\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills within and between Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_edge_weight_between_communities(graph, community1, community2):\n",
    "    total_weight = 0\n",
    "    for node1 in community1:\n",
    "        for node2 in community2:\n",
    "            if graph.has_edge(node1, node2):\n",
    "                total_weight += graph[node1][node2]['weight']  # Assuming the graph is weighted\n",
    "    return total_weight\n",
    "edge_weight_label = []\n",
    "edge_weight = np.zeros((len(dict_of_lists)-1,len(dict_of_lists)-1))\n",
    "for idx1, community_name1 in enumerate(dict_of_lists):\n",
    "    if community_name1 == list(dict_of_lists.keys())[-1]:\n",
    "        continue\n",
    "    community_node1 = list(dict_of_lists[community_name1].keys())\n",
    "    edge_weight_comm = []\n",
    "    for idx2, community_name2 in enumerate(dict_of_lists):\n",
    "        if community_name2 == list(dict_of_lists.keys())[-1]:\n",
    "            continue\n",
    "        community_node2 = list(dict_of_lists[community_name2].keys())\n",
    "        \n",
    "        edge_weight_comm.append(f\"{(len(community_node1),len(community_node2))}: {str(total_edge_weight_between_communities(G, community_node1, community_node2)/2)}\")\n",
    "        edge_weight[idx1,idx2] = total_edge_weight_between_communities(G, community_node1, community_node2)/2\n",
    "    edge_weight_label.append(edge_weight_comm)\n",
    "#diagonal means within communities\n",
    "total_weight = G.size(weight=\"weight\")\n",
    "sns.heatmap(edge_weight, annot=True, fmt=\".0f\")\n",
    "plt.title(\"Total Edge Weight Value\")\n",
    "plt.show()\n",
    "edge_weight = np.array(edge_weight)/total_weight\n",
    "sns.heatmap(edge_weight, annot=True, fmt=\".3f\")\n",
    "\n",
    "plt.title(\"Total Edge Weight Value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
